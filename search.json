[{"title":"DiskANN","url":"/2025/10/31/DiskANN/","content":"研究现状    当前最先进的近似最相邻算法为了保证快速的高召回率，其产生的索引必须存储在内存中。    但对于大规模高维数量集，单台服务器的内存容量会成为算法瓶颈，而使用分布式服务器对索引进行存储也会造成额外的磁盘访问开销和扩展成本较高。    对于这些针对主存所设计的向量检索算法，如果将算法所构建的索引存储在SSD或采用混合模式会使得其实现比较复杂，还会造成搜索性能较差（搜索延迟灾难性上升和吞吐量急剧下降）。    因此，如何突破内存容量的限制，实现既能处理海量数据又能保持低延迟的“磁盘友好型”向量搜索算法显得十分重要。\n\n具体技术本文介绍了一种新型的基于图的索引构建算法Vamana，以及DiskANN整体系统的设计。\n\nVamana算法Vamana算法通过图结构优化减少磁盘访问次数。Vamana算法使用GreedySearch算法和RobustProne算法进行图的构建，所构建的图为有向图。GreedySearch算法为节点选取出边邻居，RobustProne算法通过剪枝操作来优化和确认节点的出边邻居集合。\n\n\nGreedySearch算法\n\n对于查询节点q，从有向图G的起始节点s开始搜索，搜索列表长度为M，结果集长度为k且M&gt;=k，候选集为L，已访问集为V。在初始化的时候将s添加到L，置V为空。当L和V的差集不为空（即候选集中还存在未被访问的节点）时，进行如下循环。从上述未被访问的点中选取距离查询节点q距离最小的节点p，然后将p的出边邻居加入L，并将节点p加入V（标记为已访问）。然后判断L的长度是否大于M，若是则保留距离q最近的M个节点。最终返回L中的k个最近邻点和已访问集合V。\n\n比较L的长度是否大于M，是为了保证候选集的大小不会过大，并平衡搜索精度和效率。\n\nRobustProne算法\n\n将从GreedySearch算法得到的已访问集合作为候选集V，查询节点为p，出度上限为R，距离阈值为α，出边邻居集合为N。首先，将N加入到V，并置N为空。当V不为空时，进行下述循环。从V中选取距离点p最近的点p*，然后将其加入N。判断N的长度是否等于R，若是则结束循环，算法结束，否则遍历V中的任一点p‘，判断α · d(p*, p′) ≤ d(p, p′)是否成立，若是则从V中移除p’。最终得到p的新的出度邻居集合N。\n\nRobustProne算法使用GreedySearch算法得到的已访问集合作为候选集进行剪枝操作，原因：为查询节点引入长距离连接，避免了满足SNG图属性的线性图搜索导致大量磁盘顺序读操作，原理：通过参数α的大小主动保留那些能够快速到达远方的点，并建立远边连接。\nα · d(p, p′)≤d(p, p′)的作用：移除与所选出边邻居相似的节点，确保后续选出的邻居具有多样性（分散在不同区域），保留了长边连接的点。α&#x3D;1快速剪枝控制度数，第二轮α&gt;1补充长边提升连通性，当α&#x3D;1时，只要p’离p*不比离p远，就删除pp’边，图会比较稀疏，度数很小，当α&gt;1时，放宽条件，图更稠密，直径更小。\n\nVamana算法流程\n\n1.随机初始化图G，每个点随机指向R个近邻；2.计算全局质心，找到距离全局质心最近的点作为所有搜索请求的入口点p；3.基于图G和入口点p对数据集中的每个点执行近邻搜索，将搜索路径上经过的所有点作为候选点集，执行α=1的边修剪策略；4.调整α&gt;1重复步骤3，再次执行边修剪策略来填充边，提高图G的质量以保证召回率。\n\n对于Vamana算法，在执行RobustProne算法后，需要对出边邻居n建立反向边，若建立后Nout(n)的长度大于R，则对其执行RobustProne算法进行剪枝操作。\n使用α &gt; 1构建的图，在搜索时可以更快地接近目标点，需要的“跳数”（读取节点信息）更少。这对于后续放在 SSD 上非常重要，因为每次“跳”都需要一次 SSD 读取。\nVamana算法最终会构建一个部分对称的有向图。\nVamana算法与HNSW、NSSG算法的对比\n1.Vamana与NSG都构造了有向图，HNSW构造了无向图。2.Vamana图索引构造从随机图开始、HNSW从空图开始，NSG则从K近邻图开始。3.Vamana和NSG使用贪婪搜索的已访问集合来进行剪枝操作，而HNSW算法则使用结果集进行剪枝操作。4.HNSW和NSG都没有可调参数α，而Vamana则使用α对图度数和直径进行权衡。对比HNSW和NSG，Vamana采用两轮迭代，可以生成质量更好的图结构。\nNSG的构建始于全量数据的KNN图（KNNG），这一步骤复杂度高达O(n²)，在十亿级数据集上需要数天时间。\nDiskANN磁盘索引策略重叠簇索引、束搜索、热点缓存、全精度向量的隐式重排序\n索引构建：\n1.从全量数据上采样出部分数据，训练k-means聚类，从而将全量数据划分为k个簇。为了保证以簇为单位构建的图索引能够合并起来，DiskANN将每个数据点分配到距离最近的X个簇中；2.每个簇包含约Nx/k个点，分别在内存中为每个簇构建图索引；3.依赖簇之间的重叠点将小图索引合并为大图索引。\n\n向量查询：\n1.DiskANN在整个搜索过程中维护搜索队列、结果队列两个队列，值得强调的是搜索队列的长度有限，受超参数控制。2.首先，使用PQ距离对候选入口点排序并传入搜索队列；然后，对于搜索队列中已经缓存的点，用其原始数据和查询点计算距离并送入结果队列。3.对于未命中缓存的点，开启异步进程使用束搜索去磁盘中以Vamana索引取回其原始数据表示和邻居列表；4.将每次取回的邻居列表送入搜索队列，因为搜索队列长度有限，所以依PQ距离近似排序舍弃超出部分；5.重复以上步骤，直至搜索队列为空，返回结果队列中的top-k作为最终结果。\n\nDiskANN使用压缩数据在内存中进行距离计算，使用乘积量化对原始数据进行压缩编码。\n对于磁盘索引，吞吐量受限于随机磁盘读取的次数，延迟受限于往返磁盘的次数，对此DiskANN使用束搜索和热点缓存进行优化。通过束搜索，对于当前搜索节点p，DiskANN一次性从磁盘中读取它的W（“束宽”）个邻居。此外，根据查询分布对节点进行缓存，或简单将入口点的C(C ≈ 3)跳邻居都缓存在内存中。\n由于通过过PQ计算得到的最邻近的k个候选点，可能与使用真实距离计算得出的结果存在差异，因此DiskANN会使用全精度向量对搜索过程中已访问的节点进行重排，并选取k个最近邻节点。由于DiskANN将全精度向量与其邻域信息一同存储在SSD中，因此当在查询过程中从SSD中获取某节点的邻域信息时，也会同步获取其全精度向量且无需额外的磁盘读取操作。\n具体结果通过实验对比Vamana算法与HNSW算法和NSG算法，得出结论Vamana在来自不同来源的百维和千维数据集上的性能与当前最好的ANNS算法相当或更优。\n对于基于SSD的服务场景中，跳数与搜索延迟直接相关， 每一跳都要对SSD进行读取，相对于HNSW和NSG，Vamana算法完成一次搜索所需的跳数更少，因此更具优势。\n通过对比十亿级别数据集上Vamana算法的单次全量索引和多次合并索引，得出结论虽然单次索引的性能优于合并索引，但与单次索引相比，达到相同召回率所需的额外延迟不超过20%。\n通过DiskANN和IVFOADC+G+P进行对比，在内存占用相同的情况下，DiskANN可以保持更高的召回率。\n面向SSD的十亿规模高效向量检索方案\nDiskANN的目标就是在单台机器上，使用有限的内存，实现对十亿级数据集的快速、高精度的最近邻搜索。\n","categories":["向量检索"],"tags":["向量检索","SSD"]},{"title":"Hello World","url":"/2025/10/30/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","categories":["hexo教程"],"tags":["hexo教程"]},{"title":"LEANN","url":"/2025/10/31/LEANN/","content":"论文笔记\n论文提出了一种针对资源受限的个人设备的低存储的ANN搜索索引\n研究背景\n目前基于嵌入的搜索（向量检索技术）已经被广泛应用在推荐系统和RAG中，个人设备对此也逐渐突显需求。但由于个人设备存储空间限制和ANN索引存储开销过大的原因，无法在个人设备中存储必要的索引结构。因此，减少索引存储开销和保持搜索质量和延迟成为了关键问题。\n行业需求\n随着智能手机等边缘设备越来越普及，这些设备不断生成多模态数据（如图像、文字、音频等）。这些数据如果能被有效地检索、组合起来，对用户回忆、检索过去信息、辅助智能应用等都有潜在价值。\n边缘设备上向量检索的应用：设备端RAG系统、个性化推荐系统、图像与视频内容检索等。\n研究动机\n动态重算嵌入以节省存储：传统图索引（如 HNSW）在查询时只会访问少量向量，因此LEANN不再将所有嵌入存储在磁盘上，而是在查询时按需重算。为减少延迟，LEANN 采用了双层遍历算法（结合近似与精确队列）和动态批处理机制（提高 GPU 利用率），从而显著降低重算开销。\n图结构剪枝以压缩索引元数据：即使不存储嵌入，图索引的邻接信息仍带来较大存储负担。LEANN 通过分析发现，许多边和节点对检索效果贡献有限，因此提出了高出度保持的剪枝策略，去除冗余边、保留关键枢纽节点，从而在几乎不影响搜索精度的前提下，大幅减少索引体积。\n为此，本论文提出了LEANN，通过结合紧凑图索引和高效即时重计算，以实现最小的存储负担和快速准确的检索。\n具体技术\n在HNSW查询中，只有很少的节点会被访问，因此 LEANN 不再预先存储全部向量，而是对这些少量节点在查询时即时重算，从而大幅降低存储成本，同时保持搜索准确率和延迟在可接受范围内。\n\n重计算虽然能节省存储空间，但仍然会导致较高的延迟。\n并且剩余的图元数据可能仍然占比很大的存储开销。\n\n对此，LEANN 采用两级图遍历算法和动态批处理机制来降低重新计算的延迟。并使用一种高度保留的图剪枝技术，以显著减少图元数据所需的存储空间。\n系统工作流程\n对于给定的数据集，LEANN首先计算所有条目的嵌入向量，并利用现有的图索引方法构建向量索引。然后LEANN丢弃所有条目的嵌入向量，并通过一种高度保留的图剪枝算法对图结构进行剪枝，以减少存储开销。该算法优先保留访问频繁的“枢纽节点”因为节点访问分布往往极度偏斜。\n在查询时，LEANN使用一个两级搜索算法在被剪枝后的图上进行遍历，识别并优先探索那些更有潜力的节点。这些被选中的节点随后会被发送到嵌入服务器（一个设备端组件，用于调用原始嵌入模型重新计算节点的嵌入）以获取对应的向量表示。\n为了进一步提高 GPU 利用率并减少延迟，LEANN 采用了一种动态批处理策略，用来调度在 GPU 上的嵌入计算任务。此外，当设备上有额外磁盘空间可用时，LEANN 会将这些“枢纽节点（hub nodes）”缓存在磁盘中。运行时，系统仅对未缓存的节点重新计算嵌入，而对已缓存的节点则直接从磁盘加载，从而减少计算负担并进一步提升查询性能。\nTwo-Level Search (向量搜索过程)\n主要思想：\n只需对近似距离排名靠前的向量进行重新排序（re-ranking），就足以获得较高的召回率，同时能有效剪枝掉在错误方向上的节点。\nAQ（Approximate Queue，近似队列），用于存储所有计算近似距离的已访问节点；EQ（Exact Queue, 精确队列），用于存放计算精确距离的节点；re-ranking ratio（重新排序比例），用于控制进行精确计算节点的比例。\n算法流程\n从入口节点开始，计算其精确距离，将入口节点加入EQ、R和visited。如果EQ不为空，每次从EQ中取出距离查询向量q最近的节点v，从R中取出距离q最远的节点f。若dvq&gt;dfq则结束算法，否则遍历v的所有邻居，对每个未访问的节点n，将其加入visited，计算近似距离d_ap，然后将节点n加入到AQ（队列中的节点会保存节点标识和对应距离，d_ap的计算可以通过PQ实现）。然后从AQ中取出前a%的节点（最有潜力的节点），对这些节点计算精确距离d_ex，将其加入ED和R。\nDynamic batching strategy （搜索过程）\n图索引每次扩展都要根据上次扩展结果选出最有希望的节点，会导致每次GPU只能处理很小一批数据，使得GPU资源浪费严重、吞吐低、延迟高。动态批处理策略放宽这种严格的数据依赖性，每次扩展从候选队列中一次取出多个节点进行扩展，将它们的邻居节点使用GPU一起进行重计算。这样计算更高效、延迟降低，扩展顺序虽然有延迟，但整体搜索结果几乎没有影响。\n\n每个节点平均有 8 个邻居；\n原始做法：每次扩展 1 个节点 → batch size &#x3D; 8；\n动态批处理：一次从队列取 8 个节点 → 共 8×8 &#x3D; 64 个邻居 → batch size &#x3D; 64。\n\nHigh Degree Preserving Graph Pruning （索引构建阶段）\nLEANN引入了一个用户定义的磁盘预算C。如果图元数据超过这个阈值，LEANN会触发剪枝算法。\n主要思想：\n只要保持高度数的枢纽节点，即可维持搜索性能，在减少边数量的同时保持检索准确率。\nG：原始图；V：节点集合；ef：候选集合大小；M：高度数节点允许的最大连接数；m：普通节点允许的最大连接数(m&lt;M)；a：高节点度数比例\n\n计算G中每个节点v的度数Dv，并初始化G1用来存放剪枝后的图。\n从D中选出出度最高的前a%节点，记为集合V*\n对V中每个节点v进行计算\n\n调用Best-First Search算法得到v的候选邻居列表W，如果v属于V*，M0&#x3D;M，否则，M0&#x3D;m。\n通过原始启发式（d(p, p′)≤d(p, p′)这种启发式剪枝）从W中选取M0个节点作为v的邻居并建立双向边，并添加到G1。\n如果存在邻居的出边度数大于M，则进行剪枝出度边。\n\nLEANN 使用 按需访问和动态批处理（dynamic batching）：\n查询开始时，只将部分必要的图元数据加载到内存（例如初始候选节点及其邻居信息）。\n随着搜索的推进，按需加载更多节点信息，同时对节点向量进行重算（embedding recomputation）。\n\n\n这种策略可以避免一次性占用大量内存，同时保证图遍历的效率。\n\nLEANN 并不是把整个索引图一次性载入内存，而是按需加载图的部分结构并结合向量重算，以在低内存环境下实现高效查询。\n","categories":["向量检索"],"tags":["向量检索","边缘设备"]}]