[{"title":"AiSAQ","url":"/2025/12/23/AiSAQ/","content":"《AiSAQ : All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval》\n\n\n    总结\n    \n        DiskANN 采用乘积量化技术以降低内存占用，但其内存开销仍与数据集规模成正比。基于该问题，本文提出了一种基于乘积量化的全存储式近似最近邻搜索方法 AiSAQ，将压缩后的向量完全卸载至 SSD 索引中。在十亿级规模的数据集上，AiSAQ 在查询阶段的内存占用仅约 10 MB，且未引入显著的查询延迟性能退化。此外，AiSAQ 显著降低了查询准备阶段的索引加载时间，从而支持在多个十亿级索引之间进行快速切换。\n    \n\n\n\n在近年来大语言模型LLM的发展趋势中，检索增强生成RAG被广泛用于利用外部知识源生成更准确的回答。在某些场景下，RAG 流水线需要同时接入多个外部知识源，此时检索器需要在不同索引之间进行“切换”。这一过程要求在服务期间将所有数据源的索引常驻于内存中，或在每次请求时从外部存储中加载索引数据，从而带来较高的内存开销或显著的加载延迟。\nDrawbacks of DiskANN在 DiskANN 中，PQ 压缩向量存储在 DRAM 中，用于决定搜索路径；而完整精度向量以及节点$vi$ 的出邻居集合$N{\\mathrm{out}}(v_i)$的节点 ID，则以连续的 LBA（Logical Block Address）空间形式存放在 SSD 等外部存储设备上，本文称为该节点的节点块（node chunk）。\n\nLBA块中节点块的详细信息\n\n设每个完整精度向量占用 $b{full}$字节，图的最大出度为 R，节点 ID 或出度信息的表示需要 $b{num}$字节（通常为 4 字节），则单个节点块的大小为：\n\n\\begin{aligned}B_{DiskANN} = b_{full} + b_{num}(R + 1) \\end{aligned}操作系统以块（block）为单位向存储设备发起 I/O 请求，在大多数操作系统配置中，块大小为 B=4 KB。在多数情况下，有 $B{DiskANN}$≤B，此时一个块中可以包含一个或多个节点块(a)。若 $B{DiskANN}$&gt;B，则单个节点块将跨越多个块进行存储(b)。在这两种情况下，若某一节点块无法完全放入当前块的剩余空间中，则该节点块会被对齐至下一个块的起始位置。\n在查询过程中，DiskANN 会针对单个节点块发起读取 ⌈$\\frac{B_{DiskANN}}{B}$⌉ 个块的 I/O 请求。\nDiskANN的缺陷\n\n内存使用量高：DiskANN 需要将数据集中所有节点的 PQ 压缩向量加载至内存中，因此其内存使用量大致与数据集规模 N 成正比。\n空间利用效率低：在 DiskANN 的束搜索查询过程中，实际参与距离计算的节点数量远小于数据集的整体规模 N。\n索引切换开销大：一些LLM模型需要在不同的向量数据库之间进行切换以支持针对不同领域的LLM语料库。在需要切换语料库时会带来高昂的时间开销，因为DiskANN需要加载所有向量的PQ向量，加载时间非常长；如果选择把所有语料库的索引都保存在内存中则将带来高昂的内存开销。\n\nAiSAQ Methodology当 DiskANN 的束搜索定位于节$vi$时，其出邻居集合$N{\\mathrm{out}}(v_i)$的 PQ 向量可通过存储在节点块中的节点 ID 进行索引。这表明，这些 PQ 向量本身也可以直接存储在节点$v_i$的节点块中，即是 AiSAQ 的核心思想。\n\n节点块数据的放置\n\n由于引入 PQ 向量后节点块尺寸相较于 DiskANN 更大，AiSAQ 需要对节点块结构进行调整，以更有效地填充存储块并尽量降低 I/O 延迟的增长。设每个 PQ 向量占用 $b_{PQ}$字节，则 AiSAQ 中单个节点块的大小为：\nB_{\\mathrm{AiSAQ}}=B_{\\mathrm{DiskANN}}+R\\cdot b_{\\mathrm{PQ}}=b_{\\mathrm{full}}+b_{\\mathrm{num}}+R(b_{\\mathrm{num}}+b_{\\mathrm{PQ}})通过调整最大出度 R，使得 $B{AiSAQ}$≤nB 或 $B{AiSAQ}$≤B/n(n∈N)成立，从而更好地适配存储块大小。\n在查询阶段，$N_{\\mathrm{out}}(v_i)$的 PQ 向量直接从存储设备中读取的节点块中获取，而非从内存中加载，并用于计算其与查询向量之间的 PQ 距离。通过将 DiskANN 中存储块的未使用空间填充为 PQ 向量，AiSAQ 在获取 PQ 向量时无需额外发起 I/O 请求，使得每一次图跳转的距离计算均可在单个节点块内完成，而不依赖 DRAM。每一跳完成距离计算后，对应的 PQ 向量即可被立即丢弃。\n由于某一节点的 PQ 向量是在前一次图跳转中获得的，该方法无法直接获取入口节点（entrypoint）的 PQ 向量。因此，AiSAQ 仅在 DRAM 中保留少量入口节点的 PQ 向量（通常 $n{ep}$=1）。在该设计下，系统在任意时刻最多只需在 DRAM 中保留 (R+$n{ep}$)个 PQ 向量，其数量与数据集规模 N 无关且远小于 N，从而实现近乎零的内存占用。\nAiSAQ Implementation官方开源实现：https://github.com/KioxiaAmerica/aisaq-diskann\n","categories":["磁盘索引"],"tags":["ANNS","图索引"]},{"title":"DiskANN","url":"/2025/10/31/DiskANN/","content":"《DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node》\n\n\nDiskANN是面向SSD的十亿规模高效向量检索方案，其目标就是在单台机器上，使用有限的内存，实现对十亿级数据集的快速、高精度的最近邻搜索。\n研究现状当前最先进的近似最相邻算法为了保证快速的高召回率，其产生的索引必须存储在内存中。但对于大规模高维数量集，单台服务器的内存容量会成为算法瓶颈，而使用分布式服务器对索引进行存储也会造成额外的磁盘访问开销和扩展成本较高。对于这些针对内存所设计的向量检索算法，如果将算法所构建的索引存储在SSD或采用混合模式会使得其实现比较复杂，还会造成搜索性能较差（搜索延迟灾难性上升和吞吐量急剧下降）。因此，如何突破内存容量的限制，实现既能处理海量数据又能保持低延迟的“磁盘友好型”向量搜索算法显得十分重要。\n具体技术本文介绍了一种新型的基于图的索引构建算法Vamana，以及DiskANN整体系统的设计。\nVamana算法Vamana算法通过图结构优化减少磁盘访问次数。 Vamana算法使用GreedySearch算法和RobustProne算法进行图的构建，所构建的图为有向图。GreedySearch算法为节点选取出边邻居，RobustProne算法通过剪枝操作来优化和确认节点的出边邻居集合。\n\nGreedySearch算法\n\n\nGreedySearch 算法\n\n对于查询节点q，从有向图G的起始节点s开始搜索，搜索列表长度为M，结果集长度为k且M&gt;=k，候选集为L，已访问集为V。在初始化的时候将s添加到L，置V为空。当L和V的差集不为空（即候选集中还存在未被访问的节点）时，进行如下循环。从上述未被访问的点中选取距离查询节点q距离最小的节点p，然后将p的出边邻居加入L，并将节点p加入V（标记为已访问）。然后判断L的长度是否大于M，若是则保留距离q最近的M个节点。最终返回L中的k个最近邻点和已访问集合V。\n比较L的长度是否大于M，是为了保证候选集的大小不会过大，并平衡搜索精度和效率。\n\nRobustProne算法\n\n\nRobustProne 算法\n\n将从GreedySearch算法得到的已访问集合作为候选集V，查询节点为p，出度上限为R，距离阈值为α，出边邻居集合为N。首先，将N加入到V，并置N为空。当V不为空时，进行下述循环。从V中选取距离点p最近的点p，然后将其加入N。判断N的长度是否等于R，若是则结束循环，算法结束，否则遍历V中的任一点p‘，判断α · d(p, p′) ≤ d(p, p′)是否成立，若是则从V中移除p’。最终得到p的新的出度邻居集合N。\nRobustProne算法使用GreedySearch算法得到的已访问集合作为候选集进行剪枝操作，原因：为查询节点引入长距离连接，避免了满足SNG图属性的线性图搜索导致大量磁盘顺序读操作，原理：通过参数α的大小主动保留那些能够快速到达远方的点，并建立远边连接。\nα · d(p, p′)≤d(p, p′)的作用：移除与所选出边邻居相似的节点，确保后续选出的邻居具有多样性（分散在不同区域），保留了长边连接的点。α=1快速剪枝控制度数，第二轮α&gt;1补充长边提升连通性，当α=1时，只要p’离p*不比离p远，就删除pp’边，图会比较稀疏，度数很小，当α&gt;1时，放宽条件，图更稠密，直径更小。\n\nVamana算法流程\n\n\n随机初始化图G，每个点随机指向R个近邻；\n计算全局质心，找到距离全局质心最近的点作为所有搜索请求的入口点p；\n基于图G和入口点p对数据集中的每个点执行近邻搜索，将搜索路径上经过的所有点作为候选点集，执行α=1的边修剪策略；\n调整α&gt;1重复步骤3，再次执行边修剪策略来填充边，提高图G的质量以保证召回率。\n\n对于Vamana算法，在执行RobustProne算法后，需要对出边邻居n建立反向边，若建立后Nout(n)的长度大于R，则对其执行RobustProne算法进行剪枝操作。\n使用α &gt; 1构建的图，在搜索时可以更快地接近目标点，需要的“跳数”（读取节点信息）更少。这对于后续放在SSD上非常重要，因为每次“跳”都需要一次SSD读取。\nVamana算法最终会构建一个部分对称的有向图。\nVamana算法与HNSW、NSSG算法的对比\n\nVamana与NSG都构造了有向图，HNSW构造了无向图。\nVamana图索引构造从随机图开始、HNSW从空图开始，NSG则从K近邻图开始。\nVamana和NSG使用贪婪搜索的已访问集合来进行剪枝操作，而HNSW算法则使用结果集进行剪枝操作。\nHNSW和NSG都没有可调参数α，而Vamana则使用α对图度数和直径进行权衡。\n对比HNSW和NSG，Vamana采用两轮迭代，可以生成质量更好的图结构。\n\nNSG的构建始于全量数据的KNN图（KNNG），这一步骤复杂度高达O(n²)，在十亿级数据集上需要数天时间。\nDiskANN磁盘索引策略重叠簇索引、束搜索、热点缓存、全精度向量的隐式重排序\n索引构建：\n\n从全量数据上采样出部分数据，训练k-means聚类，从而将全量数据划分为k个簇。为了保证以簇为单位构建的图索引能够合并起来，DiskANN将每个数据点分配到距离最近的X个簇中；\n每个簇包含约Nx/k个点，分别在内存中为每个簇构建图索引；\n依赖簇之间的重叠点将小图索引合并为大图索引。\n\n向量查询：\nDiskANN在整个搜索过程中维护搜索队列、结果队列两个队列：\n\n首先，使用PQ距离对候选入口点排序并传入搜索队列；然后，对于搜索队列中已经缓存的点，用其原始数据和查询点计算距离并送入结果队列。\n对于未命中缓存的点，开启异步进程使用束搜索去磁盘中以Vamana索引取回其原始数据表示和邻居列表；\n将每次取回的邻居列表送入搜索队列，因为搜索队列长度有限，所以依PQ距离近似排序舍弃超出部分；\n重复以上步骤，直至搜索队列为空，返回结果队列中的top-k作为最终结果。\n\nDiskANN使用乘积量化对原始数据进行压缩编码，并使用压缩数据在内存中进行距离计算。\n对于磁盘索引，吞吐量受限于随机磁盘读取的次数，延迟受限于往返磁盘的次数，对此DiskANN使用束搜索和热点缓存进行优化。通过束搜索，对于当前搜索节点p，DiskANN一次性从磁盘中读取它的W（“束宽”）个邻居。此外，根据查询分布对节点进行缓存，或简单将入口点的C(C ≈ 3)跳邻居都缓存在内存中。\n由于通过过PQ计算得到的最邻近的k个候选点，可能与使用真实距离计算得出的结果存在差异，因此DiskANN会使用全精度向量对搜索过程中已访问的节点进行重排，并选取k个最近邻节点。由于DiskANN将全精度向量与其邻域信息一同存储在SSD中（使用邻接表，并定长存储），因此当在查询过程中从SSD中获取某节点信息时，也会同步获取其全精度向量和邻域信息且无需额外的磁盘读取操作。\n\n    \n        SSD的最小读单位是4KB，读512B也要读4KB，所以读4KB并不比读512B慢。\n    \n\n\n\n\n具体结果通过实验对比Vamana算法与HNSW算法和NSG算法，得出结论Vamana在来自不同来源的百维和千维数据集上的性能与当前最好的ANNS算法相当或更优。\n对于基于SSD的服务场景中，跳数与搜索延迟直接相关， 每一跳都要对SSD进行读取，相对于HNSW和NSG，Vamana算法完成一次搜索所需的跳数更少，因此更具优势。\n通过对比十亿级别数据集上Vamana算法的单次全量索引和多次合并索引，得出结论虽然单次索引的性能优于合并索引，但与单次索引相比，达到相同召回率所需的额外延迟不超过20%。\n通过DiskANN和IVFOADC+G+P进行对比，在内存占用相同的情况下，DiskANN可以保持更高的召回率。\n","categories":["磁盘索引"],"tags":["ANNS","图索引"]},{"title":"Hexo教程","url":"/2025/10/30/Hexo%E6%95%99%E7%A8%8B/","content":"相关命令创建一篇文章$ hexo new &quot;My New Post&quot;\nMore info: Writing\n本地运行Hexo命令$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo s\ngithub推送$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d\ngithub推送源码$ git status #查看修改文件$ git add .$ git commit -m &quot;更新博客内容&quot;$ git push\n拉取远程更新$ git pull\n$ git fetch$ git status$ git merge\n本地启动$ hexo server\nMore info: Server\n生成静态文件$ hexo generate\nMore info: Generating\n远程部署$ hexo deploy\nMore info: Deployment\n","categories":["Hexo教程"],"tags":["hexo相关命令"]},{"title":"LM-DiskANN","url":"/2025/12/25/LM-DiskANN/","content":"《LM-DiskANN: Low Memory Footprint in  Disk-Native Dynamic Graph-Based ANN Indexing》\n\n\n    总结\n    \n        现有的基于图的ANN算法，如DiskANN，每个图节点包含的路由决策信息不完整，需要将其所有邻居的向量加载到内存中，过程中会产生大量I/O操作。基于此，本文提出了LM-DiskANN，其中每个节点存储完整的邻居信息用于路由，搜索期间的总I/O次数将与DiskANN相似，但内存占用要低得多。\n    \n\n\nLM-DiskANN节点布局对于每个节点，与DiskANN相比，LM-DiskANN在邻居ID之后立即存储所有邻居的压缩向量，如图所示。\n\nLM-DiskANN 磁盘节点的存储结构\n\n每个节点以其ID开始,然后是其向量,然后是所有邻居的ID。如果邻居数量少于最大邻居数量,剩余空间用0填充。每个节点以邻居的压缩向量结束,如果邻居数量少于最大邻居数量,后面也跟随填充。\n通过最大邻居数量控制，可以使得每个节点可以放入单个4KB页面中，与DIskANN相比，加载节点并不需要更多的I/O操作。\nLM-SearchVamana 采用了经典的BFS算法，LM-DiakANN的搜索算法构建在Vamana之上，同样使用 BFS 搜索策略。二者的主要区别在于：在搜索过程中，LM-Search方法直接从节点自身中获取邻域信息，而不是从一个独立的内存向量数组中读取（PQ压缩向量的获取）。算法如下：\n\n在搜索算法的每一次迭代中，从候选列表中选取当前尚未访问且与查询向量 q 距离最近的节点 p*，并将其所有邻居插入候选列表中。如果插入后候选集合的大小超过 L，则对候选集合进行裁剪，仅保留与查询向量 q 距离最近的 L 个节点。\n由于节点信息包含其完整的邻域信息，因此距离映射 D 的计算主要基于邻居的压缩向量；只有当节点被访问后，其与查询向量的真实距离才会被更新。\nLM-InsertLM-DiskANN的插入操作算法如下，该插入算法同样被应用于从零开始构建图索引。\n\n算法流程：\n\n使用 LM-Search 在现有图 G 中搜索与 p 最近的节点\n将搜索过程中访问到的节点 V 全部加入 p 的出边邻居集合中，\n对节点 p 的邻居集合进行剪枝操作。\n对于 ppp 的每一个邻居节点 n：\n将 p 加入 n 的邻居集合（建立反向边）\n对节点 n 的邻居集合执行剪枝操作\n\n\n\nLM-DeleteLM-DiskANN 删除操作算法如下：\n\n算法流程：\n\n将节点 p 标记为已删除。\n对 p 的每一个邻居n：\n将p的其他邻居（除n外），全部加入到n的邻居集合中。\n对节点 n 的邻居集合执行剪枝操作。\n\n\n\nLM-PruneLM-DiskANN 剪枝操作算法如下：\n\n算法流程：将候选集合 C 初始化为节点 p 当前的邻居集合，并清空节点 p 的现有邻居列表。随后，剪枝过程以迭代方式进行：在每一步中，从候选集合 C 中选取与节点 p 距离最近的节点 p*，将其加入 p 的邻居集合中；接着，从候选集合中移除所有满足以下条件的节点 p′。\nSearch Efficiency and I/O与Vamana算法在内存中存储一份所有向量的压缩副本相比，LM-DiskANN不再在内存中维护向量数组，而是通过在节点本身的信息之后立即附加每个节点的邻居压缩向量来增强每个磁盘节点。对所访问的某个节点，其所有邻域信息在一次I/O中一起加载。\n","categories":["磁盘索引"],"tags":["ANNS","图索引"]},{"title":"EdgeRAG","url":"/2025/12/11/EdgeRAG/","content":"《EdgeRAG: Online-Indexed RAG for Edge Devices》\n\n\n    总结\n    \n        对于边缘计算设备上的大语言模型，通过与RAG技术结合，可以利用本地个人数据来生成高质量生成。受限于边缘设备的内存和算力条件，本文提出了EdgeRAG，通过剪枝二级索引的嵌入向量并在检索过程中按照需生成嵌入向量，解决边缘设备上内存受限问题。从而使向量数据库适配有限移动内存的同时满足SLO需求，实现基于RAG的LLM移动化部署。\n    \n\n\n要点：存储高计算成本簇的嵌入向量、检索过程中按需计算嵌入向量，缓存高复用、高计算成本簇的嵌入向量\n研究背景和动机在资源受限的边缘设备上部署RAG系统，需要解决向量相似性搜索的开销问题，包括向量存储和向量检索。Flat Index计算量过大且内存利用率低；IVF索引虽然能通过聚类减少搜素范围，但当索引过大而无法全部存储在内存时，频繁的数据交换会造成性能急剧下降。其次，本文通过实验分析发现，数据访问模式与访问延迟存在高度偏斜：首先，大多数嵌入向量在检索过程中未被访问；其次，不同簇的嵌入向量生成成本差异极大且呈长尾分布。\n相关技术Retrieval Augmented GenerationRAG是一种通过将外部知识库与大型语言模型相结合来提高其准确性的技术，包括索引和查找过程。\n\nRAG流程图\n\n索引过程：1、对数据数据进行预处理，将其分成由节点表示的较小重叠数据块；2、每个数据块经过一个嵌入模型，将其转化为高维向量表示；3、将嵌入向量存储在向量数据库以便后续检索。\n查找过程：1、查询向量通过相同嵌入模型生成查询向量；2、进行向量相似性搜索，查找最相似向量；3、系统返回与查询向量最接近的嵌入向量的索引；4、最后，系统检索到与最匹配索引相关的数据节点，并将这些信息输入到 LLM 中，生成用户所请求的响应。\nInverted File IndexIVF 索引采用聚类算法将相似文档分组至不同簇中。\n\nIVF索引的检索过程\n\n检索流程：1、查询向量在首层索引中与各聚类中心进行比较；2、确定相关聚类后，在其二层索引中搜索最相似的嵌入向量；3、系统检索与该向量关联的数据，并将其返回给用户。\n核心思想EdgeRAG 通过选择性索引存储来减少索引的内存占用，然后预计算并存储那些大簇嵌入来缓解生成这些嵌入的长尾延迟问题。为了进一步优化延迟，EdgeRAG 选择性地缓存生成的嵌入，以减少冗余计算并优化检索延迟。\nSelective Index Storage 对 Second level Index 进行剪枝，会将嵌入生成从索引阶段转移到检索阶段，但如果生成延迟超过访问预计算嵌入，则会增加检索延迟。本文通过实验展示，大多数聚类的生成延迟低于500ms，但存在少量簇的生成延迟超过2s，呈现 tail-heavy 分布——少量聚类会对整体检索时间产生显著影响。\n为缓解大聚类中的长尾延迟问题， EdgeRAG 采用混合策略：在初始索引阶段，估算各聚类的嵌入生成延迟；识别出超出延迟阈值的聚类后，会预计算其嵌入并存储。查询时若存在预计算嵌入则直接从存储中获取，规避在线生成嵌入的高延迟阶段。对于未预计算的聚类，则实时生成嵌入。选择性索引存储算法如下：\n\n选择性索引存储算法\n\n算法流程：1、对所有数据块进行嵌入处理，获取对应的嵌入向量；2、然后对嵌入向量进行聚类，生成聚类中心集合；3、对每个嵌入向量，把其分配到最近的簇中；4、估算每个簇的生成延迟，生成延迟=簇中数据块总长度/生成延迟；5、判断生成延迟是否大于SLO需求，若是，则预计算并存储该向量。\nAdaptive Cost-Aware Caching在检索阶段，对于较小、较常见的聚类的嵌入有频繁的复用，且被访问簇存在显著重叠。对此，EdgeRAG缓存已生成的嵌入以避免冗余计算。\n为优化缓存利用， EdgeRAG 采用自适应成本感知缓存策略，避免缓存那些生成延迟较低的较小簇的嵌入。缓存所有嵌入可能导致命中率低下和延迟增高，而仅缓存高成本嵌入虽可提升命中率却会增加整体延迟。为此，EdgeRAG 避免缓存生成延迟低于动态调整的最小延迟缓存阈值的聚类嵌入，从而平衡缓存命中率和整体延迟。缓存策略如下：\n\n成本感知缓存替换算法\n\n算法流程：1、当收到缓存访问请求时，如果命中缓存，则对应簇的访问次数+1；2、否则，遍历缓存中每个簇，获取缓存中加权成本最小的簇，并从缓存中删除该簇。获取需访问簇的数据块并生成嵌入，然后将其放入内存；3、便利缓存中所有簇，对其counter进行衰减。\n对于缓存策略中缓存阈值的动态调整，算法如下：\n\n最小延迟缓存阈值算法\n\n算法流程：初始时，阈值设为 0，有效缓存所有簇嵌入向量。随后， EdgeRAG逐渐提高阈值，同时持续监控缓存命中率和检索延迟的移动平均。若发生缓存未命中且当前检索延迟低于移动平均值，则进一步增加阈值。反之，若未发生缓存未命中，则降低阈值。\n\n    \n        自我理解：缓存miss且当前的检索延迟低于移动平均值，说明系统在处理未缓存的查询时的表现是优于过去的平均水平。因此，系统会进一步增加阈值，以优先缓存那些生成成本高的嵌入。\n    \n\n\nEdgeRAG SystemRAG索引基于传统的双层 IVF 索引结构：第一级索引常驻内存，存储聚类中心点及指向第二级索引的引用；第二级索引存储文本块的引用以及所有数据块的嵌入生成延迟。EdgeRAG通过剪枝处理，仅存储高计算成本簇的嵌入，其他簇的嵌入在检索过程中按需实时生成。\n\nEdgeRAG索引流程\n\n索引构建流程：1、将整个文本分割较小的数据块；2、对每个数据块生成对应嵌入；3、对生成的嵌入进行聚类；4、将每个簇的聚类中心存储在第一层索引；5、将相关数据块分配给各自所属的簇；6、存储对相应数据块的引用；7、与传统IVF不同，EdgeRAG会计算生成每个数据块嵌入的计算成本。如果这种成本超过预先设定的阈值（服务水平目标，SLO），那么将完整存储该数据块的所有嵌入；否则，舍弃该嵌入以优化存储。\n\nEdgeRAG检索流程\n\n向量检索流程：1、首先查找与查询嵌入最相似的簇；2、随后检测该集群是否存在预计算嵌入；3、若存在，则直接检索存储的嵌入，并加载这些嵌入；4、否则，系统会查询嵌入缓存；5、若缓存命中，则从缓存加载嵌入并检索关联数据块；若未命中，则检索集群内所有关联数据块，重新生成嵌入并更新缓存，随后完成嵌入加载；6、最终系统查找最匹配的嵌入并检索对应数据块\n插入操作：\n\n将新的数据块的嵌入向量添加到最相似的簇中。\n如果添加后该簇的嵌入生成延迟超过 SLO，则重新生成并存储该簇中所有数据块的嵌入向量。\n\n当某个簇变得过大时，系统会将其拆分为多个较小簇，并将新生成的簇添加至一级索引中 。\n删除操作：\n\n从其所属的簇中移除要删除的数据块的嵌入向量，并更新簇索引。\n如果移除后该簇的嵌入生成延迟低于 SLO，则可以删除该簇中所有数据块的嵌入向量。\n\n簇规模过小时，可能会与相邻簇进行合并。此删除过程可异步执行，因为暂时保留小规模簇不会对检索延迟产生即时负面影响  \n","categories":["低资源设备"],"tags":["ANNS","边缘设备","IVF索引"]},{"title":"HM-ANN","url":"/2025/11/16/HM-ANN/","content":"《HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory》\n\n\n    总结\n    \n        本文基于异构内存（由PMem和DRAM组成）提出HM-ANN向量检索策略，综合考虑内存与数据的异构性，在无需压缩的情况下实现了单节点上的十亿级别的相似性检索。\n    \n    \n        具体技术：1、改进HNSW算法，引入自上而下插入和自下而上提升阶段：对于HNSW算法构建最底层进行保留并置于慢速内存中；从底层图中提升枢纽节点形成上层图结构并置于快速内存；2、通过动态迁移等内存管理技术，实现从慢速内存预取待访问数据到快速内存，并通过并行搜索降低慢速内存中的搜索耗时；3、建立性能模型以根据搜索时间与召回率约束自动选择超参数。\n    \n\n\n\n研究背景因内存容量限制，ANNS算法的设计需要平衡精度与效率。现有的基于SSD的解决方案会大大增加I/O操作时间。异构内存可以大幅度增加内存容量，以便ANNS算法同时兼顾精度和效率，为ANNS算法的设计提供了新机遇。\n异构内存\n异构内存HM由高容量内存技术和高性能内存技术构成：\n\n高容量内存：如NVM、SSD，容量大但访问延迟高。\n高性能内存：如DRAM、HBM，容量小但访问延迟低。\n\n具体技术HM-ANN通过改进HNSW算法，索引构建包括引入自上而下的插入阶段与自下而上的提升阶段。\n索引构建HM-ANN的索引基于HNSW图索引进行扩展构建，包括自上而下插入和自下而上提升两个阶段。自上而下阶段构建可导航的小世界图作为最底层（L0层）置于慢速内存；自下而上阶段从底层图中提升枢纽节点形成上层图（L1层…L层）结构置于快速内存，使多数搜索操作在快速内存中完成且保持高精度。\n\nHM-ANN索引构建过程\n\nTop-down insertions：使用HNSW算法自上而下构建图索引。\nBottom-up promotions：基于L0层节点，对L1…L层索引结构进行重建（L即为图索引的层深）。\n具体算法如下：\n\nHM-ANN索引构建算法\n\n参数解释：V：向量数据集；d：向量维度；M：插入节点在某层上建立的连接数；efConstruction：动态候选列表大小；Ni：第i层允许插入的节点数量；L：HM-ANN索引结构的层深。\n算法流程：\n\n首先利用HNSW算法构建图索引。然后，遍历V中的节点v，得到每个节点在L0层的度数集合Dv，并对D进行降序排序。并移除L1…L层的节点，然后根据后续步骤重建上层索引结构。\n对按照度数进行降序排序的节点进行遍历，从L1向上层进行重建：\n如果Ni==0，选取距离插入节点v最近的节点作为下次在上层插入节点的入口节点ep；\n否则，将节点插入该层，获取该节点的最近似邻居集合W，通过启发式选取邻居并与插入节点建立双向连接。\n如果邻居节点的出度大于M，则收缩连接数。该算法将L0层中度数最高的节点提升到L1层。从i（i&gt;=2）到i+1，HM-ANN以1/M的提升率将高度数节点提升到上层（M为该层节点的最大邻居数）。\n\n\n\n\nL0层图中的枢纽节点（hub point）是那些拥有大量连接（即高度数）的节点。高度数节点提供了更好的导航能力，节点间的大多数最短路径都经过枢纽节点。当选择度数最高的相邻节点作为下一跳时，导航路径的平均长度（即跳数）最小。通过提升高度数节点形成的L1层，使HM-ANN相较于随机提升策略能有效减少在L0层的搜索次数。\n\nHM-ANN 没有使用 HNSW 的 random selection for promotion，而是使用高度推广策略 high-degree promotion strategy，将第 0 层中度数最高的元素推广到第1层。对于更高的层，HM-ANN 根据 promotion rate 将高度数节点推广到上层。\n\nHM-ANN 将更多的节点从第 0 层 promote 到第 1 层，并为第 1 层的每个元素设置更大的最大邻居数。上层节点的数量是由可用的 DRAM 空间决定的。由于第 0 层不存储在 DRAM 中，因此使存储在 DRAM 中的每一层更密集，可以提高搜索质量。\n\n向量查询向量搜索阶段总共分为两部分：\n快速内存搜索：从顶层入口点开始，自上而下进行单步贪婪搜索。在L1层通过efSearchL1控制搜索过程返回的候选列表大小，列表的候选点作为L0层搜索的入口点。\n并行L0搜索：在L0层，HM-ANN将来自L1层搜索的候选点均匀分配，作为入口点，利用Thr个线程执行并行多起点单步贪婪搜索，最后合并得到最终结果，具体算法如下：\n\n并行L0层搜索算法\n\n在 L1 层搜索时，系统异步地将efSearchL1 候选点的邻居节点及其在L1层的连接关系从慢速内存复制至快速内存的迁移空间。当L0层搜索启动时，部分待访问数据已置于快速内存，从而缩短查询时间。\n对于参数efSearch_L1和efSearch_L2的选择，HM-ANN通过建立性能模型根据搜索时间与召回率约束自动选择超参数。\n","categories":["异构内存"],"tags":["ANNS","图索引"]},{"title":"MicroNN","url":"/2026/01/11/MicroNN/","content":"《MicroNN: An On-device Disk-resident Updatable Vector Database》\n研究背景个人设备蕴含着大量丰富的信息，而这些信息通常无法离开设备。因此，上下文化搜索与推荐应用需要具备能够在设备环境约束下运行的向量搜索能力。同时，在这种低资源环境中，内存资源高度受限，需要磁盘高效的索引结构和算法，并支持持续的插入和删除操作。\n另一方面，最先进的向量数据管理系统通常针对的是拥有充足内存的大型服务器可用、向量集合为静态且不支持插入和删除操作，并且向量相似性搜索通常独立于结构化属性过滤等其他搜索标准的场景。这类系统缺乏必要的优化，无法支持需要在低资源环境中运行、同时支持更新和结合向量相似性搜索与结构化属性过滤的混合查询的可扩展设备端向量搜索用例。\n基于上述，本文提出了一种面向低资源环境的嵌入式最近邻向量搜索引擎——MicroNN，以实现可扩展的相似性搜索和支持在用户个人设备上运行的应用程序。\nIVF索引索引构建\n建立IVF向量索引的过程包括三个主要步骤：K-means 聚类、向量分配和压缩编码（可选）。\n1、均值聚类：在数据集 X 上运行K-means聚类，将高维向量空间划分为nlist簇。k-means收敛后，得到的nlist中心点构成 IVF 的 “索引目录”。\n2、每个向量被分配到其中心点最接近的聚类，形成倒列表（List_i）。每个倒列表都存储了属于该聚类的所有向量的ID和存储信息。\n为了节省内存和加快计算速度，每个簇内的向量都可以进行压缩编码。\n向量搜索\n1、计算查询向量到所有中心点的距离\n2、选择最近的nprobe群集\n3、在选定的群集中搜索最近的邻居\n参考：zilliz博客 IVF向量索引\n设备端向量相似性搜索对于个人设备，需要专门的设备端索引与搜索架构来实现个性化推荐与服务，以保护用户隐私信息不被泄露。此外，对于非隐私数据，设备端索引也应能够改善搜素延迟，并在网络连接不佳时提供离线体验。\n挑战1、资源受限的环境：用户设备性能各不相同，索引和查询算法需能够在各种内存和处理能力的硬件条件下提供足够的性能。\n2、多租户环境（Multi-tenancy）：硬件资源由多个应用程序共享，内存使用被严格限制，除非正在服务于活跃的应用场景，否则索引无法长期缓存在内存中，因此索引和搜索系统必须是磁盘驻留的（disk-resident）。\n\n    \n        不同应用在时间上交替获得 CPU 和内存资源。向量搜索往往只是某个应用的阶段性功能（例如用户主动搜索时），在非活跃阶段持续占用内存既不经济，也不被系统允许\n    \n\n\n3、I/O效率的需求：长期高频率的写入操作会加速存储介质的磨损，因此磁盘I/O也是基于磁盘的算法的性能和基于闪存的存储设备寿命的关键因素。\n需求可更新性：用户设备上个人数据的持续变化特性要求向量索引支持动态更新。\n一致性：基于嵌入向量的设备端交互式上下文化搜索体验需要准确反映系统的当前状态。即使在存在并发写入和索引维护操作的情况下，每个读操作也应始终看到一致的索引状态。\n属性过滤：许多最近邻搜索的应用是在其他一些搜索条件的背景下完成的。设备端体验通常会为每个数据项关联额外的元数据，并且相似性搜索需要与其他属性约束一起执行。\n多样化的工作负载：除需要满足交互式查询的低延迟要求外，部分工作负载还要求在执行大量查询时具备较高的吞吐能力。因此，系统应能够高效地支持大规模批量查询的执行。\n现有方法的局限性尽管向量相似性搜索系统已取得显著进展，现有方法在设备端部署场景下面临一系列根本性限制。首先，多数系统在设计上假设内存资源充足，依赖主内存索引结构，难以适应用户设备中严格受限且多租户共享的内存环境。其次，现有方法普遍缺乏对动态更新与一致性的有效支持，难以应对个人数据频繁插入、删除和更新的特性，往往需要周期性重建索引，从而导致搜索结果滞后。此外，尽管少数系统支持实时更新，但其内存常驻的索引结构使其不适用于索引规模远大于可用内存的设备端场景。\n面向充足内存环境而设计\n现有向量相似性搜索系统大多以云端和 Web 规模数据集为目标，默认假设具备充足的主内存资源，因此主要采用内存常驻索引结构。这种设计与用户设备所面临的内存受限、多应用共享资源的现实环境不相适应。\n在设备端场景下，索引无法长期缓存在内存中，必须以磁盘常驻方式运行，但多数现有方法并未针对磁盘访问进行系统性优化。例如，LSH 类方法由于高冗余需求占用大量内存，而 FAISS、HNSWlib 等主流库仅支持内存索引；即便是支持 SSD 的 DiskANN，也仍依赖在内存中缓存全部向量的压缩表示。因此，这类方法在内存占用和架构假设上均难以满足低资源设备端向量搜索的实际需求。\n缺乏可更新性与一致性保证\n现有向量相似性搜索系统大多面向只读或近似静态的数据场景设计，难以适应用户设备上个人数据频繁变化的特性。树结构和图结构方法在插入和删除操作后容易出现结构失衡或质量退化，并且更新代价高昂；基于分区的方法虽更易支持更新，但随着数据变化，预计算的聚类中心会逐渐失效，而频繁重聚类在计算和 I/O 开销上不可接受。\n因此，许多系统依赖周期性全量重建索引，导致更新无法实时反映，搜索结果可能过期。尽管少数系统如FreshDiskANN和SPFresh等支持实时并发更新并保持索引质量，但其内存常驻的设计使其不适用于内存受限的设备端场景。\n批量查询优化不足\n现有向量相似性搜索系统大多以降低单条在线查询的延迟为主要目标，缺乏对批量查询场景的吞吐优化支持。除了为交互式应用提供低延迟的单次查询性能外，设备端向量相似性搜索算法还应能够高效地执行大规模查询批次。\n基于树或图结构的索引由于依赖遍历式搜索，难以在多个查询之间共享计算，因此通常只能逐条处理查询，无法进行有效的批量优化。相比之下，基于 IVF 的分区式索引在数据布局上更有利于并行化与计算复用。尽管已有工作（如 HQI）探索了多查询优化，但其仅适用于静态、内存常驻的索引，不支持更新。\n对混合查询支持有限\n现有向量相似性搜索系统主要围绕向量距离计算进行优化，而对与之结合的结构化属性过滤支持不足。在实际应用中，最近邻搜索往往需要在属性约束条件下执行，而属性谓词的选择性差异会对最优查询执行计划产生决定性影响。高选择性与低选择性谓词对应的理想执行策略截然不同，若选择不当，可能导致查询延迟显著增加或召回率严重下降。\n此外，真实应用中的查询通常包含复杂的属性约束，但现有系统对混合查询的支持仍较为有限，多数仅支持简单的数值比较或精确文本匹配，缺乏能够根据属性选择性进行自适应优化的查询执行机制。这一不足限制了向量搜索系统在工业级、设备端混合查询场景中的适用性。\n\n高选择性谓词（high selectivity predicate）：满足条件的数据非常少；低选择性谓词（low selectivity predicate）：满足条件的数据非常多。对于高选择性谓词，更合理的策略是：先用属性过滤快速缩小候选集合，再在很小的数据子集上做向量相似性搜索。对于低选择性谓词，更合理的策略是：先做向量相似性搜索，再对结果应用属性过滤。\n\nMicroNN系统概述MicroNN是一种专门为设备端向量相似性搜索设备的ANNS系统。MicroNN的索引与搜索算法均采用常驻磁盘设计，以满足设备端低资源环境设计。MicroNN 的索引算法支持实时更新，并为批量查询和混合查询处理进行优化。MicroNN的架构如下：\n\nMicroNN系统架构\n\nMicroNN使用SQLite关系数据库来高效存储向量及其关联的元数据。在索引方面，MicroNN实现了一种IVF索引结构，通过一种针对低资源环境优化的k-means聚类算法变体将向量空间划分为多个簇，以构成了索引的分区。通过在磁盘上以按照分区存储向量，MicroNN旨在提升分区访问时的数据局部性与I/O效率。此外，MicroNN 通过引入增量存储（delta-store）机制支持实时更新，从而实现并发读写操作。关系型数据存储的使用还使得索引重建过程能够与具备事务一致性的读取操作并发执行。系统通过索引监控模块跟踪更新过程中索引质量的变化，并在必要时触发重新索引。\nMicroNN 还实现了一个简洁的查询优化器，用于为结合ANN搜索与结构化属性过滤条件的混合查询选择高效的执行策略。针对混合查询，查询优化器基于属性谓词的选择性估计，在预过滤与后过滤两种查询执行计划之间进行选择。对于批量工作负载，系统实现了一种多查询优化技术，以摊销分区扫描成本并降低整体I/O开销。\n向量索引IVF索引使用向量量化算法如k-means对向量进行分区聚类。但最基础的IVF算法进行聚类需要将整个向量集合缓存在内存中，并不能直接应用于设备端的低资源环境。并且聚类算法可能生成大小不一的簇，会对查询性能处理产生不利影响。\nMicroNN提出一种为资源受限环境优化的k-means算法变体——mini-batch k-means，以减少索引构建过程的内存占用和引入灵活的平衡约束来减少分区大小差异。算法如下：\n\nMicroNN聚类算法\n\n参数：t：目标簇大小；s：mini-batch 大小，即每轮参与聚类更新的向量数量；n：聚类迭代次数；v[c]：第 c 个簇当前分配到的向量数量、v表示每个簇目前分配的向量数量；d[x]：缓存向量 x 在当前 mini-batch 中对应的最近聚类中心、d表示每个向量临时分配的的簇的结果\n算法流程：\n\n首先，根据目标分区大小，确定簇的数量k，并随机初始化聚类中心点。\n然后，进行迭代聚类：\n从 X 中均匀随机采样 s 个向量，对每个向量，在平衡约束条件下寻找最近的中心点。\n对每个向量，获取其找到的最近中心点，更新当前簇的向量数量，根据簇中向量数量计算每个簇的学习率，然后更新当前簇中心点位置。\n\n\n在迭代聚类完成后，初始化分区分配，对X中每个向量进行分区划分。\n\n通过该算法的执行，最终得到的索引具有更均衡的聚类簇，并且向量会分散到相邻的簇，避免形成巨型簇。算法第8行在平衡约束下寻找向量x对应的分配簇，12-13行计算每个簇的学习率，当簇越大，η 越小，新来的点对簇中心的影响越弱，中心逐渐稳定。\n为提升索引构建阶段的性能，MicroNN将一个批次中的向量表示为矩阵形式，并使用利用 SIMD 指令的硬件加速线性代数库来优化索引构建过程中向量相似度的计算。在算法生成最终的聚类结果后，系统会将聚类中心持久化存储，并在底层数据库中更新各向量所属的分区信息。\n物理存储MicroNN采用SQLite关系数据库作为底层存储系统。SQLite是一种自包含、可靠且轻量级的关系型数据库引擎，并且提供了强一致性与隔离性保证。MicroNN使用SQLite进行物理存储的优势：\n并发性：依托 SQLite 的并发控制机制，MicroNN 支持多个并发客户端的访问模式，包括一个执行 upsert、删除及索引重建操作的写入者，以及跨线程和进程的多个读取者。\n性能：SQLite 提供的顺序扫描吞吐量可与原生文件访问相媲美。\n聚簇支持：通过使用聚簇主索引，可以方便地将 IVF 索引的布局映射到关系型存储结构中。\n成熟性：MicroNN 直接继承了 SQLite 在数据持久性、隔离性和可恢复性方面的成熟保障，无需重新实现一个具备事务一致性的底层存储层来支撑向量索引。\nMicroNN直接管理底层SQLite数据库以控制向量及其元数据在磁盘上的存储。MicroNN将向量以二进制大对象（blobs）形式存储在一个关系表中，并以分区ID（由IVF聚类生成）、资产ID（由客户端指定，用于标识生成该向量的资产）以及系统内部生成的向量ID作为主键。聚簇索引确保了向量表的行在磁盘上是聚集存储的，从而为同一分区内的向量提供了数据局部性。在索引构建过程执行（重新）聚类操作后，向量表中的分区ID会被更新。\n\n    \n        资产 ID就是客户端为这个原始对象指定的唯一标识，用来表明“这个向量是由哪个对象生成的”。\n    \n\n\n聚类中心存储在一张独立的表中，该表的规模显著小于向量表，可在查询时通过扫描该表来找到与查询向量最接近的聚类中心。与具体应用场景相关的属性信息存储在单独的属性表中。每个向量均可关联一组属性值，最近邻查询可以在执行相似性搜索的同时，包含针对这些属性的关系型约束条件。\n\nMicroNN 数据库示例\n\n在上述示例中，首先对聚类中心进行扫描，然后扫描符合条件发分区中的向量记录。最后将 Vectors 表与 Attributes 表进行连接，根据Attributes表的过滤条件查找符合条件的向量并返回结果。\n向量相似性搜索MicroNN同时支持KNNS与ANNS。MicroNN改进传统IVF算法并增加了对增量存储的扫描。向量搜索过程采用两级优化策略，而非将查询向量与数据库中的所有向量逐一比较，具体如下：\n\nMicroNN向量搜索算法\n\n算法参数：查询向量q、查询限制k和要扫描的分区数n\n算法流程：首先，扫描聚类中心，计算各聚类中心与查询向量的距离，并根据距离选择与查询向量最近的n个分区。在扫描选定分区时，计算查询向量与该分区所有向量的相似度，并使用堆结构维护当前的Top-k最近邻结果。最终，算法按照相似度对结果进行排序，并返回最相似的k个向量。\n除了选定的n个分区外，增量分区（delta partition）始终会被纳入扫描范围，以确保新近插入的向量也能够被考虑在查询结果中。参数n的取值直接影响搜索效果：较大的n意味着扫描更多数据，从而获得更高的召回率，但也会带来更长的查询延迟。\n为进一步加速查询处理，MicroNN 引入了多项工程优化。 MicroNN 的查询处理流水线如下：\n\n\n系统对多个数据分区进行并行扫描，以充分利用可用的磁盘带宽。\n将距离计算任务分配给多个线程，从而发挥多核 CPU 的并行计算能力。每个线程维护一个本地的Top-k堆结构，在所有线程完成各自分区处理后，通过高效的并行堆合并操作生成最终结果。\n距离计算以向量批次的方式进行：将一个批次中的向量组织为矩阵，利用SIMD指令并行化查询向量与多个向量之间的距离计算。\n通过以矩阵乘法库所期望的格式将向量以blob格式存储在数据库中，系统避免了昂贵的数据编组（marshalling）开销，并将向量拷贝次数降至最低。\n\n在“写入数据库时”就把向量存成“计算时最理想的形态”，从而在查询时避免任何格式转换和多余拷贝，让向量可以被直接送进 SIMD 加速的矩阵计算中 \n*数据编组（Marshalling） 是指将内存中一个结构化的数据对象（比如一个程序中的类实例、结构体或复杂的数据结构）转换成一种“扁平化”的、适合存储或传输的格式（比如字节流、XML或JSON）的过程。*\n批量查询处理MQO即多查询优化，旨在复用并发运行的查询所需的数据工件（如基表扫描、索引扫描，甚至中间结果）。MQO能够显著提升基于IVF等分区式索引的批量ANNS吞吐量。\nMicroNN 实现了一种用于处理批量查询的MQO变体。给定一批查询，MicroNN 首先识别出每个查询需要访问的簇集合，并按分区对查询进行分组。然后，不再为每个查询多次扫描同一个分区，而是通过一次矩阵乘法来计算该分区内所有向量与相关查询之间的距离。这种方法将扫描一个分区的成本摊销到一批查询上，极大地减少了 I/O，并显著提升了摊销后的总吞吐量。\n混合查询支持 MicroNN通过允许用户定义的属性与向量数据一同存储，并在查询时应用关系谓词形式的属性约束，来支持混合查询。客户端定义的属性使用SQLite的b-tree实现进行索引。MicroNN允许在可过滤属性上创建全文索引（FTS）。客户端可以使用SQLite的FTS5搜索语法，将最近邻搜索与文本搜索结合起来。\nMicroNN 为结合属性过滤的最近邻搜索实现了两种不同的算法：\n后过滤（post-filtering） \n\n首先执行最近邻搜索，计算出最多 k 个结果。\n然后，将这些结果与 Attributes表进行连接，并应用约束条件来过滤结果列表。\n\n这种方法效率很高（属性过滤作为 ANN 搜索后的一个轻量操作），但可能会影响召回率，因为部分近似最近邻结果可能在过滤阶段被丢弃。一个重要的优化在于：当从数据库中检索包含向量二进制大对象（blobs）的IVF分区时，就应用连接和属性表过滤。这样，不满足谓词过滤条件的向量在进入top-K计算之前就被排除了，从而提高了效率。\n预过滤（Pre-filtering） \n\n在执行向量相似性搜索之前先评估属性谓词。\n从 Attributes 表中计算属性过滤条件，生成满足条件的资产 ID 集合\n对于每个满足属性过滤的资产ID，从 Vector 表中获取其向量，计算与查询向量的相似度，并用一个堆来维护top-K结果。\n\n预过滤会扫描所有满足属性过滤条件的向量，本质上是在属性过滤后的子集上执行一次暴力最近邻搜索，因此能够保证 100% 的召回率。然而，预过滤的延迟取决于满足过滤条件的结果数量，这个数量也被称为谓词选择性（predicate selectivity）。\n面向混合查询的查询优化器针对上述两种查询执行计划，可以通过计算选择性的估计值来决定采用哪一种策略。选择性因子F定义为满足某一谓词的元组数量与关系R总大小的比值：\nF=\\frac{|\\sigma_{predicate}(R)|}{|R|}当谓词的选择性因子较低（即仅筛选出极少量记录）时，称该谓词具有高选择性；相反，当选择性因子较高（即筛选出大量记录）时，则称其具有低选择性。F即是“满足条件的数据占总数据的比例，R即总向量数。\n若谓词具有高选择性，则预过滤是最优的查询执行计划：由于属性过滤后仅剩少量向量，暴力最近邻搜索在该子集上是高效的，同时还能保证 100% 的召回率。若谓词具有低选择性，则后过滤是更优的执行策略，先执行 ANN 搜索，再丢弃不满足属性条件的结果，其被丢弃结果的数量与谓词选择性成正比。\nMicroNN 实现了一个基于谓词选择性估算的简单查询优化器。与传统的关系型优化器不同，向量搜索的查询计划选择不仅影响延迟，还影响召回率。将IVF搜索视为一个对分区ID列进行过滤的谓词，并基于扫描分区数量与目标分区大小来估计其选择性。给定扫描的 IVF 分区数量n以及目标分区大小p，IVF谓词的选择性估计值F̂_IVF为：\n\\hat{F}_{\\mathrm{IVF}}=\\frac{n\\cdot p}{|R|}若估计得到的属性谓词选择性小于F̂_IVF，则说明属性过滤对搜索空间的约束强于IVF索引，此时应选择预过滤；反之，若属性谓词的选择性高于F̂_IVF，则IVF索引在收缩搜索空间方面更有效，后过滤策略更为合适。\n在选择性估计方面，MicroNN 独立估计每个属性谓词所筛选出的结果基数。为简化处理，假设谓词之间相互独立：对合取（AND）条件取最小值，对析取（OR）条件取求和值，以估计所有属性谓词组合后的总体基数，记为|σ̂_filters(R)|。最终，属性过滤的选择性估计F̂_filters定义为：\n\\hat{F}_{\\mathrm{filters}}=\\frac{\\min(|\\hat{\\sigma}_{\\mathrm{filters}}(R)|,|R|)}{|R|}查询优化器据此在F̂_filters &lt; F̂_IVF选择预过滤策略，否则选择后过滤策略。\n\n例如，谓词A：location = Seattle，估计匹配80条。谓词B：camera = iPhone，估计匹配70条。总数据R = 100。∣σ^(R)∣=80+70=150 &gt; R\n\n更新机制MicroNN 支持插入与删除操作，其中插入在数据库中已存在相同资产ID的情况下采用更新插入upsert语义。系统支持多个读取者以快照隔离的方式并发扫描IVF索引，而添加、更新、删除资产以及索引重建在内的所有写操作则被完全串行化执行。MicroNN将底层SQLite数据库配置为预写式日志模式，以提供ACID语义并防止向量数据损坏。资产表示“向量所对应的原始对象”\n插入操作通过将数据写入增量存储（delta-store）来完成。delta-store保存尚未被分配至任何分区的最新向量，新插入的向量会暂存于delta-store中，直至 IVF 索引被重新构建。由于在每次查询时都会对delta-store进行完整扫描，若delta-store规模过大，将导致查询延迟上升，因此需要通过周期性的索引重建，将delta-store中的内容刷新并合并至IVF索引中。\n尽管在逻辑上delta-store与IVF索引是分离的，但在MicroNN的实现中，delta-store在物理上与 IVF 索引是共存的。delta-store是通过分配一个保留的分区标识符来实现，从而使其向量在物理布局上与IVF分区中的向量采用相同的表示方式。采用统一的物理布局不仅简化了实现，也有助于保证delta-store向量的数据局部性。因此，在最近邻搜索过程中，delta-store可以被视为一个额外的分区进行处理。\n将 delta-store 中的变更合并进 IVF 索引的一个关键挑战在于：如何在不阻塞交互式应用的前提下完成这一过程，同时避免不必要的 I/O 操作，以保护用户设备中常见的固态存储介质。在具体实现中，MicroNN采用了一种简化的增量索引维护机制：通过将delta-store中的向量分配给与其最近的IVF聚类中心所在分区，并更新相应聚类中心，以逐步将增量数据合并至主索引中。这种向量分配方式可能导致分区规模不断增长，从而增加查询延迟。为防止查询延迟无界增长，MicroNN允许客户端为平均分区规模的增长设定阈值，当平均分区规模达到该增长上限时，系统将触发一次完整的索引重建。\n","categories":["低资源设备"],"tags":["ANNS","IVF索引","个人设备"]},{"title":"Milvus","url":"/2025/11/04/Milvus/","content":"《Manu: A Cloud Native Vector Database Management System》\n\n研究背景及现状对于非结构化数据，通常可以通过向量化表示（embedding）进行存储和检索。但随着数据科学和人工智能的快速发展，大规模高维向量激增。传统数据库主要处理结构化或基于倒排索引的文本检索，在高维向量检索方面性能不足；现有的Annoy等相似性搜索库更偏向于单机库，难以支撑企业级的大规模分布式向量和动态向量检索需求，并且它们能提供的功能有限，无法满足通用应用的要求，（缺乏数据库级的管理能力）如缺乏实时、高效的增删改能力和元数据过滤能力（元数据指的是与指的是和向量一起存储的、用于描述该向量的额外信息）。\n行业需求对于不同行业，需要设计一种通用的专门用向量数据的数据库系统，来支持对大规模向量数据进行快速查询处理和对动态向量数据插入、删除等操作。除了简单的向量相似性搜索，还需要如属性过滤、多向量查询等的高级查询处理。\n研究动机现有的针对向量数据管理的算法和系统主要集中于向量相似性搜索，但由于在大规模和动态向量数据上性能很差且功能有限，因此无法满足如属性过滤和多向量查询等行业需求。Milvus作为专为向量计算设计的开源数据管理系统应运而生，通过优化近似最近邻搜索、支持异构硬件加速、实现分布式扩展，并实现了对向量数据的数据库级的管理能力。\n具体技术系统概述Milvus是一个专门构建的数据管理系统，旨在高效存储和检索大规模向量数据，用于数据科学和人工智能应用。Milvus的系统架构总共分为三部分，包括搜索引擎、GPU引擎和存储引擎。\n\nMilvus系统架构\n\n特点\n\n专用设计：面向高维向量数据，遵循“one-size-not-fits-all”理念，而非扩展关系型数据库。\n丰富接口：提供多语言 SDK 和 RESTful API，便于应用集成。\n异构优化：针对 CPU 与多 GPU 架构进行了性能调优。\n多样化查询：支持向量相似性搜索、属性过滤、多向量查询。\n多种索引：支持量化索引、图索引，并具备可扩展接口。\n动态数据管理：基于 LSM 结构支持插入/删除，并保持实时一致性（快照隔离）。\n分布式部署：支持多节点分布式架构，实现扩展性和高可用性。\n\n系统设计向量查询\n在Milvus中，实体被描述为一个或多个向量和一些可选的数字属性（即用来描述实体的结构化数据）。\nMilvus支持向量查询、属性过滤和多向量查询类型，同时支持多种相似度计算函数。点击跳转到相应章节\n向量索引\n对于向量索引，Milvus支持基于量化的索引和图索引，同时支持添加其他新索引。\n动态数据管理\n采用LSM-tree日志合并树的思想支持高效插入和删除：\n新插入的数据首先存放在内存中的MemTable；Memtable大小达到阈值或者每隔一秒，Memtable被刷新到磁盘作为一个新的immutable segment；较小的segments通过tiered merge机制被选择合并成更大的segment来加速顺序访问。\n\n删除操作通过”out-of-place”的方式执行（不直接在原始数据上修改），merge操作时移除已删除的数据。\n数据更新操作通过删除+插入操作完成。\n默认情况下milvus只为超过1GB的segment构建索引，用户也可以对任何大小的segment手动构建索引。\n数据和对应的索引文件存放在相同的segment中。segment是milvus搜索、调度和缓存的基本单位。\nmilvus使用快照隔离机制保证读写操作共享一致性视图，避免读写互相影响。\n\n存储管理\n\n向量存储：Milvus对于单向量实体连续存储，并通过隐式row IDs和偏移量进行访问。对于多向量实体，通过列式存储保存实体的每个向量。\n属性存储：采用列式存储，每个属性列按照形式进行存储，key为属性值、vlaue为row ID。对于磁盘上的数据构建了skip pointers索引来加速列标量属性上的点查询和范围查询。\n缓冲池：Milvus假设所有数据和索引可放入内存，内存不足时按照segment粒度进行LRU缓存替换。\n多种存储系统支持：为了灵活性和可靠性，Milvus支持包括本地文件系统、Amazon S3、HDFS在内的多种底层存储方案。\n\n异构计算\nMilvus针对包括CPU和GPU在内的异构计算平台进行了高度优化。点击跳转到相应章节\n分布式系统Milvus按照分布式系统中的存储计算分离、共享存储、读写分离、一写多读的设计原则进行构建。\n\n异构计算面向CPU的优化缓存优化由于Faiss对于相似度查询的并行处理操作是将查询向量分配给线程对数据向量进行相似度计算，每个线程的数据向量无法被下一个线程复用，会导致数据向量频繁从主存中进行加载，造成大量CPU cache miss。在查询时，线程对数据集进行流式传输，一部分数据被加载进 L3；很快被新的数据替换掉；当下一个查询开始时，之前加载的内容已经不在 cache 中了；因此又要重新从主存加载同样的数据。\n\nMilvus的缓存感知设计\n\n具体技术：\nt：线程数；n为数据向量数；m为查询向量数；b为每个线程分配的数据向量数；s为查询块大小（每次批处理的查询数）。\n\n数据划分：每个线程被分配b=n/t个数据向量。\n查询划分：将m个查询向量数划分为s个查询块，使查询块能够驻留L3 CPU cache。\n批量比较：每个线程将分配的数据向量与查询块的每个查询向量进行距离计算。每个线程会为每个查询向量q维护一个heap，将得到的top-k存入其中。\n结果合并：将每个查询向量对应的heap分别进行合并，得到最终的top-k结果。\n\nMilvus选择将数据向量平均分配给查询向量，并将查询向量划分为s块，使查询块能够常驻L3 CPU cache。每个线程将分配的数据向量与查询块的查询向量进行距离计算。每个线程会为每个查询向量q维护一个heap，将得到的top-k存入其中。最终，将每个查询向量对应的heap分别进行合并，得到最终的top-k结果。\nSIMD优化SIMD是一种并行计算技术，主要用于在一条指令下同时对多个数据进行相同操作。\nFaiss实现了基于SIMD的算法用于加速向量相似性搜索，但并不支持AVX512。\nMilvus支持SIMD的四种指令集：SSE、AVX、AVX2和AVX512。SIMD指令多版本自动调度：对于每个常用函数（例如相似度计算）Milvus实现了SSE、AVX、AVX2、AVX512四个版本并放置在不同的源文件中，针对不容的SIMD标志可以选择不同的版本进行编译。运行时milvus根据当前CPU标志自动选择合适的SIMD指令，并使用hooking技术链接合适的函数指针。\n面向GPU的优化相比于Faiss的Top-k最大为1024，Milvus通过多轮迭代支持最大k为16384。Milvus第一轮类似Faiss计算出top1024个结果对应的距离和ID，记第一轮的最大距离为dl，第二轮milvus在剩下的距离大于等于dl的其他结果中挑选下一批1024个结果，如此迭代。\nFaiss的GPU数量在编译时固定，编译后的文件只能运行在GPU数量不少于编译时的服务器上运行。Milvus编译后的文件支持在任何服务器上运行。并且Milvus引入了分段调度，对于查询任务，将数据集以segment为单位在可用的GPU设备上进行调度，每个segment由单个GPU提供服务。\nGPU-CPU协同设计在GPU显存无法加载全部数据集的情况下，Faiss使用IVF_SQ8量化索引进行数据压缩并通过PCIe总线将数据从内存搬运到GPU显存中。该策略有两个缺陷：1.Faiss采用逐个bucket搬运的方式，PCIe带宽无法被充分利用；2.考虑到数据搬运延迟，在GPU中执行查询可能不是最优选择。\n\nSQ8H算法\n\n针对上述问题，Milvus提出了一种新型索引——SQ8H。\n\nMilvus在CPU和GPU间一次传输多个bucket，提高了IO带宽。Milvus采用LSM树，删除和更新不直接修改原数据，而是写入新位置，原数据标记为无效，多桶同时复制不会影响删除/更新逻辑。\nMilvus将大batch查询放在GPU执行，例如batch size超过1000时，Milvus将使用GPU执行所有查询并将涉及的数据同时多桶加载到GPU显存中。否则，milvus将采用混合执行的方案：1.将查找n个目标bucket的计算放在GPU中执行；2.在CPU中扫描每个bucket中的量化向量数据。动机：步骤1中的buckets中心体量很小，且批查询中所有查询都需要比较相同的buckets中心数据。步骤2中不同查询请求查询的量化向量较为分散。\n\n\n高级查询处理属性过滤\n不同的属性过滤策略\n\nMilvus基于已有的研究，实现了四种属性过滤方法。属性约束为C_A，向量查询约束为C_V：\n\n策略A attribute-first-vector-full-scan：使用C_A采用标量索引进行过滤，对剩余数据采用向量全表扫描机制。当C_A高选择性时（即过滤后的数据很少）采用；\n策略B attribute-first-vector-search：使用C_A过滤数据得到位图bitmap，使用C_V进行一般向量搜索，并查询搜索结果在位图中是否有效，有效结果存入top-k结果集中。当C_A、C_V都具有一定的选择性时使用；\n策略C vector-first-attribute-full-scan：先向量搜索，后全表扫描标量过滤，向量搜素通常输出θ*k（θ&gt;1）个结果保证最后可以查找到top-k个结果。C_V高选择性时使用。\n策略D cost-based：采用AnalyticDB-V中代价估计策略评估前三个策略的执行代价，选择代价最小的策略执行查询计划，适用性很强\n\n\n策略E partition-based：该方法时Milvus还提出的一种新的属性过滤方法。Milvus动态维护一个属性查询访问计数hash表。按照频繁访问的属性划分数据集，将策略4应用到每个数据划分。每个partition应该也维护了一个属性范围用于加速过滤。\nMilvus采用离线的方式对历史数据构建partition，构建完成后应用于在线查询；\n用户可以自定义分区数量，分区数量太大导致分区内向量数量过少，标量属性过滤趋近线性搜索。分区数量太小，分区内数据量增大，过滤无关数据效率变低。Milvus推荐每个分区内容纳1 million百万条向量。\n\n\n\n多向量查询朴素解决方案（最直接、未优化的做法）\n多向量查询的朴素方案是对查询实体的每个向量，分别在对应的数据集Di上执行一次单独的查询，等到Top-k候选结果，然后再对这些候选结果进行汇总与计算，得到最终的Top-k查询结果。其中，Di为所有实体再第i维度的向量vi（vi=(a1,a2,a3,……)）的集合。这种方法会丢失同一实体的多个向量之间的关联性，导致召回率非常低。\nMilvus提出了两种新方法：\n\n向量融合\n\n对于查询实体q，使用一个聚合函数对q的u个向量生成一个聚合后的查询向量。然后，将该聚合查询向量与数据集中存储的拼接向量进行相似度搜索，得到最终结果。方法简单高效，只需要执行一次向量查询操作，但需要使用可分解的相似度函数，如内积，对于一些相似度度量函数可以通过归一化进行转换为内积。\n\n迭代融合\n\n针对底层数据没有归一化，且相似度函数不可分解的情况，Milvus在对NRA算法改进的基础上，提出了迭代融合算法。该算法通过VectorQuery(q.vi,Di,k′) 来一次性获取 q.vi 在Di 上的 top-k′个查询结果，不需要为每次访问都重新进行向量查询，同时也消除了堆维护的高开销。\n\nIterative merging算法\n\n主要思路：\n\n该算法迭代地在每个Di上执行 top-k′查询，将结果放入Ri中。（其中 Di={e.vi∣e∈D}，即所有实体在第 i个向量维度上的集合）\n然后，在所有Ri上运行 NRA 算法。如果 NRA 能够完全确定至少 k 个结果，即可以安全地停止，那么算法就终止并输出 top-k 结果。\n否则，算法会将k’加倍（double），然后重新执行上述步骤，直到k’达到预设阈值为止。\n\n其他向量数据库向量数据库，也称为基于向量的搜索引擎或相似性搜索数据库，是专门为存储、检索和管理向量数据设计的数据库系统。它们最大的特点是能高效执行相似性搜索，即找到数据库中与查询向量最相似的向量。\n位图\nbitmap 是一种高效的且占用内存很小的 判断 某个值 存在与否的数据结构。它用二进制的某一位去表示某个值是否存在。void set(int k) &#123;  M[k &gt;&gt; 3] |= (0x80 &gt;&gt; (k &amp; 0x07)); &#125;\nK&gt;&gt;3：等价于K/8，K&amp;0x07：等价于K%8bool test(int k) &#123;  return M[k &gt;&gt; 3] &amp; (0x80 &gt;&gt; (k &amp; 0x07));&#125;NRA算法\nNRA算法主要用于解决Top-k问题，即在大量数据中找到前k个最优解。与传统的TA算法不同，NRA算法只需要顺序读取数据，因此在处理大规模数据时更为高效。该算法要求聚合函数是可分配的（distributive）和单调的（monotone）。\n","categories":["向量数据库"],"tags":["异构计算","向量数据库设计"]},{"title":"LEANN","url":"/2025/10/31/LEANN/","content":"《LEANN: A Low-Storage Vector Index for Personal Devices》\n\n\n    总结\n    \n        针对ANN向量索引存储开销大而无法部署在资源受限的边缘设备上的问题，该论文针对该问题提出了一种低存储的ANN向量检索策略——LEANN，LEANN通过不存储全部原始向量而在查询时对必要节点重新计算其向量表示的思想，使用即时重计算机制、两级遍历算法和高度数节点保留的剪枝算法等方法，在保证较高召回率和可容忍延迟的前提下，大幅度降低了向量检索在边缘设备上的存储开销。\n    \n\n\n研究背景目前基于嵌入的搜索（向量检索技术）已经被广泛应用在推荐系统和RAG中，个人设备对此也逐渐突显需求。但由于个人设备存储空间限制和ANN索引存储开销过大的原因，无法在个人设备中存储必要的索引结构。因此，减少索引存储开销和保持搜索质量和延迟成为了关键问题。\n行业需求随着智能手机等边缘设备越来越普及，这些设备不断采集并生成多模态数据（如图像、文字、音频等）。这些数据如果能被有效地检索、组合起来，对用户回忆、检索过去信息、辅助智能应用等都有潜在价值。\n边缘设备上向量检索的应用：设备端RAG系统、个性化推荐系统、图像与视频内容检索等。\n研究动机\n动态重算嵌入以节省存储：\n\n传统图索引（如 HNSW）在查询时只会访问少量向量，因此LEANN不再将所有嵌入存储在磁盘上，而是在查询时按需重算。为减少延迟，LEANN 采用了双层遍历算法（结合近似与精确队列）和动态批处理机制（提高 GPU 利用率），从而显著降低重算开销。\n\n图结构剪枝以压缩索引元数据：\n\n即使不存储嵌入，图索引的邻接信息仍带来较大存储负担。LEANN 通过分析发现，许多边和节点对检索效果贡献有限，因此提出了高出度保持的剪枝策略，去除冗余边、保留关键枢纽节点，从而在几乎不影响搜索精度的前提下，大幅减少索引体积。\n为此，本论文提出了LEANN，通过结合紧凑图索引和高效即时重计算，以实现最小的存储负担和快速准确的检索。\n解决思路在HNSW查询中，只有很少的节点会被访问，因此 LEANN 不再预先存储全部向量，而是对这些少量节点在查询时即时重算，从而大幅降低存储成本，同时保持搜索准确率和延迟在可接受范围内。\n\n重计算虽然能节省存储空间，但仍然会导致较高的延迟。\n并且剩余的图元数据可能仍然占比很大的存储开销。\n\n对此，LEANN 采用两级图遍历算法和动态批处理机制来降低重新计算的延迟，并使用一种高度数保留的图剪枝技术，以显著减少图元数据所需的存储空间。\n系统工作流程\nLEANN 系统架构与工作流程\n\n对于给定的数据集，LEANN首先计算所有条目的嵌入向量，并利用现有的图索引方法构建向量索引。然后LEANN丢弃所有条目的嵌入向量，并通过一种高度保留的图剪枝算法对图结构进行剪枝，以减少存储开销。该算法优先保留访问频繁的“枢纽节点”因为节点访问分布往往极度偏斜。\n在查询时，LEANN使用一个两级搜索算法在被剪枝后的图上进行遍历，识别并优先探索那些更有潜力的节点。这些被选中的节点随后会被发送到嵌入服务器（一个设备端组件，用于调用原始嵌入模型重新计算节点的嵌入）以获取对应的向量表示。\n为了进一步提高 GPU 利用率并减少延迟，LEANN 采用了一种动态批处理策略，用来调度在 GPU 上的嵌入计算任务。此外，当设备上有额外磁盘空间可用时，LEANN 会将这些“枢纽节点（hub nodes）”缓存在磁盘中。运行时，系统仅对未缓存的节点重新计算嵌入，而对已缓存的节点则直接从磁盘加载，从而减少计算负担并进一步提升查询性能。\n总结，LEANN在索引构建过程中使用高度数保留图剪枝算法（High Degree Preserving Graph Pruning），以减少索引存储开销。并在向量搜索过程中使用两级搜索算法（Two-Level Search）和动态批处理策略（Dynamic batching strategy）来提高查询精度和性能。\nTwo-Level Search主要思想：\n只需对近似距离排名靠前的向量进行重新排序（re-ranking），就足以获得较高的召回率，同时能有效剪枝掉在错误方向上的节点。\n\nTwo-Level Search 算法\n\n参数解释：\nAQ（Approximate Queue，近似队列），用于存储所有计算近似距离的已访问节点；EQ（Exact Queue, 精确队列），用于存放计算精确距离的节点；re-ranking ratio（重新排序比例），用于控制进行精确计算节点的比例。\n算法流程：\n从入口节点开始，计算其精确距离，将入口节点加入EQ、R和visited。如果EQ不为空，每次从EQ中取出距离查询向量q最近的节点v，从R中取出距离q最远的节点f。若 d_v-q &gt; d_f-q 则结束算法，否则遍历v的所有邻居，对每个未访问的节点n，将其加入visited，计算近似距离d_approximate，然后将节点n加入到AQ（队列中的节点会保存节点标识和对应距离，d_approximate的计算可以通过PQ实现）。然后从AQ中取出前a%的节点（最有潜力的节点），对这些节点计算精确距离d_exact，将其加入ED和R。\nDynamic batching strategy图索引每次扩展都要根据上次扩展结果选出最有希望的节点，会导致每次GPU只能处理很小一批数据，使得GPU资源浪费严重、吞吐低、延迟高。动态批处理策略放宽这种严格的数据依赖性，每次扩展从候选队列中一次取出多个节点进行扩展，将它们的邻居节点使用GPU一起进行重计算。这样计算更高效、延迟降低，扩展顺序虽然有延迟，但整体搜索结果几乎没有影响。\n\n\n每个节点平均有 8 个邻居；\n原始做法：每次扩展 1 个节点 → batch size = 8；\n动态批处理：一次从队列取 8 个节点 → 共 8×8 = 64 个邻居 → batch size = 64。\n\n\nHigh Degree Preserving Graph PruningLEANN引入了一个用户定义的磁盘预算C。如果图元数据超过这个阈值，LEANN会触发剪枝算法。\n主要思想：\n只要保持高度数的枢纽节点，即可维持搜索性能，在减少边数量的同时保持检索准确率。\n\nHigh Degree Preserving Graph Pruning 算法\n\n参数解释：\nG：原始图；V：节点集合；ef：候选集合大小；M：高度数节点允许的最大连接数；m：普通节点允许的最大连接数(m&lt;M)；a：高节点度数比例\n算法流程：\n\n计算G中每个节点v的度数Dv，并初始化G1用来存放剪枝后的图。\n\n从D中选出出度最高的前a%节点，记为集合V*\n\n对V中每个节点v进行计算\n调用Best-First Search算法得到v的候选邻居列表W，如果v属于V*，M0=M，否则，M0=m。\n通过原始启发式（d(p, p′)≤d(p, p′)这种启发式剪枝）从W中选取M0个节点作为v的邻居并建立双向边，并添加到G1。\n如果存在邻居的出边度数大于M，则进行剪枝出度边。\n\n\n其他边缘设备（Edge Device）\n边缘设备是指部署在网络边缘、靠近数据源（如用户、传感器、机器设备等）的一类计算设备。它们负责在本地进行数据采集、处理、分析或决策，而不是将所有数据发送到云端或数据中心。\nRAG（Retrieval-Augmented Generation，检索增强生成）\nRAG是一种结合了信息检索技术与语言生成模型的人工智能技术。该技术通过从外部知识库中检索相关信息，并将其作为提示（Prompt）输入给大型语言模型（LLMs），以增强模型处理知识密集型任务的能力。\n","categories":["低资源设备"],"tags":["ANNS","图索引","个人设备"]},{"title":"SmartANNS","url":"/2025/11/09/SmartANNS/","content":"《Scalable Billion-point Approximate Nearest Neighbor Search Using SmartSSDs》\n\n    总结\n    \n        传统的ANNS算法需要在内存中维护大量索引，导致难以扩展到大规模数据集。现有的可行解决方案基于普通SSD，但该方案会导致I/O操作在执行时间中占比很大，并且ANNS引擎与其他软件协同工作会竞争PCIe带宽，会导致性能瓶颈。\n    \n    \n    本文通过在“主机CPU + SmartSSD”协同架构下，通过NDP技术，将高开销的向量搜索任务在SmartSSD上执行，减少与主机CPU的数据传输并降低对主机资源的占用，显著提高大规模数据集的向量搜索性能。\n    \n    \n    具体技术：1、分层索引架构：使用HBC算法进行分片，并单独为每个分片建立图索引。分片质心存储在主机内存，分片数据和图索引存储在SmartSSD中；2、基于学习的分片剪枝：对于不同的查询任务，通过GBDT模型动态预测需要搜索的最优分片数量，避免无关分片的遍历，降低计算开销；3、动态任务调度：根据分片热度和数据布局，对查询任务进行调度，在多SmartSSD间实现负载均衡与数据重用；4、FPGA优化：在SmartSSD内实现高效的向量搜索核，采用并行距离计算、流水线优化、数据池与内核池机制，实现任务并行与延迟隐藏。\n    \n\n\n\n研究背景目前大多的ANNS算法大多依赖内存索引以实现快速精确搜索，由于要内存中维护的大量索引，这些算法对内存资源需求很大，导致ANNS服务难以扩展到大规模数据集。对此，现有的一种可行解决方案是将向量索引存储在SSD上，同时结合主机内存进行实现，如DiskANN和SPANN等。但基于SSD的ANNS方案具有一定的局限性和缺点：1、I/O操作在执行时间中占比很大；2、PCIe带宽有限；3、ANNS引擎与其他软件协同工作，会竞争PCIe总线的带宽。\n相关技术SmartSSDSmartSSD（智能固态硬盘）是一种集成可编程计算单元（如FPGA或SoC）的计算存储设备。SmartSSD 利用近数据处理（NDP）范式，借助其板载DRAM和现场可编程门阵列（FPGA）在存储设备内部处理数据，减少与主机CPU的数据传输并降低对主机资源的占用，从而提升系统性能与能效。\nSmartSSD的优势：\n\n采用NDP技术，可以进行本地数据处理。\n\n​    SmartSSD可以对存储在SSD中的索引执行本地查询，随后将部分结果返回主机进行聚合\n\nSmartSSD使用内部PCIe总线进行数据传输。\n\n​    可以实现多硬盘的近似线性加速、显著降低总线PCIe带宽竞争并减少主机CPU/内存资源消耗（主机只需要接受SSD处理的数据结果）。\nHBC算法HBC（分层平衡聚类）算法对数据集进行分片划分，通过匹配质心排除不相关分片，显著减少不必要的数据访问与计算。本文通过实验对HBC算法的数据访问模式进行探索。\n\n不同查询的数据访问模式\n\n实验证明HBC算法呈现显著的数据局部性和高度偏斜的不同分片访问热度，这为后续的任务调度实现提供了理论依据。\nGBDTGBDT（梯度提升决策树）是一种迭代的决策树算法，它通过构造一组弱的学习器（树），并把多个决策树的结果累加起来作为最终的预测输出。决策树是一种有监督(现有样本已知分类结果)的机器学习方法。\n在SmartANNS中，对于GBDT模型的训练，在构建训练数据时，先通过遍历搜索获得真实最邻近向量，然后判断这些向量落在哪些分片，得到分片数量N，并以此为监督信号来训练模型。在此过程中会偶尔记录分片热度并以此对SmartSSD进行分片分配。\n主要问题&amp;解决方案\nQ：由于多块SmartSSD 之间缺乏通信通道，每个SmartSSD必须搜索更多分片才能达到所需的准确度，导致额外的计算开销。因此，需要通过一个全局协调器查阅所有分片的质心来缩小搜索空间。\nQ：采用分层索引方法时，每个ANNS查询会被发送到一部分分片，导致不同分片之间的访问分布不均（偏斜）。一批查询如果调度不当，可能导致SmartSSD 之间的负载不均衡，最终损害系统的可扩展性。\nQ：由于查询的搜索范围可能存在显著差异，通常难以确定每个查询所需的最小分片数量。静态配置可能导致不必要的计算或精度损失。\n\nS：提出”主机CPU+SmartSSD”协同处理架构，利用分层索引大幅降低SmartSSD数据访问与计算量。\n\nS：基于优化数据布局的动态任务调度机制，同时实现负载均衡与数据复用。\nS：使用一种轻量级学习型分片剪枝算法GBDT，以消除SmartSSD上不必要的计算。\n\n整体架构SmartANNS利用分层索引实现”主机CPU+SmartSSD”的协同ANNS处理。\n\nSmartSSD 整体架构\n\n离线处理：SmartANNS同样以离线方式构建索引。SmartANNS首先采用HBC算法对数据集进行均匀分片，使得片内向量具有较高相似性。然后，为每个分片单独构建HNSW索引。分片的质心存储在主机内存中，分片本身存储在SmartSSD中。SmartANNS会采样部分训练查询样本来训练梯度提升决策树，以在运行时确定每个查询的最小搜索范围。\n在线处理：当到达一个ANNS查询时，SmartANNS 计算查询向量与各分片质心之间的距离。基于 这些距离，梯度提升决策树预测为找到近邻应当搜索的分片数量。随后，主机利用任务调度机制将查询任务分发到不同的SmartSSD。 当某个SmartSSD 接收到查询任务时，如果能够复用与其他任务一起已加载到其板载DRAM 中的分片，则会优先处理该任务。对于每个任务，SmartSSD 利用内部 PCIe 交换机将该分片的索引从 SSD 传输到FPGA的板载DRAM。随后，SmartSSD内的ANNS引擎开始迭代搜索，并将结果返回给主机。 最后，主机汇总来自不同SmartSSD的所有结果。\n系统整体流程：\n在离线阶段，SmartANNS通过HBC算法对数据集进行分片。然后，SmartANNS会采样部分训练查询样本来训练GBDT，同时偶尔记录过程中不同分片热度并以此对SmartSSD进行分配分片，使得每个SmartSSD的总分片热度大致相同。对于每个分片SmartANNS会单独构建HNSW图索引。分片质心存储在主机内存，分片数据和图索引存储在SmartSSD中。\n在线查询阶段，主机接收到查询任务，计算查询向量q和所有分片质心的距离，将距离进行排序并取相对距离Dk/D1。将查询向量、相对距离和分片总数输入到GBDT模型得到需要搜索的分片数量N。对于选定分片，主机通过key-value表来确定分片位置，然后打包查询任务，并由任务调度器进行任务调度。最后，由向量搜索引擎来完成后续的任务重排序和向量搜索任务。\n具体设计分层索引SmartANNS使用HBC算法对向量数据集进行尽可能均匀的分片，使得每个分片大小几乎相同。然后，系统会在对每个分片使用HNSW算法构建图索引。\nSmartANNS在主机内存中保存所有分片的质心，并维护一个key-value对来记录各分片在SmartSSD中的存储地址。\n在索引构建过程中SmartSSD使用公式$(1)$来解决聚类过程中的边界问题。\nv\\in S_i\\Leftrightarrow Dist(v,S_i)\\leq(1+\\varepsilon)\\times Dist(v,S_1) \\tag{1}分片剪枝与IVF算法相比，SmartANNS构建了更大的分片，质心数量更少。这样设计使得主要计算开销从质心遍历转移到了分片内部的搜索，减少主机CPU负担，充分发挥SmartSSD的计算潜力。对于不同查询使用固定的分片数量并不合理，这会导致无关分片被扫描，导致计算资源的浪费。为解决该问题，SmartANNS使用GBDT模型来动态预测每个查询需要扫描的最优分片数量。\nGBDT是一种集成学习算法，通过组合多个简 单的决策树（称为“弱学习器”）来构建一个强预测模型。它通过迭代训练：先进行均值预测，计算残差， 并将弱决策树拟合到这些负梯度上。在推理阶段，模型通过融合各个弱模型的加权贡献来生成预测。该模型的输入参数包括查询向量、其与分片的空间关系以及分片的总数。\n\nGBDT 输入参数\n\n对于GBDT模型，使用相对距离作为模型训练的特征：令D1表示查询与最近分片的距离，Dk表示查询与第k近分片的距离，使用Dk与D1的比值作为决策树训练的关键特征。在构建训练数据时，先通过遍历搜索获得真实最邻近向量，然后判断这些向量落在哪些分片，得到分片数量N，并以此为监督信号来训练模型。\n任务调度SmartANNS通过多块SmartSSD并行来实现NDP功能，并通过分片热度和任务调度来充分利用多块SmartSSD之间的数据并行性和实现不同SmartSSD之间的负载均衡。\n\n任务调度举例\n\n分片热度实验证明HBC算法呈现显著的数据局部性和高度偏斜的不同分片访问热度，这为任务调度实现提供了理论依据。\n\n在构建GBDT模型的训练数据时，记录不同分片热度。\n在将分片分配给 SmartSSD 时，按照热度对分片进行排序，然后迭代地将当前热度最高的分片分配给累积热度最低的 SmartSSD，从而最终使各 SmartSSD 的热度值大致相同。\n同时在不同SmartSSD 之间进行一次性的数据副本复制，当某个 SmartSSD 负载过高时，系统可以将任务迁移到另一个 SmartSSD。\n\n任务调度任务调度器的设计关键在于考虑查询之间的数据复用，动态平衡SmartSSD的负载。任务调度的具体算法如下：\n\n任务调度算法\n\n参数解释：Devices：可用SmartSSD设备；Tasks：需要处理的查询任务序列；Device_to_Tasks：每个设备被分配的任务列表。\n具体流程：\n\n对每个查询任务，找到与存储该任务相关的分片的所有候选设备Base。然后查找是否存在该任务能复用数据的设备集合Assign。\n如果没有可复用设备或所有相关设备均可复用，从 Base 中选择当前负载最小的设备 Target，把任务插入到该设备，并更新该设备负载。\n否则，将当前任务临时插入候选设备的任务列表 Temp，调用 Estimate(Temp) 计算插入后的预期完成时间或代价，并记录到 TimeInfo[j]。选择使估计时间最小的设备 Target，将任务真正插入，并更新设备负载。\n最终返回Device_to_Tasks。\n\n向量搜索内核向量搜索内核包括：层监控器、距离计算模块、排序模块和数组更新模块。在执行搜索时，向量搜索内核维护：已访问列表、已排序候选列表和已排序最终结果列表。搜索过程类似HNSW算法，调用距离计算模块。\n\n向量搜索内核\n\n\n\n对FPGA的优化：\n\n    并行读邻居\n    遍历候选向量时并行读取所有邻居，且通过独立接口并行加载向量数据与链表。（用 m_axi 适配器实现 MAXI-A、MAXI-B 两路封装，在 MAXI-A 通道中封装查询和向量数据；在 MAXI-B 通道中封装链接表与搜索结果）\n\n\n    访问标记压缩\n    用位图（bitmap）替代原 HNSW 的布尔数组以节省片上内存。\n\n\n    高效排序\n    实现了对硬件具有高度优化特性的bitonic sort（双调排序）算法来高效更新candidate与final列表\n\n\n    距离计算并行化\n    通过HLS的循环展开与流水线、多个处理单元（PE）并行计算高维向量间距离。\n\n\n    数据/内核池化\n    在板载DRAM设数据池（data pool），在 FPGA设内核池（kernel pool）。当一个shard被加载到数据池后，多个内核可以并行处理该shard的任务；同时可在一个shard被搜索时后台加载另一个shard，从而重叠加载与搜索，隐藏数据访问延迟并提升任务并行度\n\n\n\n其他近数据处理技术\n近数据处理技术（Near Data Processing，NDP）是一种将计算操作尽可能靠近数据存储位置的策略，目的是通过减少数据移动的成本（如带宽、延迟、能耗）来提高系统效率。在传统的冯诺依曼架构中，存储器中的数据需要搬移到内存中后CPU才能执行计算操作，对数据库等数据密集型应用很不友好。因此近数据处理技术的核心思想是利用存储器中的处理能力对数据进行简单处理（例如数据库的筛选操作）后只传输数据处理结果到主机中，节约了大量的系统资源，降低了时延和能耗。\n计算存储设备\n计算存储设备（CSD）是一种在存储设备内部集成可编程计算资源的创新架构，是NDP技术的具体实现。CSD使部分数据处理任务能够在存储端“就地执行”，从而减少数据搬运开销，提高系统整体性能与能效。\n现场可编程门阵列\n现场可编程门阵列（FPGA）是一种可完成通用功能的可编程逻辑芯片，即可以对其进行编程实现某种逻辑处理功能。\nHigh Level Synthesis\n高层次综合（High Level Synthesis，HLS）是把高级语言（如 C/C++/SystemC、OpenCL）描述的算法自动转换为硬件描述（RTL：Verilog/VHDL/HDL）的工具和方法。最终目标是生成可在 FPGA 或 ASIC 上实现的硬件电路。\n","categories":["近内存计算"],"tags":["ANNS","图索引","计算存储设备"]},{"title":"Manu","url":"/2025/12/11/Manu/","content":"《Milvus: A Purpose-Built Vector Data Management System》\n\n要点：系统架构、可协调一致性、功能分层解耦、日志即数据、发布/订阅模式、索引构建和向量查询、硬件优化、系统恢复\nMaun介绍Manu是Zilliz团队提出的下一代云原生向量数据库系统架构，是对Milvus的架构演进与理念升级。Manu作为Milvus在云原生体系下的演进而设计，Milvus在2.0版本以后逐步实现Manu提出的架构功能。\n\nManu使用“日志即数据”的设计理念，将整个系统构建为一组日志发布/订阅微服务，即Pub/sub服务模式。\n设计目标：持续演进能力、可调一致性、良好的弹性、高性能以及高可用性。\n设计理念：放松事务复杂性以换取可调一致性和细颗粒度弹性。\n\n系统实现数据库设计Schema：模式是定义数据类型和数据属性的元信息。\n\n模式示例：ID+feature vector+label+numerical attribute+LSN(logical sequence number)\nlabel：用于标识实体的类别，如食物、书籍等。\nLSH：逻辑序列号，对用户隐藏的系统字段。\n\n\nSchema示例\n\nCollection：与关系数据库的table类似，用来存储向量和相关元数据。Collection之间没有相关性，不能进行连接等关系代数操作。\nShard：分片对应于插入/删除通道。在执行插入/删除操作时，系统根据实体的主键对其进行哈希，将其分配到相应的分片中。Shard并不存储数据，而是使用segment进行数据的存储\nSegment：段是一个自动创建的数据文件，用于存储插入的数据。一个 Collections 可能包含多个分段，每个分段可以容纳许多实体。\n\nCollection——Shard：一对多Shard——Segement：一对多\n\n系统架构采用面向服务设计，从上到下依次分为访问层、协调层、工作层和存储层。\n\nManu架构\n\nAccess Layer：Access layer访问层由若干个无状态的 proxy 组成。这些 proxy 负责接收用户请求，将处理过的请求转发给系统内的相关组件进行处理，并将处理结果收集整理好之后返回给用户。\nCoordinator layer：负责管理系统状态，维护系统元数据以及协调各个功能组件来完成系统中的各类任务。\n\nRoot coordinator：处理数据定义请求（如创建和删除集合），并维护collections的元数据。\nData coordinator：记录collections的详细信息（如segments的存储路径），并借条数据节点将更新请求转为linlogs。\nQuery coordinator：管理数据节点状态，调节segments（及其相关索引）到查询节点的分配以实现负载均衡。\nIndex coordinator：维护索引的元信息（如索引类型和存储路径），并协调索引节点的索引构建工作。\n\nWorker layer：负责真正的执行系统中各类任务，包括query node、data node和index node。query node用于查询处理、index node用于索引构建、data node用于日志归档。\nStorage layer：持久化系统状态、元数据、集合以及相关索引数据。\n\nManu 使用高可用的分布式 KV 系统（如 etcd）来记录系统状态和元数据。对于用户数据和索引数据等规模较大的数据，Manu 采用对象存储服务（如 S3）进行管理。\n\n日志系统Manu将所有修改系统状态的请求记录到日志中。logger作为将数据发布到WAL的入口点。\n\nManu日志系统的详细结构\n\n具体流程：对于实体插入操作，系统首先通过访问层（Access Layer）的代理接收插入请求，并对实体进行合法性验证。然后，系统使用一致性哈希算法计算实体ID所对应的逻辑桶（Logical Bucket），从而确定该实体所属的分片（Shard），并将请求分配给负责该分片的Logger。Logger向时间戳服务（TSO）获取该实体的全局日志序列号（LSN），以确保系统内写入顺序一致。接着，Logger决定该实体应存放的 Segment，并将该实体与Segment的对应关系写入本地LSM树中。LSM树的增量数据会周期性地刷新（flush）到对象存储中，以SSTable（RocksDB格式）的形式持久化保存实体–segment 的映射。同时，Logger将实体写入其负责的WAL通道（Write-Ahead Log Channel）并发布。数据节点（Data Node）订阅相应的WAL通道，默认周期读取日志数据，并将行式（row-based）的WAL记录转换为列式（column-based）存储的binlog文件，以便后续索引与查询模块使用。  \n\n每个logger将自己管理的shard中数据segment映射关系对应的SSTable从对象存储中缓存到本地。\nWAL基于云的消息队列实现。数据定义和系统协调信息使用自己的channel，数据操作请求被哈希到多个通道增加吞吐量。数据节点订阅WAL，并将基于行的日志转换为基于列的binlog。WAL中相同属性的值在binlog中以列的形式保存。索引节点只需从binlog中读取需要构建索引的列数据，减少了读放大。\n\nManu组件之间消息也是通过日志进行传递的。当segment被写入存储时数据节点将事件写入日志，当索引构建完成时索引节点将事件写入日志。\n可协调一致性Manu采用delta一致性模型来权衡性能和一致性，保证查询所看到的数据陈旧程度是有界的。\nManu的每个时间戳包括物理时间戳和逻辑时间戳两部分。物理时间戳为请求被Manu接收到的物理时间；逻辑时间戳用于区分在同一物理时间内发生的多个事件。\nManu使用time-tick机制来表示系统数据同步进度（整个系统中数据处理或事件发生的进度）。\n对于日志订阅者（如查询节点），若实现delta一致性模型，需要获取1.用户定义的可容忍陈旧程度τ（dalta值）；2.最近一次数据更新时间（最近一次系统同步的时间）；3.当前查询请求的发起时间。设日志通道最近的time-tick为Ls，查询请求发起时间为Lr，若Lr-Ls&lt;τ不满足（即数据之后超过用户设定的容忍时间），查询节点会等待下一个 time-tick，再执行查询，以保证数据“新鲜度”符合要求。\n索引构建Manu支持批量索引和流式索引构建两种方式。当用户为整个collection构建索引时使用批量索引；当用户连续插入新实体时使用流式索引，并且索引在不停止搜索服务的情况下异步构建。\n\n批量索引构建：当用户为整个collection创建索引，index调度器从数据节点获取集合内所有segment的路径，并通知索引节点为每个segment构建索引。\n流式索引构建：用户持续性的插入新数据，索引以异步方式构建，不会阻碍搜索服务。具体来讲，当segment接收新数据并到达指定大小，其所在的数据节点将此segment变为sealed状态并作为binlog写入对象存储。此后，数据调度器通知索引调度器，索引调度器通知索引节点对此segment构建索引。索引节点从对象存储中读取索引构建需要的列并构建索引。对于数据删除，Manu使用位图记录删除的向量，并在删除数据量达到一定阈值后重新为segment构建索引。\n对于批式索引构建和流式索引构建，当索引构建完成后，索引节点将索引文件持久化到对象存储，并将索引文件路径发送到索引调度器。索引调度器会通知查询调度器以便查询节点可以加载向量查询需要的索引文件。Manu也会在适当的时候在多个segment之上构建联合索引。\n\n向量搜索和Milvus一样，Manu支持经典向量搜索、属性过滤和多向量搜索。\n为了并行的处理数据查询请求，Manu 将数据集中的数据划分成固定大小的数据分片，并将这些数据分片放置在不同的 query node 上。\n查询节点从WAL、索引文件和binlog来获取数据。查询节点订阅WAL来获取新插入的实体数据，但对于新增实体进行暴力全量搜索代价较大，为解决该问题，将每个segment划分为多个slice，并对slice构建轻量临时索引（如IVF-FLAT）。当segment由growing转化为sealed时，会使用索引节点对其构建索引来替代临时索引。查询节点会在segment分布发生变化时，从binlog中访问数据，这种变化可能出现在系统伸缩、负载均衡、查询节点故障与恢复等场景。\n\n对于向量搜索，Manu将一个集合划分成多个segments并将segments分散到多个查询节点并行查询。代理proxy通过询问查询调度器获取查询节点上的segments分布，并将查询请求发送到持有对应segment的查询节点。查询节点在本地执行两阶段查询，对于top-k查询，查询节点首先获得segment粒度的top-k结果，然后内部汇总得到query节点粒度的top-k结果。然后代理proxy将收集多个节点粒度的top-k结果进行合并，返回给应用程序。对于删除操作，查询节点使用位图记录每个segment中删除的数据并从segment粒度的top-k结果中去除已被标记删除的数据。Manu也支持批量向量查询，当之前的批查询请求结果尚未返回时，查询节点在cache中对当前批查询请求进行归类，将对相同集合，使用相同相似性函数的查询请求组织为一个批次，提高处理效率。\n查询节点获取数据的渠道有三个：WAL、索引文件、binlog。对于growing segment中的数据，查询节点订阅WAL并以扫描的方式进行搜索。大segment有利于已经创建索引的数据查询，但是growing segment中的数据查询代价高。为了实现权衡，Manu将segment内部又划分为多个slices（默认包含1w个向量）。新数据被顺序插入到slice中，slice满了之后将构建轻量化索引（如IVF-FLAT）。当segment写满，构建好索引并存储到对象存储中后，索引节点被通知将索引加载到内存替换slice的临时索引。当查询节点间的segment分布因为系统节点scale、负载均衡、查询节点故障重启等发生改变后，查询节点需要访问binlog。如果查询节点故障，其负责的segment和对应的索引将被加载到其他索引节点。\n\n其他LSH算法和多哈希表策略在 LSH 中，单哈希表策略是指选取 kkk 个独立的局部敏感哈希函数，将它们组合用于同一个哈希表中。对于每个向量，通过这 kkk 个哈希函数生成一个 kkk-维哈希码（或称哈希标签），并根据该哈希码确定该向量所属的桶（Bucket），再将向量的标识符插入对应桶中。使用多个哈希函数的目的是降低不同向量发生哈希碰撞的概率，使得仅当向量高度相似时才更可能落入同一桶内。\n然而，仅使用单哈希表会存在“边界效应”问题，即某些相似向量可能因映射边界差异而被分到不同桶中，从而降低召回率。因此，LSH 引入了多哈希表策略：为每一个哈希表分别选取一组新的 k 个哈希函数，并建立多个独立的哈希表。对于每个向量，将上述过程在所有哈希表上重复执行，使得向量被插入多个桶中。多哈希表机制提升了相似向量在至少一张表中被映射到同一桶的概率，从而有效改善查全率（Recall）。\n对于多哈希表的向量检索，对查询向量，对于每一个哈希表，使用该表的k个哈希函数计算哈希签名，找到对应的桶编号。然后从对应的桶中取出候选向量进行相似度计算，得到候选结果。最后，对于所有的候选结果进行合并去重，得到最终的最近邻查询结果。\n聚类边界问题k-means 聚类是基于全局分布的划分，但查询是局部相似性判断。这样会导致两个相似的向量在与聚类中心进行距离计算后，被分配到不同的桶（聚类簇）中。而查询的粗过滤又是通过查询向量与聚类中心进行距离比较，从而导致可能漏掉一些潜在的近邻向量，造成低召回率。\n","categories":["向量数据库"],"tags":["异构计算","向量数据库设计"]},{"title":"SPANN","url":"/2025/11/11/SPANN/","content":"《SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search》\n\n研究背景目前针对超大规模向量检索场景的混合ANNS方案，如DiskANN、HM-ANN，都是基于图的解决方案。本文论证了简单倒排索引方法同样能在大规模数据集上实现召回率、延迟与内存成本的顶尖性能。\nSPANN概述不同于以往基于倒排索引且依赖有损数据压缩来降低内存开销的方法，SPANN采用了一种简洁的内存-磁盘混合方案。\n索引结构：数据向量X被划分为N 个倒排列表${\\mathbf{X}_1,\\mathbf{X}_2,\\cdots,\\mathbf{X}_N}$，$\\mathrm{X}_1\\mathrm{X}_2\\cup…\\cup\\mathrm{X}_N=\\mathrm{X}^3$。这些倒排列表的质心$\\mathbf{c}_1,\\mathbf{c}_2,\\cdots,\\mathbf{c}_N$存储在内存中作为快速粗粒度索引，指向磁盘中对应倒排列表的位置。 \n局部检索：当查询向量q到来时，我们寻找K个最近邻质心\\{c_{i1}, c_{i2}, \\ldots, c_{iK}\\}, K \\ll N，并将这K个最近质心对应的倒排列表X_{i_1}, X_{i_2}, \\cdots, X_{i_K}中的向量加载至内存，以进行后续细粒度检索。\n主要挑战challenge 1倒排表长度均衡问题：对于存储在磁盘上的倒排表，为减少磁盘访问次数，需限制每个倒排列表的长度，使其仅需数次磁盘读取即可载入内存。这不仅要求将数据分割为大量倒排列表，还需保持各列表长度均衡。不均衡的倒排列表将导致查询延迟（尤其当列表存储于磁盘时）呈现高度方差。 \nchallenge 2聚类分簇的边界问题：查询向量q的最近邻可能分布在多个倒排列表的边界区域。由于仅搜索少量相关倒排列 表，位于其他列表中的真实近邻将被遗漏。\nchallenge 3搜索过程中需要查询的倒排表数量问题：不同查询任务具有不同搜索难度。部分查询仅需搜索1-2个倒排列表，而部分查询需搜索大量列表。若对所有查询均搜索相同数量的列表，将导致低召回率或高延迟。\n主要技术针对上述挑战，本文给出了相应的解决方法。\nSolution 1对于数据分区，本文采用分层多约束平衡聚类算法。通过多次迭代地将向量聚类为少量簇，直至每个倒排表包含有限的向量数量。分层平衡聚类将大簇（黄色簇）中的向量迭代平衡划分为少量小簇（绿色簇），直至每个簇仅包含有限数量的向量（蓝色簇）。\n\n分层平衡聚类\n\n为使查询过程中查询向量与质心的距离计算更具意义， 采用最接近质心的向量替代质心来表示每个倒排列表。由此，冗余的导航计算转化为对真实候选子集的距离计算。为加速查询，为代替质心的向量建立SPTAG索引。SPTAG 通过构建空间划分树和相对邻域图作为向量索引，可将最近质心搜索加速至亚毫秒级响应时间。\n\nSPTAG索引\n\nSolution 2为解决边界问题，本文使用闭包多簇分配方案：\n\n若向量与多个聚类中心的距离几乎相同，则将该向量分配给多个最近的聚类，而非仅分配给最近的一个，如公式$(1)$：\n\n\n\\begin{aligned}\n\\mathbf{x}\\in\\mathbf{X}_{ij} & \\Longleftrightarrow\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{ij})\\leq(1+\\epsilon_1)\\times\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{i1}), \\\\\n & \\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{i1})\\leq\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{i2})\\leq\\cdots\\leq\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{iK})\n\\end{aligned}\n \\tag{1}\n聚类闭包分配\n\n若边界向量（绿色点）与多个聚类簇（蓝色与黄色聚类簇）的距离近乎相等， 则将其分配至多个最近邻聚类簇。\n\n为避免相邻倒排表中重复向量过多，导致过多不必要的计算操作和硬盘读取操作。论文采用RNG规则优化闭包聚类分配，即为边界向量选择多个代表聚类，以降低相邻倒排列表的相似度，如公式$(2)$：\n\n\n\\mathrm{Dist}(\\mathbf{c}_{ij},\\mathbf{x})>\\mathrm{Dist}(\\mathbf{c}_{ij-1},\\mathbf{c}_{ij})\n\\tag{2}核心思想：相邻倒排列表更易被索引同时召回。相较于在相邻列表中存储相似向量，存储差异化向量更能提升在线搜索的可见向量数量。\n\n    \n        自我理解：将聚类中心与边界向量的距离进行排序得到列表Cij，如果Cij与Cij-1的距离小于Li与边界向量的距离，说明两个聚类具有高度相似性，则跳过边界向量加入该聚类。\n    \n\n\n\n\n\n\n\nRNG规则优化\n\n运用RNG规则降低两个相邻倒排列表的相似度。橙色点将被分配给蓝色与灰色倒排列表，尽管其与黄色列表的距离比灰色列表更近。\nSolution 3为有效处理多样化查询，提升召回率和降低查询延迟，本文使用查询感知的动态剪枝技术。根据查询向量与质心向量的距离，动态缩减需要搜索的倒排表数量，如公式$(3)$：\n\n\\begin{aligned}\n\\mathbf{q}\\overset{search}{\\operatorname*{\\operatorname*{\\longrightarrow}}}\\mathbf{X}_{ij} & \\Longleftrightarrow\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{ij})\\leq(1+\\epsilon_2)\\times\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{i1}), \\\\\n\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{i1}) & \\leq\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{i2})\\leq\\cdots\\leq\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{iK})\n\\end{aligned}\n\\tag{3}不再为所有查询统一搜索最邻近的K个倒排列表，而是仅当某倒排列表质心与查询的距离趋近于查询到最近质心的距离时，才动态判定该列表需要被搜索。\n","categories":["磁盘索引"],"tags":["ANNS","IVF索引"]},{"title":"Pyramid","url":"/2025/12/25/Pyramid/","content":"《Processing-In-Hierarchical-Memory Architecture for  Billion-Scale Approximate Nearest Neighbor Search》\n\n    总结\n    \n        基于近内存计算架构NMC，针对十亿级别数据集，本文提出一种基于图的ANNS的分层内存处理架构——Pyramid。Pyramid结合了主存储器级NMC的高内部带宽优势和存储级NMC的容量优势，在满足存储需求的同时显著提升搜索性能。\n    \n\n\n\n研究背景近内存计算（NMC）架构在提升大数据处理性能方面展现出巨大潜力。NMC架构将计算单元放置在内存（如双列直插式内存模块，DIMM）或存储设备（如固态硬盘,SSD）内部，以利用高内部带宽并减少存储、主存和CPU之间的数据传输。但现有的NMC架构在处理基于图的ANNS算法时面临两个严重问题：（1）主存级NMC的内存容量（如64GB）无法满足十亿规模数据集ANNS的存储需求（如800GB），导致主存储器与存储设备之间产生大量数据传输；（2）不规则且细粒度的图访问与页级读取粒度之间的矛盾阻碍了存储级NMC的吞吐量。\n相关技术近内存计算近内存计算（NMC, Near-Memory Computing）是一种新型计算架构，旨在解决传统冯·诺依曼架构中的“内存墙”问题。NMC将计算单元放置在靠近内存的位置,而不是像传统架构那样将数据在处理器和内存之间长距离传输，以显著减少数据移动的延迟和能耗。近内存计算可以根据部署位置分为主存级NMC和存储级NMC两大类：\n主存级NMC：主存级NMC（main memory level NMC）是指将计算单元部署在主存（如DRAM）附近，利用内存的高内部带宽来加速计算，减少数据在CPU和内存间的数据传输。\n存储级NMC：存储级NMC (storage level NMC) 是指在SSD内部集成计算单元，利用存储的大容量优势来处理无法完全放入主存的海量数据，在存储端直接完成计算,减少与主存间的数据传输。\nPyramid概述为解决基于图的ANNS中存在的大量不规则数据访问问题，本文提出了Pyramid架构，旨在通过充分利用内存内部带宽和减少数据传输来提高吞吐量。\n\nPyramid架构\n\n本文提出了一种基于分层图聚类的ANNS方法（Hierarchical Graph-Cluster-based ANNS），如上图(b)。该方法对特征数据集执行聚类，并仅使用聚类中心来构建图。从而将大规模图上的不规则数据访问被转化为小规模图上减少了的不规则访问，以及簇内规则的顺序数据访问。\nPyramid主要由两部分构成：主存级 Pyramid-M 与 存储级 Pyramid-S。通过结合Pyramid-M的高内部带宽和Pyramid-S的容量优势，Pyramid-M在DIMM中对聚类中心图执行基于图的ANNS，利用集中式排序和分布式距离计算来降低近内存计算的硬件开销；Pyramid-S则以页为粒度读取特征簇，并执行顺序的存储内计算。\nHierarchical Graph-Cluster-based ANNS\nHierarchical Graph-Cluster-based ANNS\n\n底层包含特征数据集中的全部节点，采用SPANN所使用的聚类算法对底层节点进行聚类，对每个图簇，分配多个SSD页面来存储其节点索引和对应特征信息。\n顶层由各图簇的中心节点构成，并基于这些中心节点构建图结构，然后将该图及中心节点特征存储在主存中。\n顶层的细粒度图数据访问和底层的页级簇数据访问分别与DIMM和SSD的内存访问特性相符。\n算法流程：\n在搜索过程中，在顶层执行基于图的ANNS找到最近的中心以决定需要在底层搜索的簇。对于选定的簇，其所有特征都会被读出并逐一计算。最后，对所有被选中簇中的特征按照距离进行排序，并返回最近邻结果。\n算法优势：1、减少图数据量，使图可以存储在内存中并利用Pyramid-M处理；2、最近簇中的节点被顺序处理，避免了不规则的图数据访问；3、有效降低了I/O数据传输量。\nArchitecture Design of PyramidPyramid在保留原有存储功能的前提下，支持在DIMM和SSD中进行近内存ANNS。Pyramid-M、Pyramid-S和CPU通过内存控制中心（Memory Controller Hub，MCH）进行相互通信。MCH也负责控制整个数据流。\n\nPyramid 架构概览及数据流\n\n在主存级别，Pyramid-M将DIMM划分为邻居DIMM和特征DIMM，分别用于存储图结构和特征数据集。对于百万级数据集，Pyramid-M存储整个数据集及其对应的图。而对于十亿级数据集，Pyramid-M仅存储聚类中心图以及聚类中心的特征。距离计算分布在特征DIMM的多个NMC模块中，而距离结果由MCH中的一个集中式优先队列模块进行排序。\n在存储级别，Pyramid-S根据图聚类的结果存储整个特征数据集。在处理十亿级数据集时，Pyramid-S的计算功能会被启用。Pyramid-S执行层次化图聚类ANNS方法中底层的簇内处理。\n搜索流程：\n在搜索阶段开始时，主机CPU将查询向量发送给MCH ①，同时MCH会清空优先队列。在每一轮搜索过程中，MCH 从优先队列中读取队首节点，MCH将其节点索引（V_id）发送给邻居DIMM ②。邻居DIMM查询V_id的邻居索引（N_ids）③，并将其返回给MCH ④。接着，多个N_ids被发送到特征DIMM的不同rank，以进行分布式距离计算 ⑤。在距离计算之前，特征DIMM利用内容可寻址存储器（Content-Addressable Memory，CAM）对已访问过、且其与查询向量距离已被计算的N_id进行过滤。然后，对未访问过的N_id执行距离计算 ⑥，并将计算出的距离返回给MCH ⑦。MCH中的优先队列模块将这些距离结果插入队列并执行排序 ⑧。\n经过固定数量的搜索步骤（②∼⑧）后，主存级别的图搜索完成。对于中等规模的数据集（例如，百万级），优先队列中包含的是与查询向量最邻近特征的节点索引，这些索引便是ANNS的最终输出 ⑫。而对于大规模数据集（例如，十亿级），存储在优先队列中的节点是与查询向量最邻近的聚类中心。这些中心对应的簇索引（C_ids）被发送到Pyramid-S ⑨。Pyramid-S执行存储内距离计算和排序 ⑩，并将最近的节点列表返回给MCH ⑪。\nPyramid-M ArchitecturePyramid-M具体架构如图所示：\n\nNeighbor DIMM\n邻居DIMM负责查找给定节点索引的邻居。与传统的面向内存的DIMM相比，Pyramid-M在DIMM级别增加了一个地址生成模块。该地址生成模块接收节点索引（V_id），并计算其邻居的起始地址。\nFeature DIMM\nFeature DIMM主要包含三部分：rank 级基于 CAM 的访问标记表（visited list）、rank 级距离计算模块以及 DIMM 级距离 FIFO缓冲区。\n在rank级别，Pyramid-M将已访问节点的索引存储在基于CAM的已访问列表中，每个CAM行存储一个已访问节点的索引。当一个新的邻居索引（N_id）到达时，CAM会并行地将该N_id与所有已访问的节点索引进行比较。如果在CAM中找到了该N_id（即该Nid到查询向量的距离已被计算过），rank级别的近内存计算（NMC）模块将处理下一个邻居。否则，该N_id将被写入CAM，并且其对应的特征会从DRAM中读出。读出的特征随后被分割成多个子特征，并逐一发送到距离计算模块。DIMM级别距离FIFO用于缓冲来自该DIMM内各个rank计算出的距离。距离FIFO的每个条目包含三部分：查询ID（Q_id）、邻居索引（N_id）及其距离结果。\nCentralized Top-K priority queue module\n为提升排序吞吐率，Pyramid 设计了一个每周期支持一个输入的优先队列模块，如图所示：\n\n该模块采用 2K 个寄存器实现高吞吐率的 Top-K 排序，其中包括 K 个队列寄存器（QRegs）和 K 个临时寄存器（TRegs）。优先队列的队首始终存储当前的最小距离。在每个周期中，输入的距离值被写入第一个TReg。对于其他寄存器，如果存储在第i个TReg中的距离大于第i个QReg中的距离，则第i个TReg的数据被传输到第i + 1个TReg。否则，第i个TReg的数据被写入第i个QReg，而第i个QReg的数据则被写入第i + 1个TReg。\nPyramid-S ArchitecturePyramid-S的架构概览如图所示：\n\n相较于传统 SSD，Pyramid-S 在体系结构上主要引入了以下三点改动：\n在微控制器（micro-controller）层面，对主机接口层（Host Interface Layer，HIL）的闪存固件进行了修改，以支持聚类中心索引与逻辑块地址（Logical Block Address，LBA）之间的映射与转换。在闪存通道（flash channel）层面，由于不同通道可以并行工作，本文在每个闪存控制器上集成一个距离计算模块，以充分挖掘通道级并行性。在SSD 层面，Pyramid-S 部署了多个优先队列，用于对来自所有闪存通道的距离计算结果进行排序；优先队列的数量决定了 Pyramid-S 所支持的最大查询批处理规模。   \nWorkflow Optimization由于SSD的有效带宽会随着请求数量的增多而增加，本文通过增大 Pyramid-S 每次处理的查询数量（即批处理规模）来提高存储级计算的吞吐率。此外，Pyramid-M的搜索延迟低于Pyramid-S。为了能与Pyramid-S进行流水线处理，Pyramid-M采用逐一处理查询（one-by-one），同时避免因支持大批处理规模而进行的硬件复制。\n\n在如图所示的示例中，通过将批处理大小设置为3，吞吐量提高了2倍，而延迟仍然满足SLA的要求。\n","categories":["近内存计算"],"tags":["图索引","主存级NMC&存储级NMC"]},{"title":"VStore","url":"/2025/12/23/VStore/","content":"《VStore: In-Storage Graph Based Vector Search Accelerator》\n\n\n    总结\n    \n        本文基于可计算存储架构，提出了一种大规模向量数据图搜索解决方案，可在准确率、延迟、内存占用和数据移动开销之间进行协同优化。实验评估结果表明，VStore 在保持高搜索准确率的同时，相较于 CPU、GPU 以及 ZipNN 平台，实现了显著的搜索效率提升和能耗降低。\n    \n\n\n可计算存储架构存储与网络协会将可计算存储定义为一种能够提供与存储耦合的可计算存储功能，赋予存储器一定的计算资源，从而将 CPU 的一部分数据密集型任务卸载到存储器的内部架构，以减少数据移动带来的开销。\n\n上图是两种可计算存储的通用架构。(a)为 on 的结构，即存储控制器可以同时负责数据 IO 请求和计算功能；(b)是 offpa 的架构，这种架构是在存储器内部单独集成一个 FPGA 或者是一个高端的 arm 芯片，计算单元与存储介质通过一个内部的互联总线相连接，使得计算单元可以直接去获取存储介质当中的数据，且不需要经过主机参与。\n\n    \n        该部分内容来源：[可计算存储结构加速大规模数据检索 - 知乎](https://zhuanlan.zhihu.com/p/621359388)\n    \n\n\nVStore总体架构VStore是一种部署在SSD内部的基于图的向量搜索解决方案。本文 VStore 的实现是在 OpenSSD 里 ssd 的控制器当中，可以充分利用 ssd 内部的带宽。VStore包括查询调度引擎（Query Orchestration）、向量搜索引擎（Vector Search）、地址生成器（Address Generator）以及请求重排序引擎（Request Reordering）四部分。\n\nVStore总体架构\n\nQuery OrchestrationQuery Orchestration负责根据查询之间的语义距离，对输入查询的执行顺序进行重组，其目标是在最大程度上复用片上存储的向量数据。其核心思想：语义相似的查询往往会共享相似的图遍历路径，并最终在图中的相近区域终止。因此，通过将语义相似的查询进行聚类，可以显著提升数据复用效率。具体查询调度机制如下：\n\n查询调度机制\n\n算法流程：该机制首先在一个预定义的时间窗口内收集所有到达的查询，然后选择一个与当前加速器正在执行的查询语义相近的查询作为参考。随后，通过计算该查询与其余待处理查询之间的距离来评估它们的相似性。当距离小于给定阈值 T 时，这些查询被判定为语义相似。最终，语义相近的查询将被分组，并按照顺序提交至向量搜索引擎执行，从而实现更高的搜索效率和更优的带宽利用率。\nq ← Q.front()：从当前还没被处理的查询集合 Q 中，取出排在最前面的一个查询，作为下一轮调度的“参考查询” q。\nVector SearchVector Search包括初始顶点选择（IVS）和图遍历（GR）两阶段，当从查询调度引擎接收到输入查询向量后，向量搜索引擎首先计算查询向量与导航顶点之间的距离，以确定该查询在图中的最合适入口顶点。随后，向量搜索引擎在从内存或 NAND Flash 加载的图结构上执行遍历操作，持续搜索目标顶点，直至满足终止条件为止。最终，搜索结果通过 PCIe 接口被复制到主机内存区域。\n\n向量搜索整体架构\n\n向量搜索阶段包括7个阶段：\n1、初始顶点获取：从初始顶点缓冲区或随机数生成器（RNG）中获取初始顶点。\n\n    \n        随机数生成器用于在用户定义的范围内随机生成顶点 ID。Result reuse从历史结果中选择初始导航点（查询调度机制）\n    \n\n\n2、遍历顶点读取：从结果缓冲区中取出一个或多个尚未访问的顶点，并将其标记为已访问。\n\n    \n        预取器（Prefetcher）：用于显式预取邻居列表和向量数据，以提升带宽利用率\n    \n\n\n3、邻居列表获取：从边缓冲区中获取对应顶点的邻居列表。\n\n    \n        Edge Buffer：用于缓存顶点的邻居列表；Vector Buffer：用于缓存嵌入向量；Address Buffer：由地址生成引擎使用，用于检测所请求的顶点是否已存在于DRAM中。\n    \n\n\n4、混合顶点过滤：过滤已访问的顶点，并将未访问的顶点标记为已访问。\n\n    \n        为降低计算开销并减少片上存储资源占用，搜索引擎采用了一种混合顶点过滤器（Hybrid Vertex Filter），用于跳过已经访问过的顶点，从而避免重复计算。\n    \n\n\n5、向量数据获取：从向量缓冲区中获取对应的向量数据。\n6、距离计算：计算顶点向量与查询向量之间的距离。\n\n    \n        从结果缓冲区中获取当前查询的最大距离max和最小距离min。当dist > max，则直接丢弃；当dist < max，对顶点执行步骤7；当dist < min，除对顶点执行阶段7外，还通过Forwarding对顶点快速开启下一轮遍历（当前顶点是高价值顶点）。\n    \n\n\n7、顶点写回：通过排序网络或优先队列，将顶点写入结果缓冲区。\n\n    \n        阶段1仅在IVS阶段执行，阶段2仅在GR阶段执行，其余阶段同时在IVS和GR阶段共享。\n    \n\n\n针对上述baseline dataflow产生的较高内存开销和数据依赖问题，本文提出三项优化方案：\nHybrid Vertex Filter\n由于近似向量搜索并不要求获得完全精确的结果，已访问检查操作在一定程度上可以容忍少量误差。其中，假阳性（false positive）——即顶点被判定为已访问但实际未未访问，虽然会带来一定的准确率损失，但在近似搜索场景下是可以接受的；而假阴性（false negative）——即顶点被判定为未访问但实际已被访问，则必须严格避免，因为这将导致重复搜索并显著增加计算开销。\n对此本文采用Bloom Filter 与Bitmap Array相结合，构建混合顶点过滤机制。Bloom Filter 具有良好的空间效率，能够以牺牲一定准确率为代价处理大规模数据集，其特性在于可能产生假阳性，但不会产生假阴性。其假阳性率可根据可用存储容量进行预先配置。相比之下，位图数组在处理中等规模数据集时能够提供严格的准确性保障。\nVertex Forwarding and Bypassing\n当距离计算阶段中某个顶点的距离小于结果缓冲区中第一个尚未扩展的顶点的距离时，该顶点会在阶段7被写入结果缓冲区，并随后在2阶段被弹出处理。在这种情况下，如上图所示，可在距离计算阶段直接将该顶点前递至阶段3，从而保证最近的顶点能够被立即遍历，避免不必要的缓冲与等待。\n此外，加速器中结果缓冲区的容量是有限且不可动态扩展的。基于观察，在实际搜索过程中，结果缓冲区中通常只有前 M（满足 L&gt;M&gt;k）个元素会被频繁使用。因此，当距离计算阶段中某个顶点的距离大于结果缓冲区中所有前 M 个元素的距离时，该顶点不可能进入最终结果集合，可直接被bypass，而无需写入结果缓冲区。通过这种方式，可以跳过无效顶点在写回阶段的冗余写操作，从而进一步提升加速器的整体性能。\nResult Reuse\n由于语义相似查询的搜索结果之间往往存在重叠区域。为利用相似查询中蕴含的数据复用性，一种直接的做法是缓存近期查询及其搜索结果，从而在新查询与缓存查询相似时，避免对整个图结构执行完整的向量搜索。然而，这种简单的缓存机制难以保证搜索结果的准确性。\n为在提升性能的同时保持搜索质量，本文提出了一种结果复用方法：将与当前查询语义相似的缓存查询的搜索结果作为导航顶点，用于后续的图遍历过程。通过这种方式，可以显著缩短在图结构上的遍历路径，从而降低搜索延迟。\n需要注意的是，直接采用相似缓存查询的搜索结果可能会使搜索过程陷入局部最优。为避免这一问题，本文将初始导航顶点与相似缓存查询的搜索结果进行组合，共同作为导航顶点参与图遍历。该优化在有效降低搜索延迟的同时，能够保证搜索结果的准确性。\nAddress GeneratorAddress Generator Engine作为向量搜索引擎与 NAND Flash芯片之间的桥梁。由于 NAND Flash 的读操作以页粒度而非字节粒度进行，地址生成引擎的核心职责在于识别向量数据及其邻居列表在闪存中的物理页地址。\n\nAddress generator and Request reordering\n\n如图所示，VStore 采用了固定度（fixed-degree）的图结构以避免额外的查找开销，同时针对特定数据集，向量的维度也是固定的。当向量搜索引擎请求的向量数据或邻居列表在 DRAM 中未命中时，地址生成引擎将利用顶点 ID 计算其对应的物理 Flash 页地址，计算得到的 NAND Flash 访问请求将被提交至request reordering engine。\nRequest ReorderingRequest Reordering Engine主要承担两项核心任务：请求合并（Merge Requests）和请求分发（Distribute Requests）。\nNAND Flash 的页大小通常为 4 / 8 / 16 KiB，单个物理页可同时容纳多个向量数据及其邻居列表信息。为避免多个落在同一物理页内的独立请求触发重复的读操作，请求重排序引擎会将这些请求合并为一次 NAND Flash 读请求，从而降低冗余访问开销。\n为充分利用 通道级并行性，请求重排序引擎并不按照请求到达的顺序发起 NAND Flash 读操作。如上图所示，该引擎利用哈希函数识别每个请求所对应的目标 Flash 通道，并将其分发至相应的通道 FIFO 队列。多个通道 FIFO 队列可并发发起 NAND Flash 读操作，从而显著提升 VStore 的通道级并行性能。\n","categories":["近内存计算"],"tags":["ANNS","图索引","可计算存储"]},{"title":"Starling","url":"/2025/11/27/Starling/","content":"《Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment》\n\n\n    总结\n    \n        本文提出一种面向数据段高维向量相似性搜索的I/O高效磁盘驻留图索引框架，该框架通过优化数据布局和搜索策略，在不增加额外空间开销的前提下提升搜索性能。（在存储空间与内存空间限制的条件下，实现高搜索精度与效率。）\n    \n\n\n\n研究背景相对于单机维护大型向量索引，主流向量数据库更倾向于数据分片，将大规模数据分区为多个段，并为单机分配适量数据段。每个段在受限存储容量和计算资源下运行，并在各段构建中型索引以实现自主搜索。通过特定数据分片策略与查询协调器，可在向量查询执行时仅搜索机器的少数数据段。但是现有工作未能兼顾搜索性能与空间成本以在数据段上实现有效的高维度向量相似性搜索（HVSS），而是直接应用基于磁盘的方法来处理大规模数据。\n研究现状现有的基于磁盘的HVSS方法在数据布局和搜索策略仍然遵循基于内存解决方案的范式，导致大量磁盘I/O，限制了给定空间成本下的搜索性能。以DiskANN为例：搜索路径过长，搜索过程中每一跳对应一次I/O请求，造成大量磁盘I/O，导致高搜索延迟；数据局部性差，搜索过程中的I/O仅单个节点为有效数据，从磁盘读取的数据大部分被浪费，磁盘带宽无法充分有效利用。\n框架概述Starling是一个旨在提升数据段上基于磁盘的图索引对于HVSS 的I/O 效率的框架。Starling通过优化数据布局以提高数据局部性，并通过优化搜索策略以减少磁盘I/O次数。\n\nStarling框架图（图右）\n\n基准框架基准框架（DiskANN）将ID连续的顶点存储在相同的数据块中；基准框架采用固定或随机顶点作为搜索入口。\n\n数据块内ID连续的顶点并不具备空间邻近性，故基准框架存在数据局部性差的问题。\n在数据块的非目标顶点中找到邻近目标顶点的概率过低，无法有效提升效率。因此基准框架仅检查目标顶点而丢弃其余数据，导致单次磁盘I/O的顶点利用率偏低。\n\nStarling框架数据布局：\n\n磁盘上的数据布局：Starling尽量将顶点及其邻居顶点存储在同一数据块，从而提高数据局部性，使得单次磁盘访问不仅能获取目标顶点还能获取其他可能的候选顶点，提高了顶点利用率，并潜在减少了磁盘I/O次数。\n内存中的数据布局：Starling使用内存中的导航图为基于磁盘的图识别查询相关的入口点。Starling通过内存图算法和从磁盘图中采样的小部分向量来构建导航图。对给定查询，Starling搜索导航图以获取接近查询的顶点作为入口点。然后，在磁盘上的图索引基于选定的入口点进行搜索，有效缩短了磁盘图的搜索路径长度。\n\n搜索策略：\n\nStarling采用块搜索策略以利用数据局部性。对于每个加载的块，Starling计算顶点到查询点的距离，选择接近查询的顶点，并检查其邻居顶点以查找新的搜索候选。（降低磁盘I/O操作，但增加计算成本，Starling对此进行特定优化，详见Block Search）\n\n> eg：相比b中搜索过程需要经过五跳和六次访问磁盘。Starling只需访问磁盘三次，具体的块搜索过程：(1) 加载块0并访问其中的顶点12、0、7和8，计算它们到查询的距离，并选择顶点0访问其邻居。(2) 加载包含顶点{2,5,9,14} 的块 1，并选择顶点9作为下一跳。当Starling 根据顶点11加载下 一个块时，将找到作为搜索结果的顶点3。通过在内存中构建基于向量{4,8,9}的导航图，它获得顶点9作为磁盘图搜索的入口点。由于顶点9接近查询，仅需两次磁盘I/O（一次访问入口点，一次访问其邻居）即可获得查询结果。\n数据布局Starling磁盘上图索引的块重排和构建内存导航图，来缩短搜索路径长度、减少搜索过程中的磁盘I/O次数并提高顶点利用率。\nBlock Shuffling on the DiskStarling使用图索引G的重叠率 OR(G)来衡量图布局的局部性。其中OR(u)表示在u所在块中除u外所有顶点中其邻居顶点所占的比例。\nOR(u) =  ( |B (u )∩N (u ) ) |  |B (u) |−1 |B (u) | &gt; 1  0 |B(u)| ≤ 1\nB(u)表示包含顶点u的块内所有顶点的集合；N(u)为u的邻居集合。\n块重排：给定基于磁盘的图索引G的图布局，块重排的目标就是将n个顶点分配到m个块，以获得max OR(G)的新布局。Starling提出了三种启发式算法来处理重拍问题，如图所示：\n\n启发式Block Shuffling策略\n\nBlock Neighbor Padding\nBNP算法逐一填充磁盘块。在填充过程中，按顶点ID升序检查顶点。若顶点u尚未分配至任何块，则尝试将该顶点及其邻接顶点分配至当前块。当块填满后，算法开启新块并继续分配顶点。\n\n    \n        BNP 的时间复杂度为 O (|V |)，并通过将顶点及其邻居分配给同一块来提高重叠率 OR(G)。然而，这种改进是有限的，因为 u 的一些邻居（表示为 z, v ∈ N (u)）可能彼此不相邻，这会降低 OR(z) 或 OR(v)。此外，u的一些邻居可能更早被分配给其他块，例如z也是o的邻居，其ID比u小。它不能与 u 一起存储，因为只存储每个顶点一次。\n    \n\n\n\nBlock Neighbor Frequency\nBNF算法以BNP算法的结果作为初始布局，迭代优化OR(G)。该算法的目标是将顶点分配至包含其最多邻接顶点的块，即邻接频次最高的块。算法如下：\n\nBNF算法\n\n输入：初始块级布局（来自 BNP），即每个顶点当前所属的块（集合 B0…B_{ρ-1}）；最大迭代次数 β；OR(G) 增益阈值 τ（当本轮迭代对 OR(G) 的提升小于 τ，则停止）。输出：新的块布局。B为所有块的集合，ρ为块的数量，D为每个顶点到所属块的映射。\n算法流程：\n\n保存当前顶点ID至块ID的映射，并清空G的所有块。\n对每个顶点v：\n获取u的邻接节点所属块的ID集合H。\n如果H非空，进行循环操作，选择H中拥有u邻居最多的块ID并记为x。如果块B_x还有容量，则将u放入该块并退出循环，否则（块已满），把x从集合H中移除，继续尝试下一个频次最高的块。\n如果H为空，则把节点u加入空块。（u 放入其他候选块不会改善目标，则把它放到空块以避免损害全局结构）。\n\n\n计算 OR(G) 的增益（相对于上次迭代的布局），如果提升小于阈值 τ，则提前终止（认为收敛或无显著改进）。\n当迭代次数超过β，返回图索引G的新布局。\n\nBlock Neighbor SwapBNS算法以BNP或BNF算法的结果为初始布局来优化OR(G)。在BNS中，对于顶点u的邻接节点a和e，分别属于块B(a)和B(e)，交换B(a)和B(e)中重叠比（OR）最低的顶点，以增加OR(B(a)) 和 OR(B(e)) 的总和。\n\n在BNS算法中，图的OR(G) 是迭代次数β 的单调非减函数。\n每次迭代中，更新操作是局部的，仅影响两个块内的顶点，OR(G) 不会随着迭代次数的增加而减少。\n只有在交换后两块OR之和增加时，才会对两个块中“最低OR”的顶点进行互换。\n\n对比三种算法，BNF在效率和效果之间平衡最佳。BNS时间复杂度高，但能确保OR(G)随迭代次数不降，且OR(G)的提升最显著。相较于图索引构建过程，块重排引入的额外时间成本相对较低，且不会增加额外的空间开销。\nIn-Memory Navigation GraphStarling通过内存导航图为基于磁盘的图搜索寻找更优的入口点来减小搜索路径长度。构建步骤：\n\n数据采样：从一个段（segment）里的所有数据中，按段的内存限制随机抽取一部分数据点作为样本。\n图索引构建：采用与磁盘图相同的算法，在采样数据上建立导航图。该导航图常驻内存，能快速返回邻近查询的动态入口点。\n\n搜索策略Starling 的搜索策略包含两大组成部分：(1) 内存导航图上的顶点搜索与(2)磁盘驻留图上的块搜索。内存导航图旨在无需磁盘访问即可快速导航至查询邻近区域，采用了与现有图算法相同的顶点搜索策略。顶点搜索结果作为第二部分的入口点。为利用提升的数据局部性，块搜索在每次磁盘I/O中不仅探索目标顶点，还探查同一块内的其他顶点。\n\nBlock Search Starling 离线阶段将顶点及其邻近节点分配至同一数据块，以增强数据局部性。这种方式使得每次从磁盘读取的在线数据块能提供多个相关顶点。其块搜索策略能高效探索数据块内所有有效数据：通过计算块内各顶点至查询向量的距离来更新当前搜索结果，同时检测距离较近顶点的邻居ID以发掘新候选节点。Starling还通过三项计算优化策略来提升块搜索机制并优化搜索过程增加的计算成本。\nBlock pruning\nBlock Shuffling在多数现实数据集上的OR值在0.3到0.6之间，说明数据块内存在无关顶点。为避免探索无效数据， Starling 对加载的每个数据块进行剪枝：首先按顶点与查询向量距离升序排列，仅检测前((ε−1)·σ)个 顶点的邻居ID（其中(ε−1)为块内除目标顶点外的顶点数，σ为取值区间在(0&lt;σ≤1)的剪枝率），从而提前剔除远端顶点的邻居ID。实验表明，当σ=0.3时系统始终能获得最优性能。\nI/O and computation pipeline\n块搜索阶段的主要操作包括磁盘读取（DR）和距离计算（DC）。对此，Starling 采用 I/O 与计算流水线技术实现 DR 与DC的并行执行：先对当前加载块中的目标顶点u执行DC，然后在启动下一轮DR的同时，并行处理含u数据块内其他顶点的 DC（详见ANNS算法）。\nPQ-based approximate distance\n由于单个顶点的邻居数量通常远超块内顶点数，许多邻接节点的访问仍然需要额外磁盘I/O。对此，Starling使用乘积量化PQ对全数据集进行预处理，将全精度向量编码为可驻留内存的短码，采用近似距离代替全精度向量精确计算（即通过PQ近似距离来进行下一跳的决策）。\nApproximate Nearest Neighbor Search在近似最相邻搜索策略中，Starling采用the candidate set 和 the result set来存储候选结果和搜索结果。搜索过程开始时，将从内存导航图获取的入口点加入候选集。然后，从候选集中选取距离查询点最近的未访问顶点执行块搜索，并根据搜索结果更新两个集合。当候选集中所有顶点均被访问后，流程终止。算法流程如下：\n\n\nANNS 算法\n\n输入：内存导航图Gm，PQ短码，磁盘图Gd，候选集合C，结果集R，裁剪比σ。输出：查询点q的Top-k结果\n算法流程：\n\n从内存图上搜索，获取入口点集合S。然后，计算集合S中点到q的PQ距离，然后排序将其加入集合C和计算集合S中点到q的精确距离，将其加入到集合R。\n从 C 中取 PQ 距离最小且尚未被访问的顶点 u。\n从磁盘图（Gd）加载包含顶点 u 的数据块。\n根据顶点 u 的邻接顶点更新候选集合 C。然后，计算u与q的精确距离并将其加入到结果集R。\n在已加载的块B中（去掉 u 本身），选取前 ((ε−1)·σ) 个顶点组成 B′。\n当候选集C中存在未访问节点时进行循环：选择 C 中 PQ 距离最小的未访问顶点 v ，为 v 执行第3步（加载包含 v 的块 Bv），并对B’中的顶点同步执行第4步。当包含 v 的块 Bv 被加载后，对块 Bv 执行第4、5步。\n最终返回查询点q的Top-k结果。\n\nRange Search范围搜索（RS）旨在检索所有与查询向量 q 的距离在给定半径r以内的向量。结果长度取决于向量分布，不同查询间差异显著。Starling 基于块搜索对查询 q 执行 RS。它使用候选集C、结果集R和踢出集P分别存储候选顶点、结果和从C中踢出的顶点。 Starling 动态更改 C 的长度限制以处理不同的结果长度。\nRS的步骤如下：\n\n它从内存中的导航图中获取入口点并用它们初始化C和R。\n它迭代地探索C并更新C和R（像ANNS）。它还将从C中踢出的未访问顶点添加到P中。\n当C中的所有顶点都被访问时，它计算R的长度与C的长度之比。给定比率阈值 φ，如果|R| / |C | ≥ φ，将集合 C 的规模扩大一倍并重新开始搜索。这是因为高比率意味着大多数候选对象都是结果。在这种情况下，探索更多的顶点可能会发现新的结果。\n在下一次搜索中，Starling 将 P 中较接近的顶点（相对于q）添加到 C 中，并对 C 中未访问过的候选点重复 (2)–(3)。\n|R| / |C | ≥ φ未满足，搜索停止。\n\n调整 C 的长度后，它会使用之前的结果集R、候选集C 以及集合P 中一些更 接近（相对于q）的顶点继续进行，从而避免了额外的计算和磁盘访问。。评估表明 φ = 0.5 是最佳的。\n","categories":["磁盘索引"],"tags":["ANNS","图索引"]}]