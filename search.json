[{"title":"DiskANN论文阅读笔记","url":"/2025/10/31/DiskANN/","content":"DiskANN是面向SSD的十亿规模高效向量检索方案，其目标就是在单台机器上，使用有限的内存，实现对十亿级数据集的快速、高精度的最近邻搜索。\n研究现状当前最先进的近似最相邻算法为了保证快速的高召回率，其产生的索引必须存储在内存中。但对于大规模高维数量集，单台服务器的内存容量会成为算法瓶颈，而使用分布式服务器对索引进行存储也会造成额外的磁盘访问开销和扩展成本较高。对于这些针对内存所设计的向量检索算法，如果将算法所构建的索引存储在SSD或采用混合模式会使得其实现比较复杂，还会造成搜索性能较差（搜索延迟灾难性上升和吞吐量急剧下降）。因此，如何突破内存容量的限制，实现既能处理海量数据又能保持低延迟的“磁盘友好型”向量搜索算法显得十分重要。\n具体技术本文介绍了一种新型的基于图的索引构建算法Vamana，以及DiskANN整体系统的设计。\nVamana算法Vamana算法通过图结构优化减少磁盘访问次数。 Vamana算法使用GreedySearch算法和RobustProne算法进行图的构建，所构建的图为有向图。GreedySearch算法为节点选取出边邻居，RobustProne算法通过剪枝操作来优化和确认节点的出边邻居集合。\n\nGreedySearch算法\n\n\nGreedySearch 算法\n\n对于查询节点q，从有向图G的起始节点s开始搜索，搜索列表长度为M，结果集长度为k且M&gt;=k，候选集为L，已访问集为V。在初始化的时候将s添加到L，置V为空。当L和V的差集不为空（即候选集中还存在未被访问的节点）时，进行如下循环。从上述未被访问的点中选取距离查询节点q距离最小的节点p，然后将p的出边邻居加入L，并将节点p加入V（标记为已访问）。然后判断L的长度是否大于M，若是则保留距离q最近的M个节点。最终返回L中的k个最近邻点和已访问集合V。\n比较L的长度是否大于M，是为了保证候选集的大小不会过大，并平衡搜索精度和效率。\n\nRobustProne算法\n\n\nRobustProne 算法\n\n将从GreedySearch算法得到的已访问集合作为候选集V，查询节点为p，出度上限为R，距离阈值为α，出边邻居集合为N。首先，将N加入到V，并置N为空。当V不为空时，进行下述循环。从V中选取距离点p最近的点p，然后将其加入N。判断N的长度是否等于R，若是则结束循环，算法结束，否则遍历V中的任一点p‘，判断α · d(p, p′) ≤ d(p, p′)是否成立，若是则从V中移除p’。最终得到p的新的出度邻居集合N。\nRobustProne算法使用GreedySearch算法得到的已访问集合作为候选集进行剪枝操作，原因：为查询节点引入长距离连接，避免了满足SNG图属性的线性图搜索导致大量磁盘顺序读操作，原理：通过参数α的大小主动保留那些能够快速到达远方的点，并建立远边连接。\nα · d(p, p′)≤d(p, p′)的作用：移除与所选出边邻居相似的节点，确保后续选出的邻居具有多样性（分散在不同区域），保留了长边连接的点。α=1快速剪枝控制度数，第二轮α&gt;1补充长边提升连通性，当α=1时，只要p’离p*不比离p远，就删除pp’边，图会比较稀疏，度数很小，当α&gt;1时，放宽条件，图更稠密，直径更小。\n\nVamana算法流程\n\n\n随机初始化图G，每个点随机指向R个近邻；\n计算全局质心，找到距离全局质心最近的点作为所有搜索请求的入口点p；\n基于图G和入口点p对数据集中的每个点执行近邻搜索，将搜索路径上经过的所有点作为候选点集，执行α=1的边修剪策略；\n调整α&gt;1重复步骤3，再次执行边修剪策略来填充边，提高图G的质量以保证召回率。\n\n对于Vamana算法，在执行RobustProne算法后，需要对出边邻居n建立反向边，若建立后Nout(n)的长度大于R，则对其执行RobustProne算法进行剪枝操作。\n使用α &gt; 1构建的图，在搜索时可以更快地接近目标点，需要的“跳数”（读取节点信息）更少。这对于后续放在SSD上非常重要，因为每次“跳”都需要一次SSD读取。\nVamana算法最终会构建一个部分对称的有向图。\nVamana算法与HNSW、NSSG算法的对比\n\nVamana与NSG都构造了有向图，HNSW构造了无向图。\nVamana图索引构造从随机图开始、HNSW从空图开始，NSG则从K近邻图开始。\nVamana和NSG使用贪婪搜索的已访问集合来进行剪枝操作，而HNSW算法则使用结果集进行剪枝操作。\nHNSW和NSG都没有可调参数α，而Vamana则使用α对图度数和直径进行权衡。\n对比HNSW和NSG，Vamana采用两轮迭代，可以生成质量更好的图结构。\n\nNSG的构建始于全量数据的KNN图（KNNG），这一步骤复杂度高达O(n²)，在十亿级数据集上需要数天时间。\nDiskANN磁盘索引策略重叠簇索引、束搜索、热点缓存、全精度向量的隐式重排序\n索引构建：\n\n从全量数据上采样出部分数据，训练k-means聚类，从而将全量数据划分为k个簇。为了保证以簇为单位构建的图索引能够合并起来，DiskANN将每个数据点分配到距离最近的X个簇中；\n每个簇包含约Nx/k个点，分别在内存中为每个簇构建图索引；\n依赖簇之间的重叠点将小图索引合并为大图索引。\n\n向量查询：\nDiskANN在整个搜索过程中维护搜索队列、结果队列两个队列：\n\n首先，使用PQ距离对候选入口点排序并传入搜索队列；然后，对于搜索队列中已经缓存的点，用其原始数据和查询点计算距离并送入结果队列。\n对于未命中缓存的点，开启异步进程使用束搜索去磁盘中以Vamana索引取回其原始数据表示和邻居列表；\n将每次取回的邻居列表送入搜索队列，因为搜索队列长度有限，所以依PQ距离近似排序舍弃超出部分；\n重复以上步骤，直至搜索队列为空，返回结果队列中的top-k作为最终结果。\n\nDiskANN使用乘积量化对原始数据进行压缩编码，并使用压缩数据在内存中进行距离计算。\n对于磁盘索引，吞吐量受限于随机磁盘读取的次数，延迟受限于往返磁盘的次数，对此DiskANN使用束搜索和热点缓存进行优化。通过束搜索，对于当前搜索节点p，DiskANN一次性从磁盘中读取它的W（“束宽”）个邻居。此外，根据查询分布对节点进行缓存，或简单将入口点的C(C ≈ 3)跳邻居都缓存在内存中。\n由于通过过PQ计算得到的最邻近的k个候选点，可能与使用真实距离计算得出的结果存在差异，因此DiskANN会使用全精度向量对搜索过程中已访问的节点进行重排，并选取k个最近邻节点。由于DiskANN将全精度向量与其邻域信息一同存储在SSD中（使用邻接表，并定长存储），因此当在查询过程中从SSD中获取某节点信息时，也会同步获取其全精度向量和邻域信息且无需额外的磁盘读取操作。\n\n    \n        SSD的最小读单位是4KB，读512B也要读4KB，所以读4KB并不比读512B慢。\n    \n\n\n\n\n具体结果通过实验对比Vamana算法与HNSW算法和NSG算法，得出结论Vamana在来自不同来源的百维和千维数据集上的性能与当前最好的ANNS算法相当或更优。\n对于基于SSD的服务场景中，跳数与搜索延迟直接相关， 每一跳都要对SSD进行读取，相对于HNSW和NSG，Vamana算法完成一次搜索所需的跳数更少，因此更具优势。\n通过对比十亿级别数据集上Vamana算法的单次全量索引和多次合并索引，得出结论虽然单次索引的性能优于合并索引，但与单次索引相比，达到相同召回率所需的额外延迟不超过20%。\n通过DiskANN和IVFOADC+G+P进行对比，在内存占用相同的情况下，DiskANN可以保持更高的召回率。\n","categories":["基于SSD的ANN检索策略"],"tags":["ANN向量索引策略","SSD"]},{"title":"LEANN论文阅读笔记","url":"/2025/10/31/LEANN/","content":"针对ANN向量索引存储开销大而无法部署在资源受限的边缘设备上的问题，该论文针对该问题提出了一种低存储的ANN向量检索策略——LEANN，LEANN通过不存储全部原始向量而在查询时对必要节点重新计算其向量表示的思想，使用即时重计算机制、两级遍历算法和高度数节点保留的剪枝算法等方法，在保证较高召回率和可容忍延迟的前提下，大幅度降低了向量检索在边缘设备上的存储开销。\n研究背景目前基于嵌入的搜索（向量检索技术）已经被广泛应用在推荐系统和RAG中，个人设备对此也逐渐突显需求。但由于个人设备存储空间限制和ANN索引存储开销过大的原因，无法在个人设备中存储必要的索引结构。因此，减少索引存储开销和保持搜索质量和延迟成为了关键问题。\n行业需求随着智能手机等边缘设备越来越普及，这些设备不断采集并生成多模态数据（如图像、文字、音频等）。这些数据如果能被有效地检索、组合起来，对用户回忆、检索过去信息、辅助智能应用等都有潜在价值。\n边缘设备上向量检索的应用：设备端RAG系统、个性化推荐系统、图像与视频内容检索等。\n研究动机\n动态重算嵌入以节省存储：\n\n传统图索引（如 HNSW）在查询时只会访问少量向量，因此LEANN不再将所有嵌入存储在磁盘上，而是在查询时按需重算。为减少延迟，LEANN 采用了双层遍历算法（结合近似与精确队列）和动态批处理机制（提高 GPU 利用率），从而显著降低重算开销。\n\n图结构剪枝以压缩索引元数据：\n\n即使不存储嵌入，图索引的邻接信息仍带来较大存储负担。LEANN 通过分析发现，许多边和节点对检索效果贡献有限，因此提出了高出度保持的剪枝策略，去除冗余边、保留关键枢纽节点，从而在几乎不影响搜索精度的前提下，大幅减少索引体积。\n为此，本论文提出了LEANN，通过结合紧凑图索引和高效即时重计算，以实现最小的存储负担和快速准确的检索。\n具体技术在HNSW查询中，只有很少的节点会被访问，因此 LEANN 不再预先存储全部向量，而是对这些少量节点在查询时即时重算，从而大幅降低存储成本，同时保持搜索准确率和延迟在可接受范围内。\n\n重计算虽然能节省存储空间，但仍然会导致较高的延迟。\n并且剩余的图元数据可能仍然占比很大的存储开销。\n\n对此，LEANN 采用两级图遍历算法和动态批处理机制来降低重新计算的延迟，并使用一种高度数保留的图剪枝技术，以显著减少图元数据所需的存储空间。\n系统工作流程\nLEANN 系统架构与工作流程\n\n对于给定的数据集，LEANN首先计算所有条目的嵌入向量，并利用现有的图索引方法构建向量索引。然后LEANN丢弃所有条目的嵌入向量，并通过一种高度保留的图剪枝算法对图结构进行剪枝，以减少存储开销。该算法优先保留访问频繁的“枢纽节点”因为节点访问分布往往极度偏斜。\n在查询时，LEANN使用一个两级搜索算法在被剪枝后的图上进行遍历，识别并优先探索那些更有潜力的节点。这些被选中的节点随后会被发送到嵌入服务器（一个设备端组件，用于调用原始嵌入模型重新计算节点的嵌入）以获取对应的向量表示。\n为了进一步提高 GPU 利用率并减少延迟，LEANN 采用了一种动态批处理策略，用来调度在 GPU 上的嵌入计算任务。此外，当设备上有额外磁盘空间可用时，LEANN 会将这些“枢纽节点（hub nodes）”缓存在磁盘中。运行时，系统仅对未缓存的节点重新计算嵌入，而对已缓存的节点则直接从磁盘加载，从而减少计算负担并进一步提升查询性能。\n总结，LEANN在索引构建过程中使用高度数保留图剪枝算法（High Degree Preserving Graph Pruning），以减少索引存储开销。并在向量搜索过程中使用两级搜索算法（Two-Level Search）和动态批处理策略（Dynamic batching strategy）来提高查询精度和性能。\nTwo-Level Search主要思想：\n只需对近似距离排名靠前的向量进行重新排序（re-ranking），就足以获得较高的召回率，同时能有效剪枝掉在错误方向上的节点。\n\nTwo-Level Search 算法\n\n参数解释：\nAQ（Approximate Queue，近似队列），用于存储所有计算近似距离的已访问节点；EQ（Exact Queue, 精确队列），用于存放计算精确距离的节点；re-ranking ratio（重新排序比例），用于控制进行精确计算节点的比例。\n算法流程：\n从入口节点开始，计算其精确距离，将入口节点加入EQ、R和visited。如果EQ不为空，每次从EQ中取出距离查询向量q最近的节点v，从R中取出距离q最远的节点f。若 d_v-q &gt; d_f-q 则结束算法，否则遍历v的所有邻居，对每个未访问的节点n，将其加入visited，计算近似距离d_approximate，然后将节点n加入到AQ（队列中的节点会保存节点标识和对应距离，d_approximate的计算可以通过PQ实现）。然后从AQ中取出前a%的节点（最有潜力的节点），对这些节点计算精确距离d_exact，将其加入ED和R。\nDynamic batching strategy图索引每次扩展都要根据上次扩展结果选出最有希望的节点，会导致每次GPU只能处理很小一批数据，使得GPU资源浪费严重、吞吐低、延迟高。动态批处理策略放宽这种严格的数据依赖性，每次扩展从候选队列中一次取出多个节点进行扩展，将它们的邻居节点使用GPU一起进行重计算。这样计算更高效、延迟降低，扩展顺序虽然有延迟，但整体搜索结果几乎没有影响。\n\n\n每个节点平均有 8 个邻居；\n原始做法：每次扩展 1 个节点 → batch size = 8；\n动态批处理：一次从队列取 8 个节点 → 共 8×8 = 64 个邻居 → batch size = 64。\n\n\nHigh Degree Preserving Graph PruningLEANN引入了一个用户定义的磁盘预算C。如果图元数据超过这个阈值，LEANN会触发剪枝算法。\n主要思想：\n只要保持高度数的枢纽节点，即可维持搜索性能，在减少边数量的同时保持检索准确率。\n\nHigh Degree Preserving Graph Pruning 算法\n\n参数解释：\nG：原始图；V：节点集合；ef：候选集合大小；M：高度数节点允许的最大连接数；m：普通节点允许的最大连接数(m&lt;M)；a：高节点度数比例\n算法流程：\n\n计算G中每个节点v的度数Dv，并初始化G1用来存放剪枝后的图。\n\n从D中选出出度最高的前a%节点，记为集合V*\n\n对V中每个节点v进行计算\n调用Best-First Search算法得到v的候选邻居列表W，如果v属于V*，M0=M，否则，M0=m。\n通过原始启发式（d(p, p′)≤d(p, p′)这种启发式剪枝）从W中选取M0个节点作为v的邻居并建立双向边，并添加到G1。\n如果存在邻居的出边度数大于M，则进行剪枝出度边。\n\n\n其他边缘设备（Edge Device）\n边缘设备是指部署在网络边缘、靠近数据源（如用户、传感器、机器设备等）的一类计算设备。它们负责在本地进行数据采集、处理、分析或决策，而不是将所有数据发送到云端或数据中心。\nRAG（Retrieval-Augmented Generation，检索增强生成）\nRAG是一种结合了信息检索技术与语言生成模型的人工智能技术。该技术通过从外部知识库中检索相关信息，并将其作为提示（Prompt）输入给大型语言模型（LLMs），以增强模型处理知识密集型任务的能力。\n","categories":["面向边缘设备的向量检索策略"],"tags":["ANN向量索引策略","边缘设备"]},{"title":"Milvus","url":"/2025/11/04/Milvus/","content":"研究背景及现状对于非结构化数据，通常可以通过向量化表示（embedding）进行存储和检索。但随着数据科学和人工智能的快速发展，大规模高维向量激增。传统数据库主要处理结构化或基于倒排索引的文本检索，在高维向量检索方面性能不足；现有的Annoy等相似性搜索库更偏向于单机库，难以支撑企业级的大规模分布式向量和动态向量检索需求，并且它们能提供的功能有限，无法满足通用应用的要求，（缺乏数据库级的管理能力）如缺乏实时、高效的增删改能力和元数据过滤能力（元数据指的是与指的是和向量一起存储的、用于描述该向量的额外信息）。\n行业需求对于不同行业，需要设计一种通用的专门用向量数据的数据库系统，来支持对大规模向量数据进行快速查询处理和对动态向量数据插入、删除等操作。除了简单的向量相似性搜索，还需要如属性过滤、多向量查询等的高级查询处理。\n研究动机现有的针对向量数据管理的算法和系统主要集中于向量相似性搜索，但由于在大规模和动态向量数据上性能很差且功能有限，因此无法满足如属性过滤和多向量查询等行业需求。Milvus作为专为向量计算设计的开源数据管理系统应运而生，通过优化近似最近邻搜索、支持异构硬件加速、实现分布式扩展，并实现了对向量数据的数据库级的管理能力。\n具体技术系统概述Milvus是一个专门构建的数据管理系统，旨在高效存储和检索大规模向量数据，用于数据科学和人工智能应用。Milvus的系统架构总共分为三部分，包括搜索引擎、GPU引擎和存储引擎。\n\nMilvus系统架构\n\n特点\n\n专用设计：面向高维向量数据，遵循“one-size-not-fits-all”理念，而非扩展关系型数据库。\n丰富接口：提供多语言 SDK 和 RESTful API，便于应用集成。\n异构优化：针对 CPU 与多 GPU 架构进行了性能调优。\n多样化查询：支持向量相似性搜索、属性过滤、多向量查询。\n多种索引：支持量化索引、图索引，并具备可扩展接口。\n动态数据管理：基于 LSM 结构支持插入/删除，并保持实时一致性（快照隔离）。\n分布式部署：支持多节点分布式架构，实现扩展性和高可用性。\n\n系统设计向量查询\n在Milvus中，实体被描述为一个或多个向量和一些可选的数字属性（即用来描述实体的结构化数据）。\nMilvus支持向量查询、属性过滤和多向量查询类型，同时支持多种相似度计算函数。点击跳转到相应章节\n向量索引\n对于向量索引，Milvus支持基于量化的索引和图索引，同时支持添加其他新索引。\n动态数据管理\n采用LSM-tree日志合并树的思想支持高效插入和删除：\n新插入的数据首先存放在内存中的MemTable；Memtable大小达到阈值或者每隔一秒，Memtable被刷新到磁盘作为一个新的immutable segment；较小的segments通过tiered merge机制被选择合并成更大的segment来加速顺序访问。\n\n删除操作通过”out-of-place”的方式执行（不直接在原始数据上修改），merge操作时移除已删除的数据。\n数据更新操作通过删除+插入操作完成。\n默认情况下milvus只为超过1GB的segment构建索引，用户也可以对任何大小的segment手动构建索引。\n数据和对应的索引文件存放在相同的segment中。segment是milvus搜索、调度和缓存的基本单位。\nmilvus使用快照隔离机制保证读写操作共享一致性视图，避免读写互相影响。\n\n存储管理\n\n向量存储：Milvus对于单向量实体连续存储，并通过隐式row IDs和偏移量进行访问。对于多向量实体，通过列式存储保存实体的每个向量。\n属性存储：采用列式存储，每个属性列按照形式进行存储，key为属性值、vlaue为row ID。对于磁盘上的数据构建了skip pointers索引来加速列标量属性上的点查询和范围查询。\n缓冲池：Milvus假设所有数据和索引可放入内存，内存不足时按照segment粒度进行LRU缓存替换。\n多种存储系统支持：为了灵活性和可靠性，Milvus支持包括本地文件系统、Amazon S3、HDFS在内的多种底层存储方案。\n\n异构计算\nMilvus针对包括CPU和GPU在内的异构计算平台进行了高度优化。点击跳转到相应章节\n分布式系统Milvus按照分布式系统中的存储计算分离、共享存储、读写分离、一写多读的设计原则进行构建。\n\n异构计算面向CPU的优化缓存优化由于Faiss对于相似度查询的并行处理操作是将查询向量分配给线程对数据向量进行相似度计算，每个线程的数据向量无法被下一个线程复用，会导致数据向量频繁从主存中进行加载，造成大量CPU cache miss。在查询时，线程对数据集进行流式传输，一部分数据被加载进 L3；很快被新的数据替换掉；当下一个查询开始时，之前加载的内容已经不在 cache 中了；因此又要重新从主存加载同样的数据。\n\nMilvus的缓存感知设计\n\n具体技术：\nt：线程数；n为数据向量数；m为查询向量数；b为每个线程分配的数据向量数；s为查询块大小（每次批处理的查询数）。\n\n数据划分：每个线程被分配b=n/t个数据向量。\n查询划分：将m个查询向量数划分为s个查询块，使查询块能够驻留L3 CPU cache。\n批量比较：每个线程将分配的数据向量与查询块的每个查询向量进行距离计算。每个线程会为每个查询向量q维护一个heap，将得到的top-k存入其中。\n结果合并：将每个查询向量对应的heap分别进行合并，得到最终的top-k结果。\n\nMilvus选择将数据向量平均分配给查询向量，并将查询向量划分为s块，使查询块能够常驻L3 CPU cache。每个线程将分配的数据向量与查询块的查询向量进行距离计算。每个线程会为每个查询向量q维护一个heap，将得到的top-k存入其中。最终，将每个查询向量对应的heap分别进行合并，得到最终的top-k结果。\nSIMD优化SIMD是一种并行计算技术，主要用于在一条指令下同时对多个数据进行相同操作。\nFaiss实现了基于SIMD的算法用于加速向量相似性搜索，但并不支持AVX512。\nMilvus支持SIMD的四种指令集：SSE、AVX、AVX2和AVX512。SIMD指令多版本自动调度：对于每个常用函数（例如相似度计算）Milvus实现了SSE、AVX、AVX2、AVX512四个版本并放置在不同的源文件中，针对不容的SIMD标志可以选择不同的版本进行编译。运行时milvus根据当前CPU标志自动选择合适的SIMD指令，并使用hooking技术链接合适的函数指针。\n面向GPU的优化相比于Faiss的Top-k最大为1024，Milvus通过多轮迭代支持最大k为16384。Milvus第一轮类似Faiss计算出top1024个结果对应的距离和ID，记第一轮的最大距离为dl，第二轮milvus在剩下的距离大于等于dl的其他结果中挑选下一批1024个结果，如此迭代。\nFaiss的GPU数量在编译时固定，编译后的文件只能运行在GPU数量不少于编译时的服务器上运行。Milvus编译后的文件支持在任何服务器上运行。并且Milvus引入了分段调度，对于查询任务，将数据集以segment为单位在可用的GPU设备上进行调度，每个segment由单个GPU提供服务。\nGPU-CPU协同设计在GPU显存无法加载全部数据集的情况下，Faiss使用IVF_SQ8量化索引进行数据压缩并通过PCIe总线将数据从内存搬运到GPU显存中。该策略有两个缺陷：1.Faiss采用逐个bucket搬运的方式，PCIe带宽无法被充分利用；2.考虑到数据搬运延迟，在GPU中执行查询可能不是最优选择。\n\nSQ8H算法\n\n针对上述问题，Milvus提出了一种新型索引——SQ8H。\n\nMilvus在CPU和GPU间一次传输多个bucket，提高了IO带宽。Milvus采用LSM树，删除和更新不直接修改原数据，而是写入新位置，原数据标记为无效，多桶同时复制不会影响删除/更新逻辑。\nMilvus将大batch查询放在GPU执行，例如batch size超过1000时，Milvus将使用GPU执行所有查询并将涉及的数据同时多桶加载到GPU显存中。否则，milvus将采用混合执行的方案：1.将查找n个目标bucket的计算放在GPU中执行；2.在CPU中扫描每个bucket中的量化向量数据。动机：步骤1中的buckets中心体量很小，且批查询中所有查询都需要比较相同的buckets中心数据。步骤2中不同查询请求查询的量化向量较为分散。\n\n\n高级查询处理属性过滤\n不同的属性过滤策略\n\nMilvus基于已有的研究，实现了四种属性过滤方法。属性约束为C_A，向量查询约束为C_V：\n\n策略A attribute-first-vector-full-scan：使用C_A采用标量索引进行过滤，对剩余数据采用向量全表扫描机制。当C_A高选择性时（即过滤后的数据很少）采用；\n策略B attribute-first-vector-search：使用C_A过滤数据得到位图bitmap，使用C_V进行一般向量搜索，并查询搜索结果在位图中是否有效，有效结果存入top-k结果集中。当C_A、C_V都具有一定的选择性时使用；\n策略C vector-first-attribute-full-scan：先向量搜索，后全表扫描标量过滤，向量搜素通常输出θ*k（θ&gt;1）个结果保证最后可以查找到top-k个结果。C_V高选择性时使用。\n策略D cost-based：采用AnalyticDB-V中代价估计策略评估前三个策略的执行代价，选择代价最小的策略执行查询计划，适用性很强\n\n\n策略E partition-based：该方法时Milvus还提出的一种新的属性过滤方法。Milvus动态维护一个属性查询访问计数hash表。按照频繁访问的属性划分数据集，将策略4应用到每个数据划分。每个partition应该也维护了一个属性范围用于加速过滤。\nMilvus采用离线的方式对历史数据构建partition，构建完成后应用于在线查询；\n用户可以自定义分区数量，分区数量太大导致分区内向量数量过少，标量属性过滤趋近线性搜索。分区数量太小，分区内数据量增大，过滤无关数据效率变低。Milvus推荐每个分区内容纳1 million百万条向量。\n\n\n\n多向量查询朴素解决方案（最直接、未优化的做法）\n多向量查询的朴素方案是对查询实体的每个向量，分别在对应的数据集Di上执行一次单独的查询，等到Top-k候选结果，然后再对这些候选结果进行汇总与计算，得到最终的Top-k查询结果。其中，Di为所有实体再第i维度的向量vi（vi=(a1,a2,a3,……)）的集合。这种方法会丢失同一实体的多个向量之间的关联性，导致召回率非常低。\nMilvus提出了两种新方法：\n\n向量融合\n\n对于查询实体q，使用一个聚合函数对q的u个向量生成一个聚合后的查询向量。然后，将该聚合查询向量与数据集中存储的拼接向量进行相似度搜索，得到最终结果。方法简单高效，只需要执行一次向量查询操作，但需要使用可分解的相似度函数，如内积，对于一些相似度度量函数可以通过归一化进行转换为内积。\n\n迭代融合\n\n针对底层数据没有归一化，且相似度函数不可分解的情况，Milvus在对NRA算法改进的基础上，提出了迭代融合算法。该算法通过VectorQuery(q.vi,Di,k′) 来一次性获取 q.vi 在Di 上的 top-k′个查询结果，不需要为每次访问都重新进行向量查询，同时也消除了堆维护的高开销。\n\nIterative merging算法\n\n主要思路：\n\n该算法迭代地在每个Di上执行 top-k′查询，将结果放入Ri中。（其中 Di={e.vi∣e∈D}，即所有实体在第 i个向量维度上的集合）\n然后，在所有Ri上运行 NRA 算法。如果 NRA 能够完全确定至少 k 个结果，即可以安全地停止，那么算法就终止并输出 top-k 结果。\n否则，算法会将k’加倍（double），然后重新执行上述步骤，直到k’达到预设阈值为止。\n\n其他向量数据库向量数据库，也称为基于向量的搜索引擎或相似性搜索数据库，是专门为存储、检索和管理向量数据设计的数据库系统。它们最大的特点是能高效执行相似性搜索，即找到数据库中与查询向量最相似的向量。\n位图\nbitmap 是一种高效的且占用内存很小的 判断 某个值 存在与否的数据结构。它用二进制的某一位去表示某个值是否存在。void set(int k) &#123;  M[k &gt;&gt; 3] |= (0x80 &gt;&gt; (k &amp; 0x07)); &#125;\nK&gt;&gt;3：等价于K/8，K&amp;0x07：等价于K%8bool test(int k) &#123;  return M[k &gt;&gt; 3] &amp; (0x80 &gt;&gt; (k &amp; 0x07));&#125;NRA算法\nNRA算法主要用于解决Top-k问题，即在大量数据中找到前k个最优解。与传统的TA算法不同，NRA算法只需要顺序读取数据，因此在处理大规模数据时更为高效。该算法要求聚合函数是可分配的（distributive）和单调的（monotone）。\n","categories":["向量数据库"],"tags":["异构计算","向量数据库设计"]},{"title":"Hexo教程","url":"/2025/10/30/Hexo%E6%95%99%E7%A8%8B/","content":"相关命令创建一篇文章$ hexo new &quot;My New Post&quot;\nMore info: Writing\n本地运行Hexo命令$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo s\ngithub推送$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d\ngithub推送源码$ git status #查看修改文件$ git add .$ git commit -m &quot;更新博客内容&quot;$ git push\n拉取远程更新$ git pull\n$ git fetch$ git status$ git merge\n本地启动$ hexo server\nMore info: Server\n生成静态文件$ hexo generate\nMore info: Generating\n远程部署$ hexo deploy\nMore info: Deployment\n","categories":["Hexo教程"],"tags":["hexo相关命令"]},{"title":"SPANN","url":"/2025/11/11/SPANN/","content":"研究背景目前针对超大规模向量检索场景的混合ANNS方案，如DiskANN、HM-ANN，都是基于图的解决方案。本文论证了简单倒排索引方法同样能在大规模数据集上实现召回率、延迟与内存成本的顶尖性能。\nSPANN概述不同于以往基于倒排索引且依赖有损数据压缩来降低内存开销的方法，SPANN采用了一种简洁的内存-磁盘混合方案。\n索引结构：数据向量X被划分为N 个倒排列表${\\mathbf{X}_1,\\mathbf{X}_2,\\cdots,\\mathbf{X}_N}$，$\\mathrm{X}_1\\mathrm{X}_2\\cup…\\cup\\mathrm{X}_N=\\mathrm{X}^3$。这些倒排列表的质心$\\mathbf{c}_1,\\mathbf{c}_2,\\cdots,\\mathbf{c}_N$存储在内存中作为快速粗粒度索引，指向磁盘中对应倒排列表的位置。 \n局部检索：当查询向量q到来时，我们寻找K个最近邻质心\\{c_{i1}, c_{i2}, \\ldots, c_{iK}\\}, K \\ll N，并将这K个最近质心对应的倒排列表X_{i_1}, X_{i_2}, \\cdots, X_{i_K}中的向量加载至内存，以进行后续细粒度检索。\n主要挑战challenge 1倒排表长度均衡问题：对于存储在磁盘上的倒排表，为减少磁盘访问次数，需限制每个倒排列表的长度，使其仅需数次磁盘读取即可载入内存。这不仅要求将数据分割为大量倒排列表，还需保持各列表长度均衡。不均衡的倒排列表将导致查询延迟（尤其当列表存储于磁盘时）呈现高度方差。 \nchallenge 2聚类分簇的边界问题：查询向量q的最近邻可能分布在多个倒排列表的边界区域。由于仅搜索少量相关倒排列 表，位于其他列表中的真实近邻将被遗漏。\nchallenge 3搜索过程中需要查询的倒排表数量问题：不同查询任务具有不同搜索难度。部分查询仅需搜索1-2个倒排列表，而部分查询需搜索大量列表。若对所有查询均搜索相同数量的列表，将导致低召回率或高延迟。\n主要技术针对上述挑战，本文给出了相应的解决方法。\nSolution 1对于数据分区，本文采用分层多约束平衡聚类算法。通过多次迭代地将向量聚类为少量簇，直至每个倒排表包含有限的向量数量。分层平衡聚类将大簇（黄色簇）中的向量迭代平衡划分为少量小簇（绿色簇），直至每个簇仅包含有限数量的向量（蓝色簇）。\n\n分层平衡聚类\n\n为使查询过程中查询向量与质心的距离计算更具意义， 采用最接近质心的向量替代质心来表示每个倒排列表。由此，冗余的导航计算转化为对真实候选子集的距离计算。为加速查询，为代替质心的向量建立SPTAG索引。SPTAG 通过构建空间划分树和相对邻域图作为向量索引，可将最近质心搜索加速至亚毫秒级响应时间。\n\nSPTAG索引\n\nSolution 2为解决边界问题，本文使用闭包多簇分配方案：\n\n若向量与多个聚类中心的距离几乎相同，则将该向量分配给多个最近的聚类，而非仅分配给最近的一个，如公式$(1)$：\n\n\n\\begin{aligned}\n\\mathbf{x}\\in\\mathbf{X}_{ij} & \\Longleftrightarrow\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{ij})\\leq(1+\\epsilon_1)\\times\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{i1}), \\\\\n & \\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{i1})\\leq\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{i2})\\leq\\cdots\\leq\\mathrm{Dist}(\\mathbf{x},\\mathbf{c}_{iK})\n\\end{aligned}\n \\tag{1}\n聚类闭包分配\n\n若边界向量（绿色点）与多个聚类簇（蓝色与黄色聚类簇）的距离近乎相等， 则将其分配至多个最近邻聚类簇。\n\n为避免相邻倒排表中重复向量过多，导致过多不必要的计算操作和硬盘读取操作。论文采用RNG规则优化闭包聚类分配，即为边界向量选择多个代表聚类，以降低相邻倒排列表的相似度，如公式$(2)$：\n\n\n\\mathrm{Dist}(\\mathbf{c}_{ij},\\mathbf{x})>\\mathrm{Dist}(\\mathbf{c}_{ij-1},\\mathbf{c}_{ij})\n\\tag{2}核心思想：相邻倒排列表更易被索引同时召回。相较于在相邻列表中存储相似向量，存储差异化向量更能提升在线搜索的可见向量数量。\n\n    \n        自我理解：将聚类中心与边界向量的距离进行排序得到列表Cij，如果Cij与Cij-1的距离小于Li与边界向量的距离，说明两个聚类具有高度相似性，则跳过边界向量加入该聚类。\n    \n\n\n\n\n\n\n\nRNG规则优化\n\n运用RNG规则降低两个相邻倒排列表的相似度。橙色点将被分配给蓝色与灰色倒排列表，尽管其与黄色列表的距离比灰色列表更近。\nSolution 3为有效处理多样化查询，提升召回率和降低查询延迟，本文使用查询感知的动态剪枝技术。根据查询向量与质心向量的距离，动态缩减需要搜索的倒排表数量，如公式$(3)$：\n\n\\begin{aligned}\n\\mathbf{q}\\overset{search}{\\operatorname*{\\operatorname*{\\longrightarrow}}}\\mathbf{X}_{ij} & \\Longleftrightarrow\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{ij})\\leq(1+\\epsilon_2)\\times\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{i1}), \\\\\n\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{i1}) & \\leq\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{i2})\\leq\\cdots\\leq\\mathrm{Dist}(\\mathbf{q},\\mathbf{c}_{iK})\n\\end{aligned}\n\\tag{3}不再为所有查询统一搜索最邻近的K个倒排列表，而是仅当某倒排列表质心与查询的距离趋近于查询到最近质心的距离时，才动态判定该列表需要被搜索。\n","categories":["基于SSD的ANN检索策略"],"tags":["ANN向量索引策略","SSD"]},{"title":"SmartANNS","url":"/2025/11/09/SmartANNS/","content":"\n    概述\n    \n        传统的ANNS算法需要在内存中维护大量索引，导致难以扩展到大规模数据集。现有的可行解决方案基于普通SSD，但该方案会导致I/O操作在执行时间中占比很大，并且ANNS引擎与其他软件协同工作会竞争PCIe带宽，会导致性能瓶颈。\n    \n    \n    本文通过在“主机CPU + SmartSSD”协同架构下，通过NDP技术，将高开销的向量搜索任务在SmartSSD上执行，减少与主机CPU的数据传输并降低对主机资源的占用，显著提高大规模数据集的向量搜索性能。\n    \n    \n    具体技术：1、分层索引架构：使用HBC算法进行分片，并单独为每个分片建立图索引。分片质心存储在主机内存，分片数据和图索引存储在SmartSSD中；2、基于学习的分片剪枝：对于不同的查询任务，通过GBDT模型动态预测需要搜索的最优分片数量，避免无关分片的遍历，降低计算开销；3、动态任务调度：根据分片热度和数据布局，对查询任务进行调度，在多SmartSSD间实现负载均衡与数据重用；4、FPGA优化：在SmartSSD内实现高效的向量搜索核，采用并行距离计算、流水线优化、数据池与内核池机制，实现任务并行与延迟隐藏。\n    \n\n\n研究背景目前大多的ANNS算法大多依赖内存索引以实现快速精确搜索，由于要内存中维护的大量索引，这些算法对内存资源需求很大，导致ANNS服务难以扩展到大规模数据集。对此，现有的一种可行解决方案是将向量索引存储在SSD上，同时结合主机内存进行实现，如DiskANN和SPANN等。但基于SSD的ANNS方案具有一定的局限性和缺点：1、I/O操作在执行时间中占比很大；2、PCIe带宽有限；3、ANNS引擎与其他软件协同工作，会竞争PCIe总线的带宽。\n相关技术SmartSSDSmartSSD（智能固态硬盘）是一种集成可编程计算单元（如FPGA或SoC）的计算存储设备。SmartSSD 利用近数据处理（NDP）范式，借助其板载DRAM和现场可编程门阵列（FPGA）在存储设备内部处理数据，减少与主机CPU的数据传输并降低对主机资源的占用，从而提升系统性能与能效。\nSmartSSD的优势：\n\n采用NDP技术，可以进行本地数据处理。\n\n​    SmartSSD可以对存储在SSD中的索引执行本地查询，随后将部分结果返回主机进行聚合\n\nSmartSSD使用内部PCIe总线进行数据传输。\n\n​    可以实现多硬盘的近似线性加速、显著降低总线PCIe带宽竞争并减少主机CPU/内存资源消耗（主机只需要接受SSD处理的数据结果）。\nHBC算法HBC（分层平衡聚类）算法对数据集进行分片划分，通过匹配质心排除不相关分片，显著减少不必要的数据访问与计算。本文通过实验对HBC算法的数据访问模式进行探索。\n\n不同查询的数据访问模式\n\n实验证明HBC算法呈现显著的数据局部性和高度偏斜的不同分片访问热度，这为后续的任务调度实现提供了理论依据。\nGBDTGBDT（梯度提升决策树）是一种迭代的决策树算法，它通过构造一组弱的学习器（树），并把多个决策树的结果累加起来作为最终的预测输出。决策树是一种有监督(现有样本已知分类结果)的机器学习方法。\n在SmartANNS中，对于GBDT模型的训练，在构建训练数据时，先通过遍历搜索获得真实最邻近向量，然后判断这些向量落在哪些分片，得到分片数量N，并以此为监督信号来训练模型。在此过程中会偶尔记录分片热度并以此对SmartSSD进行分片分配。\n主要问题&amp;解决方案\nQ：由于多块SmartSSD 之间缺乏通信通道，每个SmartSSD必须搜索更多分片才能达到所需的准确度，导致额外的计算开销。因此，需要通过一个全局协调器查阅所有分片的质心来缩小搜索空间。\nQ：采用分层索引方法时，每个ANNS查询会被发送到一部分分片，导致不同分片之间的访问分布不均（偏斜）。一批查询如果调度不当，可能导致SmartSSD 之间的负载不均衡，最终损害系统的可扩展性。\nQ：由于查询的搜索范围可能存在显著差异，通常难以确定每个查询所需的最小分片数量。静态配置可能导致不必要的计算或精度损失。\n\nS：提出”主机CPU+SmartSSD”协同处理架构，利用分层索引大幅降低SmartSSD数据访问与计算量。\n\nS：基于优化数据布局的动态任务调度机制，同时实现负载均衡与数据复用。\nS：使用一种轻量级学习型分片剪枝算法GBDT，以消除SmartSSD上不必要的计算。\n\n整体架构SmartANNS利用分层索引实现”主机CPU+SmartSSD”的协同ANNS处理。\n\nSmartSSD 整体架构\n\n离线处理：SmartANNS同样以离线方式构建索引。SmartANNS首先采用HBC算法对数据集进行均匀分片，使得片内向量具有较高相似性。然后，为每个分片单独构建HNSW索引。分片的质心存储在主机内存中，分片本身存储在SmartSSD中。SmartANNS会采样部分训练查询样本来训练梯度提升决策树，以在运行时确定每个查询的最小搜索范围。\n在线处理：当到达一个ANNS查询时，SmartANNS 计算查询向量与各分片质心之间的距离。基于 这些距离，梯度提升决策树预测为找到近邻应当搜索的分片数量。随后，主机利用任务调度机制将查询任务分发到不同的SmartSSD。 当某个SmartSSD 接收到查询任务时，如果能够复用与其他任务一起已加载到其板载DRAM 中的分片，则会优先处理该任务。对于每个任务，SmartSSD 利用内部 PCIe 交换机将该分片的索引从 SSD 传输到FPGA的板载DRAM。随后，SmartSSD内的ANNS引擎开始迭代搜索，并将结果返回给主机。 最后，主机汇总来自不同SmartSSD的所有结果。\n系统整体流程：\n在离线阶段，SmartANNS通过HBC算法对数据集进行分片。然后，SmartANNS会采样部分训练查询样本来训练GBDT，同时偶尔记录过程中不同分片热度并以此对SmartSSD进行分配分片，使得每个SmartSSD的总分片热度大致相同。对于每个分片SmartANNS会单独构建HNSW图索引。分片质心存储在主机内存，分片数据和图索引存储在SmartSSD中。\n在线查询阶段，主机接收到查询任务，计算查询向量q和所有分片质心的距离，将距离进行排序并取相对距离Dk/D1。将查询向量、相对距离和分片总数输入到GBDT模型得到需要搜索的分片数量N。对于选定分片，主机通过key-value表来确定分片位置，然后打包查询任务，并由任务调度器进行任务调度。最后，由向量搜索引擎来完成后续的任务重排序和向量搜索任务。\n具体设计分层索引SmartANNS使用HBC算法对向量数据集进行尽可能均匀的分片，使得每个分片大小几乎相同。然后，系统会在对每个分片使用HNSW算法构建图索引。\nSmartANNS在主机内存中保存所有分片的质心，并维护一个key-value对来记录各分片在SmartSSD中的存储地址。\n在索引构建过程中SmartSSD使用公式$(1)$来解决聚类过程中的边界问题。\nv\\in S_i\\Leftrightarrow Dist(v,S_i)\\leq(1+\\varepsilon)\\times Dist(v,S_1) \\tag{1}分片剪枝与IVF算法相比，SmartANNS构建了更大的分片，质心数量更少。这样设计使得主要计算开销从质心遍历转移到了分片内部的搜索，减少主机CPU负担，充分发挥SmartSSD的计算潜力。对于不同查询使用固定的分片数量并不合理，这会导致无关分片被扫描，导致计算资源的浪费。为解决该问题，SmartANNS使用GBDT模型来动态预测每个查询需要扫描的最优分片数量。\nGBDT是一种集成学习算法，通过组合多个简 单的决策树（称为“弱学习器”）来构建一个强预测模型。它通过迭代训练：先进行均值预测，计算残差， 并将弱决策树拟合到这些负梯度上。在推理阶段，模型通过融合各个弱模型的加权贡献来生成预测。该模型的输入参数包括查询向量、其与分片的空间关系以及分片的总数。\n\nGBDT 输入参数\n\n对于GBDT模型，使用相对距离作为模型训练的特征：令D1表示查询与最近分片的距离，Dk表示查询与第k近分片的距离，使用Dk与D1的比值作为决策树训练的关键特征。在构建训练数据时，先通过遍历搜索获得真实最邻近向量，然后判断这些向量落在哪些分片，得到分片数量N，并以此为监督信号来训练模型。\n任务调度SmartANNS通过多块SmartSSD并行来实现NDP功能，并通过分片热度和任务调度来充分利用多块SmartSSD之间的数据并行性和实现不同SmartSSD之间的负载均衡。\n\n任务调度举例\n\n分片热度实验证明HBC算法呈现显著的数据局部性和高度偏斜的不同分片访问热度，这为任务调度实现提供了理论依据。\n\n在构建GBDT模型的训练数据时，记录不同分片热度。\n在将分片分配给 SmartSSD 时，按照热度对分片进行排序，然后迭代地将当前热度最高的分片分配给累积热度最低的 SmartSSD，从而最终使各 SmartSSD 的热度值大致相同。\n同时在不同SmartSSD 之间进行一次性的数据副本复制，当某个 SmartSSD 负载过高时，系统可以将任务迁移到另一个 SmartSSD。\n\n任务调度任务调度器的设计关键在于考虑查询之间的数据复用，动态平衡SmartSSD的负载。任务调度的具体算法如下：\n\n任务调度算法\n\n参数解释：Devices：可用SmartSSD设备；Tasks：需要处理的查询任务序列；Device_to_Tasks：每个设备被分配的任务列表。\n具体流程：\n\n对每个查询任务，找到与存储该任务相关的分片的所有候选设备Base。然后查找是否存在该任务能复用数据的设备集合Assign。\n如果没有可复用设备或所有相关设备均可复用，从 Base 中选择当前负载最小的设备 Target，把任务插入到该设备，并更新该设备负载。\n否则，将当前任务临时插入候选设备的任务列表 Temp，调用 Estimate(Temp) 计算插入后的预期完成时间或代价，并记录到 TimeInfo[j]。选择使估计时间最小的设备 Target，将任务真正插入，并更新设备负载。\n最终返回Device_to_Tasks。\n\n向量搜索内核向量搜索内核包括：层监控器、距离计算模块、排序模块和数组更新模块。在执行搜索时，向量搜索内核维护：已访问列表、已排序候选列表和已排序最终结果列表。搜索过程类似HNSW算法，调用距离计算模块。\n\n向量搜索内核\n\n\n\n对FPGA的优化：\n\n    并行读邻居\n    遍历候选向量时并行读取所有邻居，且通过独立接口并行加载向量数据与链表。（用 m_axi 适配器实现 MAXI-A、MAXI-B 两路封装，在 MAXI-A 通道中封装查询和向量数据；在 MAXI-B 通道中封装链接表与搜索结果）\n\n\n    访问标记压缩\n    用位图（bitmap）替代原 HNSW 的布尔数组以节省片上内存。\n\n\n    高效排序\n    实现了对硬件具有高度优化特性的bitonic sort（双调排序）算法来高效更新candidate与final列表\n\n\n    距离计算并行化\n    通过HLS的循环展开与流水线、多个处理单元（PE）并行计算高维向量间距离。\n\n\n    数据/内核池化\n    在板载DRAM设数据池（data pool），在 FPGA设内核池（kernel pool）。当一个shard被加载到数据池后，多个内核可以并行处理该shard的任务；同时可在一个shard被搜索时后台加载另一个shard，从而重叠加载与搜索，隐藏数据访问延迟并提升任务并行度\n\n\n\n其他近数据处理技术\n近数据处理技术（Near Data Processing，NDP）是一种将计算操作尽可能靠近数据存储位置的策略，目的是通过减少数据移动的成本（如带宽、延迟、能耗）来提高系统效率。在传统的冯诺依曼架构中，存储器中的数据需要搬移到内存中后CPU才能执行计算操作，对数据库等数据密集型应用很不友好。因此近数据处理技术的核心思想是利用存储器中的处理能力对数据进行简单处理（例如数据库的筛选操作）后只传输数据处理结果到主机中，节约了大量的系统资源，降低了时延和能耗。\n计算存储设备\n计算存储设备（CSD）是一种在存储设备内部集成可编程计算资源的创新架构，是NDP技术的具体实现。CSD使部分数据处理任务能够在存储端“就地执行”，从而减少数据搬运开销，提高系统整体性能与能效。\n现场可编程门阵列\n现场可编程门阵列（FPGA）是一种可完成通用功能的可编程逻辑芯片，即可以对其进行编程实现某种逻辑处理功能。\nHigh Level Synthesis\n高层次综合（High Level Synthesis，HLS）是把高级语言（如 C/C++/SystemC、OpenCL）描述的算法自动转换为硬件描述（RTL：Verilog/VHDL/HDL）的工具和方法。最终目标是生成可在 FPGA 或 ASIC 上实现的硬件电路。\n","categories":["基于CSD的向量检索策略"],"tags":["ANN向量索引策略","计算存储设备","SmartSSD"]},{"title":"HM-ANN","url":"/2025/11/16/HM-ANN/","content":"\n    总结\n    \n        本文基于异构内存（由PMem和DRAM组成）提出HM-ANN向量检索策略，综合考虑内存与数据的异构性，在无需压缩的情况下实现了单节点上的十亿级别的相似性检索。\n    \n    \n        具体技术：1、改进HNSW算法，引入自上而下插入和自下而上提升阶段：对于HNSW算法构建最底层进行保留并置于慢速内存中；从底层图中提升枢纽节点形成上层图结构并置于快速内存；2、通过动态迁移等内存管理技术，实现从慢速内存预取待访问数据到快速内存，并通过并行搜索降低慢速内存中的搜索耗时；3、建立性能模型以根据搜索时间与召回率约束自动选择超参数。\n    \n\n\n\n研究背景因内存容量限制，ANNS算法的设计需要平衡精度与效率。现有的基于SSD的解决方案会大大增加I/O操作时间。异构内存可以大幅度增加内存容量，以便ANNS算法同时兼顾精度和效率，为ANNS算法的设计提供了新机遇。\n异构内存\n异构内存HM由高容量内存技术和高性能内存技术构成：\n\n高容量内存：如NVM、SSD，容量大但访问延迟高。\n高性能内存：如DRAM、HBM，容量小但访问延迟低。\n\n具体技术HM-ANN通过改进HNSW算法，索引构建包括引入自上而下的插入阶段与自下而上的提升阶段。\n索引构建HM-ANN的索引基于HNSW图索引进行扩展构建，包括自上而下插入和自下而上提升两个阶段。自上而下阶段构建可导航的小世界图作为最底层（L0层）置于慢速内存；自下而上阶段从底层图中提升枢纽节点形成上层图（L1层…L层）结构置于快速内存，使多数搜索操作在快速内存中完成且保持高精度。\n\nHM-ANN索引构建过程\n\nTop-down insertions：使用HNSW算法自上而下构建图索引。\nBottom-up promotions：基于L0层节点，对L1…L层索引结构进行重建（L即为图索引的层深）。\n具体算法如下：\n\nHM-ANN索引构建算法\n\n参数解释：V：向量数据集；d：向量维度；M：插入节点在某层上建立的连接数；efConstruction：动态候选列表大小；Ni：第i层允许插入的节点数量；L：HM-ANN索引结构的层深。\n算法流程：\n\n首先利用HNSW算法构建图索引。然后，遍历V中的节点v，得到每个节点在L0层的度数集合Dv，并对D进行降序排序。并移除L1…L层的节点，然后根据后续步骤重建上层索引结构。\n对按照度数进行降序排序的节点进行遍历，从L1向上层进行重建：\n如果Ni==0，选取距离插入节点v最近的节点作为下次在上层插入节点的入口节点ep；\n否则，将节点插入该层，获取该节点的最近似邻居集合W，通过启发式选取邻居并与插入节点建立双向连接。\n如果邻居节点的出度大于M，则收缩连接数。该算法将L0层中度数最高的节点提升到L1层。从i（i&gt;=2）到i+1，HM-ANN以1/M的提升率将高度数节点提升到上层（M为该层节点的最大邻居数）。\n\n\n\n\nL0层图中的枢纽节点（hub point）是那些拥有大量连接（即高度数）的节点。高度数节点提供了更好的导航能力，节点间的大多数最短路径都经过枢纽节点。当选择度数最高的相邻节点作为下一跳时，导航路径的平均长度（即跳数）最小。通过提升高度数节点形成的L1层，使HM-ANN相较于随机提升策略能有效减少在L0层的搜索次数。\n\nHM-ANN 没有使用 HNSW 的 random selection for promotion，而是使用高度推广策略 high-degree promotion strategy，将第 0 层中度数最高的元素推广到第1层。对于更高的层，HM-ANN 根据 promotion rate 将高度数节点推广到上层。\n\nHM-ANN 将更多的节点从第 0 层 promote 到第 1 层，并为第 1 层的每个元素设置更大的最大邻居数。上层节点的数量是由可用的 DRAM 空间决定的。由于第 0 层不存储在 DRAM 中，因此使存储在 DRAM 中的每一层更密集，可以提高搜索质量。\n\n向量查询向量搜索阶段总共分为两部分：\n快速内存搜索：从顶层入口点开始，自上而下进行单步贪婪搜索。在L1层通过efSearchL1控制搜索过程返回的候选列表大小，列表的候选点作为L0层搜索的入口点。\n并行L0搜索：在L0层，HM-ANN将来自L1层搜索的候选点均匀分配，作为入口点，利用Thr个线程执行并行多起点单步贪婪搜索，最后合并得到最终结果，具体算法如下：\n\n并行L0层搜索算法\n\n在 L1 层搜索时，系统异步地将efSearchL1 候选点的邻居节点及其在L1层的连接关系从慢速内存复制至快速内存的迁移空间。当L0层搜索启动时，部分待访问数据已置于快速内存，从而缩短查询时间。\n对于参数efSearch_L1和efSearch_L2的选择，HM-ANN通过建立性能模型根据搜索时间与召回率约束自动选择超参数。\n","categories":["基于HM的向量检索策略"],"tags":["ANN向量索引策略","异构内存"]},{"title":"Starling","url":"/2025/11/27/Starling/","content":"\n    总结\n    \n        本文提出一种面向数据段高维向量相似性搜索的I/O高效磁盘驻留图索引框架，该框架通过优化数据布局和搜索策略，在不增加额外空间开销的前提下提升搜索性能。（在存储空间与内存空间限制的条件下，实现高搜索精度与效率。）\n    \n\n\n\n研究背景相对于单机维护大型向量索引，主流向量数据库更倾向于数据分片，将大规模数据分区为多个段，并为单机分配适量数据段。每个段在受限存储容量和计算资源下运行，并在各段构建中型索引以实现自主搜索。通过特定数据分片策略与查询协调器，可在向量查询执行时仅搜索机器的少数数据段。但是现有工作未能兼顾搜索性能与空间成本以在数据段上实现有效的高维度向量相似性搜索（HVSS），而是直接应用基于磁盘的方法来处理大规模数据。\n研究现状现有的基于磁盘的HVSS方法在数据布局和搜索策略仍然遵循基于内存解决方案的范式，导致大量磁盘I/O，限制了给定空间成本下的搜索性能。以DiskANN为例：搜索路径过长，搜索过程中每一跳对应一次I/O请求，造成大量磁盘I/O，导致高搜索延迟；数据局部性差，搜索过程中的I/O仅单个节点为有效数据，从磁盘读取的数据大部分被浪费，磁盘带宽无法充分有效利用。\n框架概述Starling是一个旨在提升数据段上基于磁盘的图索引对于HVSS 的I/O 效率的框架。Starling通过优化数据布局以提高数据局部性，并通过优化搜索策略以减少磁盘I/O次数。\n\nStarling框架图（图右）\n\n基准框架基准框架（DiskANN）将ID连续的顶点存储在相同的数据块中；基准框架采用固定或随机顶点作为搜索入口。\n\n数据块内ID连续的顶点并不具备空间邻近性，故基准框架存在数据局部性差的问题。\n在数据块的非目标顶点中找到邻近目标顶点的概率过低，无法有效提升效率。因此基准框架仅检查目标顶点而丢弃其余数据，导致单次磁盘I/O的顶点利用率偏低。\n\nStarling框架数据布局：\n\n磁盘上的数据布局：Starling尽量将顶点及其邻居顶点存储在同一数据块，从而提高数据局部性，使得单次磁盘访问不仅能获取目标顶点还能获取其他可能的候选顶点，提高了顶点利用率，并潜在减少了磁盘I/O次数。\n内存中的数据布局：Starling使用内存中的导航图为基于磁盘的图识别查询相关的入口点。Starling通过内存图算法和从磁盘图中采样的小部分向量来构建导航图。对给定查询，Starling搜索导航图以获取接近查询的顶点作为入口点。然后，在磁盘上的图索引基于选定的入口点进行搜索，有效缩短了磁盘图的搜索路径长度。\n\n搜索策略：\n\nStarling采用块搜索策略以利用数据局部性。对于每个加载的块，Starling计算顶点到查询点的距离，选择接近查询的顶点，并检查其邻居顶点以查找新的搜索候选。（降低磁盘I/O操作，但增加计算成本，Starling对此进行特定优化，详见Block Search）\n\n> eg：相比b中搜索过程需要经过五跳和六次访问磁盘。Starling只需访问磁盘三次，具体的块搜索过程：(1) 加载块0并访问其中的顶点12、0、7和8，计算它们到查询的距离，并选择顶点0访问其邻居。(2) 加载包含顶点{2,5,9,14} 的块 1，并选择顶点9作为下一跳。当Starling 根据顶点11加载下 一个块时，将找到作为搜索结果的顶点3。通过在内存中构建基于向量{4,8,9}的导航图，它获得顶点9作为磁盘图搜索的入口点。由于顶点9接近查询，仅需两次磁盘I/O（一次访问入口点，一次访问其邻居）即可获得查询结果。\n数据布局Starling磁盘上图索引的块重排和构建内存导航图，来缩短搜索路径长度、减少搜索过程中的磁盘I/O次数并提高顶点利用率。\nBlock Shuffling on the DiskStarling使用图索引G的重叠率 OR(G)来衡量图布局的局部性。其中OR(u)表示在u所在块中除u外所有顶点中其邻居顶点所占的比例。\nOR(u) =  ( |B (u )∩N (u ) ) |  |B (u) |−1 |B (u) | &gt; 1  0 |B(u)| ≤ 1\nB(u)表示包含顶点u的块内所有顶点的集合；N(u)为u的邻居集合。\n块重排：给定基于磁盘的图索引G的图布局，块重排的目标就是将n个顶点分配到m个块，以获得max OR(G)的新布局。Starling提出了三种启发式算法来处理重拍问题，如图所示：\n\n启发式Block Shuffling策略\n\nBlock Neighbor Padding\nBNP算法逐一填充磁盘块。在填充过程中，按顶点ID升序检查顶点。若顶点u尚未分配至任何块，则尝试将该顶点及其邻接顶点分配至当前块。当块填满后，算法开启新块并继续分配顶点。\n\n    \n        BNP 的时间复杂度为 O (|V |)，并通过将顶点及其邻居分配给同一块来提高重叠率 OR(G)。然而，这种改进是有限的，因为 u 的一些邻居（表示为 z, v ∈ N (u)）可能彼此不相邻，这会降低 OR(z) 或 OR(v)。此外，u的一些邻居可能更早被分配给其他块，例如z也是o的邻居，其ID比u小。它不能与 u 一起存储，因为只存储每个顶点一次。\n    \n\n\n\nBlock Neighbor Frequency\nBNF算法以BNP算法的结果作为初始布局，迭代优化OR(G)。该算法的目标是将顶点分配至包含其最多邻接顶点的块，即邻接频次最高的块。算法如下：\n\nBNF算法\n\n输入：初始块级布局（来自 BNP），即每个顶点当前所属的块（集合 B0…B_{ρ-1}）；最大迭代次数 β；OR(G) 增益阈值 τ（当本轮迭代对 OR(G) 的提升小于 τ，则停止）。输出：新的块布局。B为所有块的集合，ρ为块的数量，D为每个顶点到所属块的映射。\n算法流程：\n\n保存当前顶点ID至块ID的映射，并清空G的所有块。\n对每个顶点v：\n获取u的邻接节点所属块的ID集合H。\n如果H非空，进行循环操作，选择H中拥有u邻居最多的块ID并记为x。如果块B_x还有容量，则将u放入该块并退出循环，否则（块已满），把x从集合H中移除，继续尝试下一个频次最高的块。\n如果H为空，则把节点u加入空块。（u 放入其他候选块不会改善目标，则把它放到空块以避免损害全局结构）。\n\n\n计算 OR(G) 的增益（相对于上次迭代的布局），如果提升小于阈值 τ，则提前终止（认为收敛或无显著改进）。\n当迭代次数超过β，返回图索引G的新布局。\n\nBlock Neighbor SwapBNS算法以BNP或BNF算法的结果为初始布局来优化OR(G)。在BNS中，对于顶点u的邻接节点a和e，分别属于块B(a)和B(e)，交换B(a)和B(e)中重叠比（OR）最低的顶点，以增加OR(B(a)) 和 OR(B(e)) 的总和。\n\n在BNS算法中，图的OR(G) 是迭代次数β 的单调非减函数。\n每次迭代中，更新操作是局部的，仅影响两个块内的顶点，OR(G) 不会随着迭代次数的增加而减少。\n只有在交换后两块OR之和增加时，才会对两个块中“最低OR”的顶点进行互换。\n\n对比三种算法，BNF在效率和效果之间平衡最佳。BNS时间复杂度高，但能确保OR(G)随迭代次数不降，且OR(G)的提升最显著。相较于图索引构建过程，块重排引入的额外时间成本相对较低，且不会增加额外的空间开销。\nIn-Memory Navigation GraphStarling通过内存导航图为基于磁盘的图搜索寻找更优的入口点来减小搜索路径长度。构建步骤：\n\n数据采样：从一个段（segment）里的所有数据中，按段的内存限制随机抽取一部分数据点作为样本。\n图索引构建：采用与磁盘图相同的算法，在采样数据上建立导航图。该导航图常驻内存，能快速返回邻近查询的动态入口点。\n\n搜索策略Starling 的搜索策略包含两大组成部分：(1) 内存导航图上的顶点搜索与(2)磁盘驻留图上的块搜索。内存导航图旨在无需磁盘访问即可快速导航至查询邻近区域，采用了与现有图算法相同的顶点搜索策略。顶点搜索结果作为第二部分的入口点。为利用提升的数据局部性，块搜索在每次磁盘I/O中不仅探索目标顶点，还探查同一块内的其他顶点。\n\nBlock Search Starling 离线阶段将顶点及其邻近节点分配至同一数据块，以增强数据局部性。这种方式使得每次从磁盘读取的在线数据块能提供多个相关顶点。其块搜索策略能高效探索数据块内所有有效数据：通过计算块内各顶点至查询向量的距离来更新当前搜索结果，同时检测距离较近顶点的邻居ID以发掘新候选节点。Starling还通过三项计算优化策略来提升块搜索机制并优化搜索过程增加的计算成本。\nBlock pruning\nBlock Shuffling在多数现实数据集上的OR值在0.3到0.6之间，说明数据块内存在无关顶点。为避免探索无效数据， Starling 对加载的每个数据块进行剪枝：首先按顶点与查询向量距离升序排列，仅检测前((ε−1)·σ)个 顶点的邻居ID（其中(ε−1)为块内除目标顶点外的顶点数，σ为取值区间在(0&lt;σ≤1)的剪枝率），从而提前剔除远端顶点的邻居ID。实验表明，当σ=0.3时系统始终能获得最优性能。\nI/O and computation pipeline\n块搜索阶段的主要操作包括磁盘读取（DR）和距离计算（DC）。对此，Starling 采用 I/O 与计算流水线技术实现 DR 与DC的并行执行：先对当前加载块中的目标顶点u执行DC，然后在启动下一轮DR的同时，并行处理含u数据块内其他顶点的 DC（详见ANNS算法）。\nPQ-based approximate distance\n由于单个顶点的邻居数量通常远超块内顶点数，许多邻接节点的访问仍然需要额外磁盘I/O。对此，Starling使用乘积量化PQ对全数据集进行预处理，将全精度向量编码为可驻留内存的短码，采用近似距离代替全精度向量精确计算（即通过PQ近似距离来进行下一跳的决策）。\nApproximate Nearest Neighbor Search在近似最相邻搜索策略中，Starling采用the candidate set 和 the result set来存储候选结果和搜索结果。搜索过程开始时，将从内存导航图获取的入口点加入候选集。然后，从候选集中选取距离查询点最近的未访问顶点执行块搜索，并根据搜索结果更新两个集合。当候选集中所有顶点均被访问后，流程终止。算法流程如下：\n\n\nANNS 算法\n\n输入：内存导航图Gm，PQ短码，磁盘图Gd，候选集合C，结果集R，裁剪比σ。输出：查询点q的Top-k结果\n算法流程：\n\n从内存图上搜索，获取入口点集合S。然后，计算集合S中点到q的PQ距离，然后排序将其加入集合C和计算集合S中点到q的精确距离，将其加入到集合R。\n从 C 中取 PQ 距离最小且尚未被访问的顶点 u。\n从磁盘图（Gd）加载包含顶点 u 的数据块。\n根据顶点 u 的邻接顶点更新候选集合 C。然后，计算u与q的精确距离并将其加入到结果集R。\n在已加载的块B中（去掉 u 本身），选取前 ((ε−1)·σ) 个顶点组成 B′。\n当候选集C中存在未访问节点时进行循环：选择 C 中 PQ 距离最小的未访问顶点 v ，为 v 执行第3步（加载包含 v 的块 Bv），并对B’中的顶点同步执行第4步。当包含 v 的块 Bv 被加载后，对块 Bv 执行第4、5步。\n最终返回查询点q的Top-k结果。\n\nRange Search范围搜索（RS）旨在检索所有与查询向量 q 的距离在给定半径r以内的向量。结果长度取决于向量分布，不同查询间差异显著。Starling 基于块搜索对查询 q 执行 RS。它使用候选集C、结果集R和踢出集P分别存储候选顶点、结果和从C中踢出的顶点。 Starling 动态更改 C 的长度限制以处理不同的结果长度。\nRS的步骤如下：\n\n它从内存中的导航图中获取入口点并用它们初始化C和R。\n它迭代地探索C并更新C和R（像ANNS）。它还将从C中踢出的未访问顶点添加到P中。\n当C中的所有顶点都被访问时，它计算R的长度与C的长度之比。给定比率阈值 φ，如果|R| / |C | ≥ φ，将集合 C 的规模扩大一倍并重新开始搜索。这是因为高比率意味着大多数候选对象都是结果。在这种情况下，探索更多的顶点可能会发现新的结果。\n在下一次搜索中，Starling 将 P 中较接近的顶点（相对于q）添加到 C 中，并对 C 中未访问过的候选点重复 (2)–(3)。\n|R| / |C | ≥ φ未满足，搜索停止。\n\n调整 C 的长度后，它会使用之前的结果集R、候选集C 以及集合P 中一些更 接近（相对于q）的顶点继续进行，从而避免了额外的计算和磁盘访问。。评估表明 φ = 0.5 是最佳的。\n","categories":["基于SSD的ANN检索策略"],"tags":["ANN向量索引策略","SSD"]}]